{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14523807,"sourceType":"datasetVersion","datasetId":9276062},{"sourceId":394223,"sourceType":"datasetVersion","datasetId":174616}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-17T09:11:53.294826Z","iopub.execute_input":"2026-01-17T09:11:53.295463Z","iopub.status.idle":"2026-01-17T09:11:53.572889Z","shell.execute_reply.started":"2026-01-17T09:11:53.295433Z","shell.execute_reply":"2026-01-17T09:11:53.572109Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nsl-kdd-augmented/smote_augmented.csv\n/kaggle/input/nslkdd/KDDTest+.arff\n/kaggle/input/nslkdd/KDDTest-21.arff\n/kaggle/input/nslkdd/KDDTest1.jpg\n/kaggle/input/nslkdd/KDDTrain+.txt\n/kaggle/input/nslkdd/KDDTrain+_20Percent.txt\n/kaggle/input/nslkdd/KDDTest-21.txt\n/kaggle/input/nslkdd/KDDTest+.txt\n/kaggle/input/nslkdd/KDDTrain+.arff\n/kaggle/input/nslkdd/index.html\n/kaggle/input/nslkdd/KDDTrain+_20Percent.arff\n/kaggle/input/nslkdd/KDDTrain1.jpg\n/kaggle/input/nslkdd/nsl-kdd/KDDTest+.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTest-21.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTest1.jpg\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+_20Percent.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTest-21.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTest+.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+.arff\n/kaggle/input/nslkdd/nsl-kdd/index.html\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+_20Percent.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain1.jpg\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import QuantileTransformer, LabelEncoder\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ncolumns = [\n    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land',\n    'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n    'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n    'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count',\n    'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n    'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n    'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n    'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate',\n    'outcome', 'level'\n]\n\n# ===========================================\n# 1️⃣ Advanced Preprocessing (Quantile Mapping)\n# ===========================================\n# Load your SMOTE data\ndf_train = pd.read_csv(\"/kaggle/input/nsl-kdd-augmented/smote_augmented.csv\") \ndf_test = pd.read_csv(\"/kaggle/input/nslkdd/KDDTest+.txt\", header=None)\ndf_test.columns = columns # Ensure 'columns' list is defined in your session\n\n# Ensure labels match exactly\ntrain_labels = set(df_train['outcome'].unique())\ndf_test = df_test[df_test['outcome'].isin(train_labels)].reset_index(drop=True)\n\ncat_cols = ['protocol_type', 'service', 'flag']\nnum_cols = [c for c in df_train.columns if c not in cat_cols + ['outcome', 'level']]\n\n# Label Encode for Embeddings\ncat_dims = []\nfor col in cat_cols:\n    le_c = LabelEncoder()\n    df_train[col] = le_c.fit_transform(df_train[col].astype(str))\n    # Stable mapping for test set\n    train_classes = {cls: i for i, cls in enumerate(le_c.classes_)}\n    df_test[col] = df_test[col].map(lambda x: train_classes.get(str(x), 0))\n    cat_dims.append(len(le_c.classes_))\n\n# Quantile Transformation (Q1 Standard for handling skewed network data)\nqt = QuantileTransformer(output_distribution='normal', random_state=42)\nX_train_num = qt.fit_transform(df_train[num_cols]).astype(np.float32)\nX_test_num = qt.transform(df_test[num_cols]).astype(np.float32)\n\nle_target = LabelEncoder()\ny_train = le_target.fit_transform(df_train['outcome'])\ny_test = le_target.transform(df_test['outcome'])\n\n# ===========================================\n# 2️⃣ Novel Architecture: Contextual Gated Transformer\n# ===========================================\nclass FeatureSieve(nn.Module):\n    \"\"\"Novelty: Dynamically gates numerical noise based on statistical intensity.\"\"\"\n    def __init__(self, num_dim, emb_dim):\n        super().__init__()\n        self.gate = nn.Sequential(\n            nn.Linear(num_dim, num_dim),\n            nn.Sigmoid()\n        )\n        self.projection = nn.Linear(num_dim, emb_dim)\n        self.norm = nn.LayerNorm(emb_dim)\n\n    def forward(self, x):\n        g = self.gate(x)\n        # Apply gate to original signal before projection to keep mapping stable\n        x = x * g \n        return self.norm(self.projection(x))\n\nclass DualStreamTransformer(nn.Module):\n    def __init__(self, cat_dims, num_feat_dim, num_classes, emb_dim=64):\n        super().__init__()\n        # 1. Semantic Stream (Categorical Embeddings)\n        self.embs = nn.ModuleList([nn.Embedding(d, emb_dim) for d in cat_dims])\n        \n        # 2. Statistical Stream (Gated Numerical Path)\n        self.sieve = FeatureSieve(num_feat_dim, emb_dim)\n        \n        # 3. Attention Backbone\n        t_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=8, batch_first=True)\n        self.transformer = nn.TransformerEncoder(t_layer, num_layers=3)\n        \n        # 4. Global Fusion Head\n        self.head = nn.Sequential(\n            nn.Linear(emb_dim * (len(cat_dims) + 1), 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x_cat, x_num):\n        # Stream 1: Semantic\n        x_c = [emb(x_cat[:, i]) for i, emb in enumerate(self.embs)]\n        x_c = torch.stack(x_c, dim=1) # [Batch, 3, Emb]\n        \n        # Stream 2: Statistical\n        x_n = self.sieve(x_num).unsqueeze(1) # [Batch, 1, Emb]\n        \n        # Cross-Stream Attention\n        combined = torch.cat([x_c, x_n], dim=1) # [Batch, 4, Emb]\n        context = self.transformer(combined).flatten(1)\n        \n        return self.head(context)\n\n# ===========================================\n# 3️⃣ Training and Evaluation\n# ===========================================\nclass NSLDataset(Dataset):\n    def __init__(self, c, n, y):\n        self.c, self.n, self.y = torch.tensor(c), torch.tensor(n), torch.tensor(y)\n    def __len__(self): return len(self.y)\n    def __getitem__(self, i): return self.c[i], self.n[i], self.y[i]\n\ntrain_loader = DataLoader(NSLDataset(df_train[cat_cols].values, X_train_num, y_train), batch_size=512, shuffle=True)\ntest_loader = DataLoader(NSLDataset(df_test[cat_cols].values, X_test_num, y_test), batch_size=512, shuffle=False)\n\nmodel = DualStreamTransformer(cat_dims, X_train_num.shape[1], len(le_target.classes_)).to(DEVICE)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\ncriterion = nn.CrossEntropyLoss()\n\nfor epoch in range(20):\n    model.train()\n    total_loss = 0\n    for xc, xn, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        xc, xn, y = xc.to(DEVICE), xn.to(DEVICE), y.to(DEVICE)\n        optimizer.zero_grad()\n        out = model(xc, xn)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Loss: {total_loss/len(train_loader):.4f}\")\n\n# Final Metrics\nmodel.eval()\nall_p, all_y = [], []\nwith torch.no_grad():\n    for xc, xn, y in test_loader:\n        out = model(xc.to(DEVICE), xn.to(DEVICE))\n        all_p.extend(torch.argmax(out, 1).cpu().numpy())\n        all_y.extend(y.numpy())\n\nprint(classification_report(all_y, all_p, target_names=le_target.classes_, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T09:12:24.214983Z","iopub.execute_input":"2026-01-17T09:12:24.215666Z","iopub.status.idle":"2026-01-17T09:19:56.578547Z","shell.execute_reply.started":"2026-01-17T09:12:24.215634Z","shell.execute_reply":"2026-01-17T09:19:56.577721Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 1090/1090 [00:22<00:00, 49.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.1794\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 1090/1090 [00:21<00:00, 50.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0970\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 1090/1090 [00:21<00:00, 50.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0792\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 1090/1090 [00:21<00:00, 49.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0688\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 1090/1090 [00:21<00:00, 49.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0629\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 1090/1090 [00:21<00:00, 49.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0588\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 1090/1090 [00:22<00:00, 49.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0547\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|██████████| 1090/1090 [00:22<00:00, 48.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0516\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|██████████| 1090/1090 [00:22<00:00, 48.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0487\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|██████████| 1090/1090 [00:22<00:00, 48.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0463\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11: 100%|██████████| 1090/1090 [00:22<00:00, 48.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0443\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12: 100%|██████████| 1090/1090 [00:22<00:00, 48.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0419\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13: 100%|██████████| 1090/1090 [00:22<00:00, 48.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0407\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14: 100%|██████████| 1090/1090 [00:22<00:00, 48.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0401\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15: 100%|██████████| 1090/1090 [00:22<00:00, 48.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0378\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16: 100%|██████████| 1090/1090 [00:22<00:00, 48.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0373\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17: 100%|██████████| 1090/1090 [00:22<00:00, 48.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0361\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18: 100%|██████████| 1090/1090 [00:22<00:00, 48.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0345\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19: 100%|██████████| 1090/1090 [00:22<00:00, 48.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0341\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20: 100%|██████████| 1090/1090 [00:22<00:00, 47.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0334\n                 precision    recall  f1-score   support\n\n           back       0.99      1.00      0.99       359\nbuffer_overflow       0.12      0.20      0.15        20\n      ftp_write       0.00      0.00      0.00         3\n   guess_passwd       0.71      0.00      0.01      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.80      0.96      0.87       141\n           land       1.00      1.00      1.00         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      0.99      0.99      4657\n           nmap       0.72      1.00      0.84        73\n         normal       0.86      0.93      0.90      9711\n           perl       0.20      0.50      0.29         2\n            phf       0.03      0.50      0.05         2\n            pod       0.62      0.88      0.73        41\n      portsweep       0.67      0.91      0.77       157\n        rootkit       0.00      0.00      0.00        13\n          satan       0.77      0.89      0.83       735\n          smurf       0.78      1.00      0.88       665\n            spy       0.00      0.00      0.00         0\n       teardrop       0.18      0.67      0.28        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       0.86      0.01      0.01       944\n\n       accuracy                           0.84     18794\n      macro avg       0.45      0.50      0.42     18794\n   weighted avg       0.87      0.84      0.81     18794\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import QuantileTransformer, LabelEncoder\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# ===========================================\n# 1️⃣ Advanced Preprocessing (Quantile Mapping)\n# ===========================================\n# Use your stable loading logic\ndf_train = pd.read_csv(\"/kaggle/input/nsl-kdd-augmented/smote_augmented.csv\") \ndf_test = pd.read_csv(\"/kaggle/input/nslkdd/KDDTest+.txt\", header=None)\ndf_test.columns = columns # Ensure columns are assigned correctly\n\ntrain_labels = set(df_train['outcome'].unique())\ndf_test = df_test[df_test['outcome'].isin(train_labels)].reset_index(drop=True)\n\ncat_cols = ['protocol_type', 'service', 'flag']\nnum_cols = [c for c in df_train.columns if c not in cat_cols + ['outcome', 'level']]\n\n# Label Encode for Embeddings\ncat_dims = []\nfor col in cat_cols:\n    le_c = LabelEncoder()\n    df_train[col] = le_c.fit_transform(df_train[col].astype(str))\n    train_classes = {cls: i for i, cls in enumerate(le_c.classes_)}\n    df_test[col] = df_test[col].map(lambda x: train_classes.get(str(x), 0))\n    cat_dims.append(len(le_c.classes_))\n\n# Quantile Transformer handles skewed network outliers better than StandardScaler\nqt = QuantileTransformer(output_distribution='normal', random_state=42)\nX_train_num = qt.fit_transform(df_train[num_cols]).astype(np.float32)\nX_test_num = qt.transform(df_test[num_cols]).astype(np.float32)\n\nle_target = LabelEncoder()\ny_train = le_target.fit_transform(df_train['outcome'])\ny_test = le_target.transform(df_test['outcome'])\n\n# ===========================================\n# 2️⃣ Novel Architecture: Gated Residual Transformer\n# ===========================================\nclass GLUBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc = nn.Linear(dim, dim * 2)\n        self.ln = nn.LayerNorm(dim)\n    def forward(self, x):\n        res = x\n        x = self.fc(x)\n        content, gate = x.chunk(2, dim=-1)\n        x = content * torch.sigmoid(gate)\n        return self.ln(x + res)\n\nclass GMHResNet(nn.Module):\n    def __init__(self, cat_dims, num_dim, n_classes, emb_dim=64):\n        super().__init__()\n        self.embs = nn.ModuleList([nn.Embedding(d, emb_dim) for d in cat_dims])\n        self.num_gate = nn.Sequential(nn.Linear(num_dim, 128), GLUBlock(128))\n        self.num_proj = nn.Linear(128, emb_dim)\n        \n        t_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=8, batch_first=True)\n        self.transformer = nn.TransformerEncoder(t_layer, num_layers=3)\n        \n        self.head = nn.Sequential(\n            nn.Linear(emb_dim * 4, 512),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(512, n_classes)\n        )\n\n    def forward(self, x_c, x_n):\n        x_c_emb = torch.stack([e(x_c[:, i]) for i, e in enumerate(self.embs)], dim=1)\n        x_n_gated = self.num_proj(self.num_gate(x_n)).unsqueeze(1)\n        \n        combined = torch.cat([x_c_emb, x_n_gated], dim=1)\n        x = self.transformer(combined).flatten(1)\n        return self.head(x)\n\n# ===========================================\n# 3️⃣ Training & Balanced Loss\n# ===========================================\nclass_counts = np.bincount(y_train)\nlogit_adj = torch.tensor(class_counts + 1).float().log().to(DEVICE)\n\nmodel = GMHResNet(cat_dims, X_train_num.shape[1], len(le_target.classes_)).to(DEVICE)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n\ntrain_loader = DataLoader(NSLDataset(df_train[cat_cols].values, X_train_num, y_train), batch_size=512, shuffle=True)\ntest_loader = DataLoader(NSLDataset(df_test[cat_cols].values, X_test_num, y_test), batch_size=512, shuffle=False)\n\nfor epoch in range(20):\n    model.train()\n    for xc, xn, y in tqdm(train_loader):\n        xc, xn, y = xc.to(DEVICE), xn.to(DEVICE), y.to(DEVICE)\n        optimizer.zero_grad()\n        logits = model(xc, xn)\n        # Apply logit adjustment to shift boundary for rare classes\n        loss = nn.functional.cross_entropy(logits - 0.5 * logit_adj, y)\n        loss.backward()\n        optimizer.step()\n\n# Evaluation\nmodel.eval()\nall_p, all_y = [], []\nwith torch.no_grad():\n    for xc, xn, y in test_loader:\n        out = model(xc.to(DEVICE), xn.to(DEVICE))\n        all_p.extend(torch.argmax(out, 1).cpu().numpy())\n        all_y.extend(y.numpy())\n\nprint(classification_report(all_y, all_p, target_names=le_target.classes_, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T09:24:15.139894Z","iopub.execute_input":"2026-01-17T09:24:15.140679Z","iopub.status.idle":"2026-01-17T09:30:23.827956Z","shell.execute_reply.started":"2026-01-17T09:24:15.140647Z","shell.execute_reply":"2026-01-17T09:30:23.827308Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 1090/1090 [00:17<00:00, 61.26it/s]\n100%|██████████| 1090/1090 [00:17<00:00, 61.26it/s]\n100%|██████████| 1090/1090 [00:17<00:00, 61.68it/s]\n100%|██████████| 1090/1090 [00:18<00:00, 60.31it/s]\n100%|██████████| 1090/1090 [00:18<00:00, 60.46it/s]\n100%|██████████| 1090/1090 [00:18<00:00, 59.96it/s]\n100%|██████████| 1090/1090 [00:18<00:00, 59.78it/s]\n100%|██████████| 1090/1090 [00:18<00:00, 59.57it/s]\n100%|██████████| 1090/1090 [00:18<00:00, 59.37it/s]\n100%|██████████| 1090/1090 [00:18<00:00, 59.60it/s]\n100%|██████████| 1090/1090 [00:18<00:00, 59.60it/s]\n100%|██████████| 1090/1090 [00:18<00:00, 59.60it/s]\n100%|██████████| 1090/1090 [00:18<00:00, 59.64it/s]\n100%|██████████| 1090/1090 [00:18<00:00, 59.62it/s]\n100%|██████████| 1090/1090 [00:18<00:00, 59.56it/s]\n100%|██████████| 1090/1090 [00:18<00:00, 59.46it/s]\n100%|██████████| 1090/1090 [00:18<00:00, 59.53it/s]\n100%|██████████| 1090/1090 [00:18<00:00, 59.59it/s]\n100%|██████████| 1090/1090 [00:18<00:00, 59.21it/s]\n100%|██████████| 1090/1090 [00:18<00:00, 59.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"                 precision    recall  f1-score   support\n\n           back       0.98      0.97      0.97       359\nbuffer_overflow       0.08      0.20      0.11        20\n      ftp_write       0.00      0.00      0.00         3\n   guess_passwd       1.00      0.00      0.01      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.69      0.94      0.79       141\n           land       1.00      1.00      1.00         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       0.99      0.98      0.99      4657\n           nmap       0.57      1.00      0.72        73\n         normal       0.86      0.92      0.89      9711\n           perl       0.22      1.00      0.36         2\n            phf       0.03      0.50      0.05         2\n            pod       0.45      0.80      0.57        41\n      portsweep       0.76      0.91      0.83       157\n        rootkit       0.00      0.00      0.00        13\n          satan       0.73      0.90      0.81       735\n          smurf       0.74      1.00      0.85       665\n            spy       0.00      0.00      0.00         0\n       teardrop       0.20      0.75      0.31        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       1.00      0.00      0.00       944\n\n       accuracy                           0.83     18794\n      macro avg       0.45      0.52      0.40     18794\n   weighted avg       0.89      0.83      0.80     18794\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\n\n# ===========================================\n# 1️⃣ Load SMOTE-Augmented Data & Test Data\n# ===========================================\nlocal_path = \"/kaggle/input/nslkdd/\"\ntrain_path = \"/kaggle/input/nsl-kdd-augmented/\"\n\n# Load augmented training data and standard test data\ndf_train = pd.read_csv(train_path + \"smote_augmented.csv\") \ndf_test  = pd.read_csv(local_path + \"KDDTest+.txt\", header=None)\n\ncolumns = [\n    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land',\n    'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n    'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n    'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count',\n    'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n    'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n    'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n    'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate',\n    'outcome', 'level'\n]\ndf_test.columns = columns\n\n# Standardize outcome to string to prevent type mismatch\ndf_train['outcome'] = df_train['outcome'].astype(str)\ndf_test['outcome'] = df_test['outcome'].astype(str)\n\n# Filter test set: Only evaluate on classes present in the augmented training set\ntrain_labels = set(df_train['outcome'].unique())\ndf_test = df_test[df_test['outcome'].isin(train_labels)].reset_index(drop=True)\n\n# ===========================================\n# 2️⃣ Preprocessing\n# ===========================================\ntarget = 'outcome'\ncat_cols = ['protocol_type', 'service', 'flag']\nnum_cols = [c for c in columns if c not in cat_cols + [target, 'level']]\n\npreprocessor = ColumnTransformer([\n    ('num', StandardScaler(), num_cols),\n    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n])\n\nX_train_proc = preprocessor.fit_transform(df_train.drop(columns=[target, 'level'], errors='ignore'))\nX_test_proc  = preprocessor.transform(df_test.drop(columns=[target, 'level'], errors='ignore'))\n\nle = LabelEncoder()\ny_train_enc = le.fit_transform(df_train[target])\ny_test_enc  = le.transform(df_test[target])\nnum_classes = len(le.classes_)\n\n# ===========================================\n# 3️⃣ Data Loaders (No Sampler needed for SMOTE)\n# ===========================================\n# Since SMOTE already balanced the classes, we don't need a Weighted Sampler. \n# Standard Shuffling is preferred to let the model learn the new distributions.\ntrain_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train_proc, dtype=torch.float32), \n                                               torch.tensor(y_train_enc, dtype=torch.long))\ntest_dataset  = torch.utils.data.TensorDataset(torch.tensor(X_test_proc, dtype=torch.float32), \n                                               torch.tensor(y_test_enc, dtype=torch.long))\n\ntrain_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\ntest_loader  = DataLoader(test_dataset, batch_size=512, shuffle=False)\n\n# ===========================================\n# 4️⃣ Architecture: GMH-ResNet (Gated Multi-Head Residual)\n# ===========================================\nclass GLULayer(nn.Module):\n    \"\"\"Gated Linear Unit for non-linear feature selection.\"\"\"\n    def __init__(self, dim):\n        super().__init__()\n        self.fc = nn.Linear(dim, dim * 2)\n    def forward(self, x):\n        x = self.fc(x)\n        content, gate = x.chunk(2, dim=-1)\n        return content * torch.sigmoid(gate)\n\nclass GatedResidualBlock(nn.Module):\n    def __init__(self, dim, dropout=0.2):\n        super().__init__()\n        self.glu = GLULayer(dim)\n        self.ln = nn.LayerNorm(dim)\n        self.drop = nn.Dropout(dropout)\n    def forward(self, x):\n        return x + self.drop(self.ln(self.glu(x)))\n\nclass GMHResNet(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        self.stem = nn.Linear(input_dim, 512)\n        self.blocks = nn.Sequential(\n            GatedResidualBlock(512),\n            nn.Linear(512, 256),\n            GatedResidualBlock(256),\n            nn.Dropout(0.3)\n        )\n        self.head = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.stem(x))\n        x = self.blocks(x)\n        return self.head(x)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = GMHResNet(X_train_proc.shape[1], num_classes).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\ncriterion = nn.CrossEntropyLoss()\n\n# ===========================================\n# 5️⃣ Training Loop\n# ===========================================\nfor epoch in range(20):\n    model.train()\n    total_loss = 0\n    for X_b, y_b in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        X_b, y_b = X_b.to(device), y_b.to(device)\n        optimizer.zero_grad()\n        logits = model(X_b)\n        loss = criterion(logits, y_b)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Loss: {total_loss/len(train_loader):.4f}\")\n\n# ===========================================\n# 6️⃣ Evaluation\n# ===========================================\nmodel.eval()\nall_p, all_y = [], []\nwith torch.no_grad():\n    for X_b, y_b in test_loader:\n        out = model(X_b.to(device))\n        all_p.extend(torch.argmax(out, dim=1).cpu().numpy())\n        all_y.extend(y_b.numpy())\n\nprint(\"\\n--- Final Q1 Evaluation (SMOTE + GMH-ResNet) ---\")\nprint(classification_report(all_y, all_p, labels=np.arange(num_classes), target_names=le.classes_, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T09:35:20.174230Z","iopub.execute_input":"2026-01-17T09:35:20.174935Z","iopub.status.idle":"2026-01-17T09:37:50.630762Z","shell.execute_reply.started":"2026-01-17T09:35:20.174908Z","shell.execute_reply":"2026-01-17T09:37:50.630078Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 1090/1090 [00:07<00:00, 146.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.1544\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 1090/1090 [00:07<00:00, 149.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0982\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 1090/1090 [00:07<00:00, 147.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0846\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 1090/1090 [00:07<00:00, 149.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0759\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 1090/1090 [00:07<00:00, 147.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0701\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 1090/1090 [00:07<00:00, 150.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0644\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 1090/1090 [00:07<00:00, 146.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0614\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|██████████| 1090/1090 [00:07<00:00, 149.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0582\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|██████████| 1090/1090 [00:07<00:00, 147.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0556\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|██████████| 1090/1090 [00:07<00:00, 149.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0537\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11: 100%|██████████| 1090/1090 [00:07<00:00, 147.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0518\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12: 100%|██████████| 1090/1090 [00:07<00:00, 150.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0490\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13: 100%|██████████| 1090/1090 [00:07<00:00, 148.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0480\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14: 100%|██████████| 1090/1090 [00:07<00:00, 143.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0465\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15: 100%|██████████| 1090/1090 [00:07<00:00, 149.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0450\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16: 100%|██████████| 1090/1090 [00:07<00:00, 146.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0437\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17: 100%|██████████| 1090/1090 [00:07<00:00, 150.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0422\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18: 100%|██████████| 1090/1090 [00:07<00:00, 147.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0405\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19: 100%|██████████| 1090/1090 [00:07<00:00, 151.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0399\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20: 100%|██████████| 1090/1090 [00:07<00:00, 147.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.0392\n\n--- Final Q1 Evaluation (SMOTE + GMH-ResNet) ---\n                 precision    recall  f1-score   support\n\n           back       0.56      1.00      0.72       359\nbuffer_overflow       0.00      0.00      0.00        20\n      ftp_write       0.00      0.00      0.00         3\n   guess_passwd       1.00      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.65      0.99      0.79       141\n           land       1.00      1.00      1.00         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      0.98      0.99      4657\n           nmap       0.97      1.00      0.99        73\n         normal       0.86      0.95      0.90      9711\n           perl       0.11      0.50      0.18         2\n            phf       0.08      0.50      0.13         2\n            pod       0.62      0.88      0.73        41\n      portsweep       0.75      0.90      0.82       157\n        rootkit       0.03      0.08      0.04        13\n          satan       0.67      0.85      0.75       735\n          smurf       0.98      1.00      0.99       665\n            spy       0.00      0.00      0.00         0\n       teardrop       0.23      1.00      0.37        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       0.00      0.00      0.00       944\n\n       accuracy                           0.84     18794\n      macro avg       0.41      0.51      0.41     18794\n   weighted avg       0.84      0.84      0.81     18794\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\n\n# ===========================================\n# 1️⃣ Stable Preprocessing Pipeline\n# ===========================================\n# (Using your provided paths and loading logic)\ndf_train = pd.read_csv(\"/kaggle/input/nsl-kdd-augmented/smote_augmented.csv\") \ndf_test  = pd.read_csv(\"/kaggle/input/nslkdd/KDDTest+.txt\", header=None)\ndf_test.columns = columns # Ensure 'columns' is defined\n\ndf_train['outcome'] = df_train['outcome'].astype(str)\ndf_test['outcome'] = df_test['outcome'].astype(str)\ntrain_labels = set(df_train['outcome'].unique())\ndf_test = df_test[df_test['outcome'].isin(train_labels)].reset_index(drop=True)\n\ntarget = 'outcome'\ncat_cols = ['protocol_type', 'service', 'flag']\nnum_cols = [c for c in columns if c not in cat_cols + [target, 'level']]\n\npreprocessor = ColumnTransformer([\n    ('num', StandardScaler(), num_cols),\n    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n])\n\nX_train_proc = preprocessor.fit_transform(df_train.drop(columns=[target, 'level'], errors='ignore'))\nX_test_proc  = preprocessor.transform(df_test.drop(columns=[target, 'level'], errors='ignore'))\n\nle = LabelEncoder()\ny_train_enc = le.fit_transform(df_train[target])\ny_test_enc  = le.transform(df_test[target])\nnum_classes = len(le.classes_)\n\n# ===========================================\n# 2️⃣ Novel Architecture: Dual-Attention Feature Sieve (DAFS)\n# ===========================================\nclass ChannelAttention(nn.Module):\n    \"\"\"Novelty: Squeeze-and-Excitation to recalibrate feature importance.\"\"\"\n    def __init__(self, channel, reduction=16):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n    def forward(self, x):\n        return x * self.fc(x)\n\nclass DAFSBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.attn = ChannelAttention(dim)\n        self.glu = nn.Sequential(nn.Linear(dim, dim * 2), nn.GLU())\n        self.norm = nn.LayerNorm(dim)\n    def forward(self, x):\n        res = x\n        x = self.attn(x)\n        x = self.glu(x)\n        return self.norm(x + res)\n\nclass DAFSNet(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        self.stem = nn.Linear(input_dim, 1024)\n        self.blocks = nn.Sequential(\n            DAFSBlock(1024),\n            nn.Linear(1024, 512),\n            DAFSBlock(512),\n            nn.Dropout(0.4)\n        )\n        self.head = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        x = F.gelu(self.stem(x))\n        x = self.blocks(x)\n        return self.head(x)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = DAFSNet(X_train_proc.shape[1], num_classes).to(device)\n\n# NOVELTY: Logit Adjustment for Long-Tail Minority Classes\nclass_counts = np.bincount(y_train_enc)\nlogit_adj = torch.tensor(class_counts + 1).float().log().to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n\n# ===========================================\n# 3️⃣ Training Loop with Logit Adjustment\n# ===========================================\ntrain_loader = DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_train_proc, dtype=torch.float32), \n                                                        torch.tensor(y_train_enc, dtype=torch.long)), \n                          batch_size=512, shuffle=True)\n\nfor epoch in range(25):\n    model.train()\n    for X_b, y_b in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        X_b, y_b = X_b.to(device), y_b.to(device)\n        optimizer.zero_grad()\n        logits = model(X_b)\n        \n        # Novelty: Logit Adjustment shifts the decision boundary during loss calculation\n        loss = F.cross_entropy(logits - 0.5 * logit_adj, y_b, label_smoothing=0.1)\n        \n        loss.backward()\n        optimizer.step()\n\n# ===========================================\n# 4️⃣ Evaluation\n# ===========================================\ntest_loader = DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test_proc, dtype=torch.float32), \n                                                       torch.tensor(y_test_enc, dtype=torch.long)), \n                         batch_size=512, shuffle=False)\nmodel.eval()\nall_p, all_y = [], []\nwith torch.no_grad():\n    for X_b, y_b in test_loader:\n        out = model(X_b.to(device))\n        all_p.extend(torch.argmax(out, dim=1).cpu().numpy())\n        all_y.extend(y_b.numpy())\n\nprint(classification_report(all_y, all_p, labels=np.arange(num_classes), target_names=le.classes_, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T09:40:15.448949Z","iopub.execute_input":"2026-01-17T09:40:15.449332Z","iopub.status.idle":"2026-01-17T09:43:23.782491Z","shell.execute_reply.started":"2026-01-17T09:40:15.449301Z","shell.execute_reply":"2026-01-17T09:43:23.781709Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 1090/1090 [00:07<00:00, 146.68it/s]\nEpoch 2: 100%|██████████| 1090/1090 [00:07<00:00, 146.28it/s]\nEpoch 3: 100%|██████████| 1090/1090 [00:07<00:00, 146.25it/s]\nEpoch 4: 100%|██████████| 1090/1090 [00:07<00:00, 149.53it/s]\nEpoch 5: 100%|██████████| 1090/1090 [00:07<00:00, 149.98it/s]\nEpoch 6: 100%|██████████| 1090/1090 [00:07<00:00, 145.52it/s]\nEpoch 7: 100%|██████████| 1090/1090 [00:07<00:00, 149.97it/s]\nEpoch 8: 100%|██████████| 1090/1090 [00:07<00:00, 146.79it/s]\nEpoch 9: 100%|██████████| 1090/1090 [00:07<00:00, 149.51it/s]\nEpoch 10: 100%|██████████| 1090/1090 [00:07<00:00, 146.18it/s]\nEpoch 11: 100%|██████████| 1090/1090 [00:07<00:00, 146.50it/s]\nEpoch 12: 100%|██████████| 1090/1090 [00:07<00:00, 150.00it/s]\nEpoch 13: 100%|██████████| 1090/1090 [00:07<00:00, 149.99it/s]\nEpoch 14: 100%|██████████| 1090/1090 [00:07<00:00, 146.78it/s]\nEpoch 15: 100%|██████████| 1090/1090 [00:07<00:00, 146.72it/s]\nEpoch 16: 100%|██████████| 1090/1090 [00:07<00:00, 148.81it/s]\nEpoch 17: 100%|██████████| 1090/1090 [00:07<00:00, 150.40it/s]\nEpoch 18: 100%|██████████| 1090/1090 [00:07<00:00, 145.81it/s]\nEpoch 19: 100%|██████████| 1090/1090 [00:07<00:00, 145.67it/s]\nEpoch 20: 100%|██████████| 1090/1090 [00:07<00:00, 143.40it/s]\nEpoch 21: 100%|██████████| 1090/1090 [00:07<00:00, 149.59it/s]\nEpoch 22: 100%|██████████| 1090/1090 [00:07<00:00, 150.18it/s]\nEpoch 23: 100%|██████████| 1090/1090 [00:07<00:00, 149.75it/s]\nEpoch 24: 100%|██████████| 1090/1090 [00:07<00:00, 146.54it/s]\nEpoch 25: 100%|██████████| 1090/1090 [00:07<00:00, 144.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"                 precision    recall  f1-score   support\n\n           back       0.56      1.00      0.72       359\nbuffer_overflow       0.02      0.10      0.03        20\n      ftp_write       0.12      0.33      0.18         3\n   guess_passwd       0.75      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.73      0.98      0.84       141\n           land       1.00      1.00      1.00         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      0.99      0.99      4657\n           nmap       0.94      1.00      0.97        73\n         normal       0.86      0.94      0.90      9711\n           perl       0.00      0.00      0.00         2\n            phf       0.02      0.50      0.05         2\n            pod       0.64      0.88      0.74        41\n      portsweep       0.76      0.85      0.80       157\n        rootkit       0.03      0.08      0.04        13\n          satan       0.67      0.89      0.77       735\n          smurf       0.83      0.97      0.89       665\n            spy       0.00      0.00      0.00         0\n       teardrop       0.22      0.83      0.35        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       0.00      0.00      0.00       944\n\n       accuracy                           0.84     18794\n      macro avg       0.40      0.49      0.40     18794\n   weighted avg       0.82      0.84      0.81     18794\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\n\n# ===========================================\n# 1️⃣ Stable Preprocessing (Mapping preserved)\n# ===========================================\n# (Keep your existing data loading and ColumnTransformer logic here)\n# ... [Assuming df_train (SMOTE), df_test, and preprocessor are defined as before] ...\n\nX_train_proc = preprocessor.fit_transform(df_train.drop(columns=[target, 'level'], errors='ignore'))\nX_test_proc  = preprocessor.transform(df_test.drop(columns=[target, 'level'], errors='ignore'))\n\nle = LabelEncoder()\ny_train_enc = le.fit_transform(df_train[target])\ny_test_enc  = le.transform(df_test[target])\nnum_classes = len(le.classes_)\n\n# Calculate class frequencies for Logit Adjustment\nclass_counts = np.bincount(y_train_enc)\nlogit_adj = torch.tensor(class_counts + 1).float().log().to('cuda' if torch.cuda.is_available() else 'cpu')\n\n# ===========================================\n# 2️⃣ Novel Architecture: Multi-Head Sparse-Attention Gated Transformer\n# ===========================================\n\nclass GatedSparseAttention(nn.Module):\n    \"\"\"Novelty: Gating mechanism to prioritize minority class features.\"\"\"\n    def __init__(self, dim, heads=8):\n        super().__init__()\n        self.heads = heads\n        self.scale = (dim // heads) ** -0.5\n        self.qkv = nn.Linear(dim, dim * 3)\n        self.gate = nn.Sequential(\n            nn.Linear(dim, dim),\n            nn.Sigmoid()\n        )\n        self.proj = nn.Linear(dim, dim)\n        self.norm = nn.LayerNorm(dim)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.heads, C // self.heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        # Sparse Attention: Only attend to the most relevant features\n        attn = attn.softmax(dim=-1)\n        \n        out = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        # Apply Gating: Allows minority signals to pass while filtering majority noise\n        g = self.gate(x)\n        return self.norm(x + self.proj(out * g))\n\nclass MSAGTNet(nn.Module):\n    def __init__(self, input_dim, num_classes, embed_dim=256):\n        super().__init__()\n        # Stem: Project high-dim tabular data into Transformer space\n        self.stem = nn.Sequential(\n            nn.Linear(input_dim, embed_dim),\n            nn.GELU(),\n            nn.LayerNorm(embed_dim)\n        )\n        \n        # Transformer Blocks with Sparse Gating\n        self.blocks = nn.Sequential(\n            GatedSparseAttention(embed_dim),\n            GatedSparseAttention(embed_dim),\n            GatedSparseAttention(embed_dim)\n        )\n        \n        # Classification Head\n        self.classifier = nn.Sequential(\n            nn.Linear(embed_dim, 512),\n            nn.GELU(),\n            nn.Dropout(0.4),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x):\n        # Treat features as a sequence of 1 (Tabular style)\n        x = self.stem(x).unsqueeze(1) \n        x = self.blocks(x)\n        x = x.squeeze(1)\n        return self.classifier(x)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = MSAGTNet(X_train_proc.shape[1], num_classes).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.05)\n\n# ===========================================\n# 3️⃣ Training with Logit Adjustment\n# ===========================================\n\ntrain_loader = DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_train_proc, dtype=torch.float32), \n                                                        torch.tensor(y_train_enc, dtype=torch.long)), \n                          batch_size=512, shuffle=True)\n\nfor epoch in range(30):\n    model.train()\n    for X_b, y_b in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        X_b, y_b = X_b.to(device), y_b.to(device)\n        optimizer.zero_grad()\n        \n        logits = model(X_b)\n        \n        # Logit Adjustment: Corrects the decision boundary for rare attacks\n        # Subtracting log-priors makes the model 'work harder' to identify rare classes\n        loss = F.cross_entropy(logits - 0.5 * logit_adj, y_b, label_smoothing=0.1)\n        \n        loss.backward()\n        optimizer.step()\n\n# ===========================================\n# 4️⃣ Evaluation\n# ===========================================\nmodel.eval()\ntest_loader = DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test_proc, dtype=torch.float32), \n                                                       torch.tensor(y_test_enc, dtype=torch.long)), \n                         batch_size=512, shuffle=False)\nall_p, all_y = [], []\nwith torch.no_grad():\n    for X_b, y_b in test_loader:\n        out = model(X_b.to(device))\n        all_p.extend(torch.argmax(out, 1).cpu().numpy())\n        all_y.extend(y_b.numpy())\n\nprint(classification_report(all_y, all_p, labels=np.arange(num_classes), target_names=le.classes_, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T09:46:47.663862Z","iopub.execute_input":"2026-01-17T09:46:47.664247Z","iopub.status.idle":"2026-01-17T09:52:05.929769Z","shell.execute_reply.started":"2026-01-17T09:46:47.664216Z","shell.execute_reply":"2026-01-17T09:52:05.928883Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 1090/1090 [00:10<00:00, 102.29it/s]\nEpoch 2: 100%|██████████| 1090/1090 [00:10<00:00, 103.06it/s]\nEpoch 3: 100%|██████████| 1090/1090 [00:10<00:00, 102.11it/s]\nEpoch 4: 100%|██████████| 1090/1090 [00:10<00:00, 103.00it/s]\nEpoch 5: 100%|██████████| 1090/1090 [00:10<00:00, 102.59it/s]\nEpoch 6: 100%|██████████| 1090/1090 [00:10<00:00, 103.54it/s]\nEpoch 7: 100%|██████████| 1090/1090 [00:10<00:00, 103.30it/s]\nEpoch 8: 100%|██████████| 1090/1090 [00:10<00:00, 102.96it/s]\nEpoch 9: 100%|██████████| 1090/1090 [00:10<00:00, 104.19it/s]\nEpoch 10: 100%|██████████| 1090/1090 [00:10<00:00, 103.46it/s]\nEpoch 11: 100%|██████████| 1090/1090 [00:10<00:00, 103.40it/s]\nEpoch 12: 100%|██████████| 1090/1090 [00:10<00:00, 102.34it/s]\nEpoch 13: 100%|██████████| 1090/1090 [00:10<00:00, 103.40it/s]\nEpoch 14: 100%|██████████| 1090/1090 [00:10<00:00, 103.51it/s]\nEpoch 15: 100%|██████████| 1090/1090 [00:10<00:00, 104.00it/s]\nEpoch 16: 100%|██████████| 1090/1090 [00:10<00:00, 103.67it/s]\nEpoch 17: 100%|██████████| 1090/1090 [00:10<00:00, 103.42it/s]\nEpoch 18: 100%|██████████| 1090/1090 [00:10<00:00, 103.51it/s]\nEpoch 19: 100%|██████████| 1090/1090 [00:10<00:00, 104.23it/s]\nEpoch 20: 100%|██████████| 1090/1090 [00:10<00:00, 102.75it/s]\nEpoch 21: 100%|██████████| 1090/1090 [00:10<00:00, 103.43it/s]\nEpoch 22: 100%|██████████| 1090/1090 [00:10<00:00, 103.88it/s]\nEpoch 23: 100%|██████████| 1090/1090 [00:10<00:00, 104.09it/s]\nEpoch 24: 100%|██████████| 1090/1090 [00:10<00:00, 104.11it/s]\nEpoch 25: 100%|██████████| 1090/1090 [00:10<00:00, 103.67it/s]\nEpoch 26: 100%|██████████| 1090/1090 [00:10<00:00, 103.90it/s]\nEpoch 27: 100%|██████████| 1090/1090 [00:10<00:00, 104.38it/s]\nEpoch 28: 100%|██████████| 1090/1090 [00:10<00:00, 103.06it/s]\nEpoch 29: 100%|██████████| 1090/1090 [00:10<00:00, 104.11it/s]\nEpoch 30: 100%|██████████| 1090/1090 [00:10<00:00, 103.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"                 precision    recall  f1-score   support\n\n           back       0.56      1.00      0.72       359\nbuffer_overflow       0.02      0.20      0.04        20\n      ftp_write       0.17      0.33      0.22         3\n   guess_passwd       1.00      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.77      0.98      0.86       141\n           land       1.00      0.86      0.92         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      0.95      0.97      4657\n           nmap       0.78      1.00      0.87        73\n         normal       0.89      0.95      0.92      9711\n           perl       0.17      1.00      0.29         2\n            phf       0.11      0.50      0.18         2\n            pod       0.51      0.88      0.65        41\n      portsweep       0.75      0.93      0.83       157\n        rootkit       0.01      0.15      0.01        13\n          satan       0.64      0.97      0.77       735\n          smurf       1.00      1.00      1.00       665\n            spy       0.00      0.00      0.00         0\n       teardrop       0.22      0.92      0.36        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       0.25      0.00      0.00       944\n\n       accuracy                           0.84     18794\n      macro avg       0.43      0.55      0.42     18794\n   weighted avg       0.87      0.84      0.81     18794\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\nimport numpy as np\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\n\n# ===========================================\n# 1️⃣ Novel Architecture: Adaptive Class-Attention Transformer (ACAT)\n# ===========================================\n\nclass FeatureAttentionGate(nn.Module):\n    \"\"\"Novelty: Squeezes features to find rare attack signatures.\"\"\"\n    def __init__(self, dim, reduction=4):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(dim, dim // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(dim // reduction, dim, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        # Recalibrates feature importance specifically for minority classes\n        return x * self.fc(x)\n\nclass ACATBlock(nn.Module):\n    def __init__(self, dim, heads=8, dropout=0.2):\n        super().__init__()\n        self.gate = FeatureAttentionGate(dim)\n        self.attn = nn.MultiheadAttention(dim, heads, dropout=dropout, batch_first=True)\n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n        self.ffn = nn.Sequential(\n            nn.Linear(dim, dim * 4),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim * 4, dim)\n        )\n\n    def forward(self, x):\n        # Gating happens before attention to boost rare signals\n        x_g = self.gate(x)\n        attn_out, _ = self.attn(x_g, x_g, x_g)\n        x = self.norm1(x + attn_out)\n        ffn_out = self.ffn(x)\n        x = self.norm2(x + ffn_out)\n        return x\n\nclass ACATNet(nn.Module):\n    def __init__(self, input_dim, num_classes, embed_dim=256):\n        super().__init__()\n        self.embedding_stem = nn.Sequential(\n            nn.Linear(input_dim, embed_dim),\n            nn.LayerNorm(embed_dim),\n            nn.GELU()\n        )\n        \n        # Deep Transformer backbone with Class-Attention blocks\n        self.transformer_blocks = nn.ModuleList([\n            ACATBlock(embed_dim) for _ in range(4)\n        ])\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(embed_dim, 512),\n            nn.LayerNorm(512),\n            nn.GELU(),\n            nn.Dropout(0.4),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x):\n        # Transform tabular vector into a sequence of 1 for Transformer processing\n        x = self.embedding_stem(x).unsqueeze(1) \n        for block in self.transformer_blocks:\n            x = block(x)\n        return self.classifier(x.squeeze(1))\n\n# ===========================================\n# 2️⃣ Focal Logit-Adjusted Loss\n# ===========================================\n\nclass FocalLogitLoss(nn.Module):\n    \"\"\"Novelty: Combines Focal Loss (difficulty) with Logit Adjustment (frequency).\"\"\"\n    def __init__(self, logit_adj, gamma=2.0):\n        super().__init__()\n        self.logit_adj = logit_adj\n        self.gamma = gamma\n\n    def forward(self, logits, targets):\n        # Adjust logits to push decision boundaries away from 'Normal'\n        logits_adj = logits - self.logit_adj\n        \n        ce_loss = F.cross_entropy(logits_adj, targets, reduction='none')\n        pt = torch.exp(-ce_loss)\n        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n        return focal_loss\n\n# ===========================================\n# 3️⃣ Execution Strategy\n# ===========================================\n\n# Calculate Logit Adjustment from your y_train_enc\ncounts = np.bincount(y_train_enc)\nlogit_adj = torch.tensor(np.log(counts + 1e-6)).float().to(device)\n\nmodel = ACATNet(X_train_proc.shape[1], num_classes).to(device)\ncriterion = FocalLogitLoss(logit_adj, gamma=2.5) # Higher gamma focuses more on rare classes\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.05)\n\n# Training Loop\nfor epoch in range(30):\n    model.train()\n    for X_b, y_b in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        X_b, y_b = X_b.to(device), y_b.to(device)\n        optimizer.zero_grad()\n        logits = model(X_b)\n        loss = criterion(logits, y_b)\n        loss.backward()\n        optimizer.step()\n\n# Evaluation\nmodel.eval()\nall_preds, all_labels = [], []\nwith torch.no_grad():\n    for X_b, y_b in test_loader:\n        out = model(X_b.to(device))\n        all_preds.extend(torch.argmax(out, dim=1).cpu().numpy())\n        all_labels.extend(y_b.numpy())\n\nprint(classification_report(all_labels, all_preds, target_names=le.classes_, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T09:52:56.790487Z","iopub.execute_input":"2026-01-17T09:52:56.790829Z","iopub.status.idle":"2026-01-17T10:00:48.791828Z","shell.execute_reply.started":"2026-01-17T09:52:56.790800Z","shell.execute_reply":"2026-01-17T10:00:48.791112Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 1090/1090 [00:15<00:00, 69.45it/s]\nEpoch 2: 100%|██████████| 1090/1090 [00:15<00:00, 69.03it/s]\nEpoch 3: 100%|██████████| 1090/1090 [00:15<00:00, 69.21it/s]\nEpoch 4: 100%|██████████| 1090/1090 [00:15<00:00, 69.14it/s]\nEpoch 5: 100%|██████████| 1090/1090 [00:15<00:00, 69.32it/s]\nEpoch 6: 100%|██████████| 1090/1090 [00:15<00:00, 69.06it/s]\nEpoch 7: 100%|██████████| 1090/1090 [00:15<00:00, 69.32it/s]\nEpoch 8: 100%|██████████| 1090/1090 [00:15<00:00, 69.53it/s]\nEpoch 9: 100%|██████████| 1090/1090 [00:15<00:00, 69.46it/s]\nEpoch 10: 100%|██████████| 1090/1090 [00:15<00:00, 70.02it/s]\nEpoch 11: 100%|██████████| 1090/1090 [00:15<00:00, 68.69it/s]\nEpoch 12: 100%|██████████| 1090/1090 [00:15<00:00, 68.88it/s]\nEpoch 13: 100%|██████████| 1090/1090 [00:15<00:00, 68.34it/s]\nEpoch 14: 100%|██████████| 1090/1090 [00:15<00:00, 69.34it/s]\nEpoch 15: 100%|██████████| 1090/1090 [00:15<00:00, 69.74it/s]\nEpoch 16: 100%|██████████| 1090/1090 [00:15<00:00, 69.16it/s]\nEpoch 17: 100%|██████████| 1090/1090 [00:15<00:00, 69.65it/s]\nEpoch 18: 100%|██████████| 1090/1090 [00:15<00:00, 69.40it/s]\nEpoch 19: 100%|██████████| 1090/1090 [00:15<00:00, 69.14it/s]\nEpoch 20: 100%|██████████| 1090/1090 [00:15<00:00, 69.77it/s]\nEpoch 21: 100%|██████████| 1090/1090 [00:15<00:00, 69.49it/s]\nEpoch 22: 100%|██████████| 1090/1090 [00:15<00:00, 68.93it/s]\nEpoch 23: 100%|██████████| 1090/1090 [00:15<00:00, 69.66it/s]\nEpoch 24: 100%|██████████| 1090/1090 [00:15<00:00, 69.93it/s]\nEpoch 25: 100%|██████████| 1090/1090 [00:15<00:00, 69.54it/s]\nEpoch 26: 100%|██████████| 1090/1090 [00:15<00:00, 68.91it/s]\nEpoch 27: 100%|██████████| 1090/1090 [00:15<00:00, 69.34it/s]\nEpoch 28: 100%|██████████| 1090/1090 [00:15<00:00, 69.64it/s]\nEpoch 29: 100%|██████████| 1090/1090 [00:15<00:00, 69.62it/s]\nEpoch 30: 100%|██████████| 1090/1090 [00:15<00:00, 69.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"                 precision    recall  f1-score   support\n\n           back       0.56      1.00      0.72       359\nbuffer_overflow       0.02      0.15      0.03        20\n      ftp_write       0.00      0.00      0.00         3\n   guess_passwd       0.67      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.75      0.98      0.85       141\n           land       0.88      1.00      0.93         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      0.99      1.00      4657\n           nmap       0.73      1.00      0.84        73\n         normal       0.86      0.90      0.88      9711\n           perl       0.22      1.00      0.36         2\n            phf       0.01      0.50      0.02         2\n            pod       0.51      0.88      0.65        41\n      portsweep       0.72      0.77      0.74       157\n        rootkit       0.07      0.23      0.10        13\n          satan       0.52      0.91      0.66       735\n          smurf       1.00      0.96      0.98       665\n            spy       0.00      0.00      0.00         0\n       teardrop       0.23      1.00      0.37        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       0.50      0.00      0.00       944\n\n       accuracy                           0.82     18794\n      macro avg       0.40      0.53      0.40     18794\n   weighted avg       0.84      0.82      0.79     18794\n\n","output_type":"stream"}],"execution_count":8}]}