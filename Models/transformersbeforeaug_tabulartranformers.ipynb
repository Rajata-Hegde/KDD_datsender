{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":394223,"sourceType":"datasetVersion","datasetId":174616}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-17T06:54:52.880272Z","iopub.execute_input":"2026-01-17T06:54:52.880650Z","iopub.status.idle":"2026-01-17T06:54:54.535749Z","shell.execute_reply.started":"2026-01-17T06:54:52.880609Z","shell.execute_reply":"2026-01-17T06:54:54.534788Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nslkdd/KDDTest+.arff\n/kaggle/input/nslkdd/KDDTest-21.arff\n/kaggle/input/nslkdd/KDDTest1.jpg\n/kaggle/input/nslkdd/KDDTrain+.txt\n/kaggle/input/nslkdd/KDDTrain+_20Percent.txt\n/kaggle/input/nslkdd/KDDTest-21.txt\n/kaggle/input/nslkdd/KDDTest+.txt\n/kaggle/input/nslkdd/KDDTrain+.arff\n/kaggle/input/nslkdd/index.html\n/kaggle/input/nslkdd/KDDTrain+_20Percent.arff\n/kaggle/input/nslkdd/KDDTrain1.jpg\n/kaggle/input/nslkdd/nsl-kdd/KDDTest+.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTest-21.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTest1.jpg\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+_20Percent.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTest-21.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTest+.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+.arff\n/kaggle/input/nslkdd/nsl-kdd/index.html\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+_20Percent.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain1.jpg\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\n\nBATCH_SIZE = 512\nEPOCHS = 30\nLR = 1e-3\nLAMBDA = 0.5\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T06:54:54.537632Z","iopub.execute_input":"2026-01-17T06:54:54.538182Z","iopub.status.idle":"2026-01-17T06:55:00.588490Z","shell.execute_reply.started":"2026-01-17T06:54:54.538142Z","shell.execute_reply":"2026-01-17T06:55:00.587434Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\n\n# ---------------------------\n# Paths\n# ---------------------------\nlocal_path = \"/kaggle/input/nslkdd/\"\ntrain_path = \"/kaggle/input/nsl-kdd-augmented/\"\n\n# ---------------------------\n# Load datasets\n# ---------------------------\n# Note: Use header=0 if your augmented CSV has headers, otherwise None\ndf_train = pd.read_csv(local_path + \"KDDTrain+.txt\", header=None) \ndf_test = pd.read_csv(local_path + \"KDDTest+.txt\", header=None)\n\n# ---------------------------\n# Assign columns to test data\n# ---------------------------\n# Note: The 'outcome' is typically index 41 and 'level' is index 42 in the raw txt\ncolumns = [\n    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land',\n    'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n    'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n    'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count',\n    'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n    'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n    'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n    'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate',\n    'outcome', 'level'\n]\ndf_train.columns = columns\ndf_test.columns = columns\n\n# ---------------------------\n# Separate features and labels\n# ---------------------------\ntarget = \"outcome\"\ncat_cols = ['protocol_type', 'service', 'flag']\n\n# Ensure num_cols uses all available features except target and level\nX_train_aug = df_train.drop(columns=['outcome', 'level'], errors='ignore')\ny_train_aug = df_train['outcome']\n\nX_test = df_test.drop(columns=['outcome', 'level'], errors='ignore')\ny_test = df_test['outcome']\n\nnum_cols = [c for c in X_train_aug.columns if c not in cat_cols]\n\n# ---------------------------\n# Filter test labels that exist in training\n# ---------------------------\n# CRITICAL: We only keep test rows where the outcome exists in the augmented training set\n# If SMOTE reduced classes, we must filter the test set to avoid LabelEncoder errors\ntrain_labels = set(y_train_aug.unique())\nmask = y_test.isin(train_labels)\n\nif mask.sum() == 0:\n    print(\"WARNING: No matching labels between train and test! Checking for string/int mismatch...\")\n    # Standardize to string in case of type mismatch\n    y_train_aug = y_train_aug.astype(str)\n    y_test = y_test.astype(str)\n    train_labels = set(y_train_aug.unique())\n    mask = y_test.isin(train_labels)\n\nX_test = X_test[mask].reset_index(drop=True)\ny_test = y_test[mask].reset_index(drop=True)\n\nprint(f\"Test data retained: {len(X_test)} samples\")\nprint(\"Filtered test label distribution:\\n\", y_test.value_counts())\n\n# ---------------------------\n# Column Transformer: scale numeric + encode categorical\n# ---------------------------\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), num_cols),\n        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n    ]\n)\n\nX_train_processed = preprocessor.fit_transform(X_train_aug)\nX_test_processed  = preprocessor.transform(X_test)\n\n# ---------------------------\n# Encode target labels\n# ---------------------------\nlabel_enc = LabelEncoder()\ny_train_encoded = label_enc.fit_transform(y_train_aug)\ny_test_encoded  = label_enc.transform(y_test)\n\nprint(f\"Processed training shape: {X_train_processed.shape}\")\nprint(f\"Processed test shape: {X_test_processed.shape}\")\n\nclass TabDataset(Dataset):\n    def __init__(self, X_processed, y_encoded, cat_indices, num_indices):\n        # X_processed is the 122-feature matrix from the ColumnTransformer\n        self.x_cat = torch.tensor(X_processed[:, cat_indices], dtype=torch.long)\n        self.x_num = torch.tensor(X_processed[:, num_indices], dtype=torch.float32)\n        self.y = torch.tensor(y_encoded, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return self.x_cat[idx], self.x_num[idx], self.y[idx]\n\n# Identify indices for cat and num features in the 122-column processed matrix\n# Assuming 'preprocessor' is the ColumnTransformer from previous step\ncat_slice = preprocessor.output_indices_['cat']\nnum_slice = preprocessor.output_indices_['num']\n\ntrain_dataset = TabDataset(X_train_processed, y_train_encoded, cat_slice, num_slice)\ntest_dataset = TabDataset(X_test_processed, y_test_encoded, cat_slice, num_slice)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T06:55:00.589584Z","iopub.execute_input":"2026-01-17T06:55:00.590162Z","iopub.status.idle":"2026-01-17T06:55:02.279337Z","shell.execute_reply.started":"2026-01-17T06:55:00.590122Z","shell.execute_reply":"2026-01-17T06:55:02.278458Z"}},"outputs":[{"name":"stdout","text":"Test data retained: 18794 samples\nFiltered test label distribution:\n outcome\nnormal             9711\nneptune            4657\nguess_passwd       1231\nwarezmaster         944\nsatan               735\nsmurf               665\nback                359\nportsweep           157\nipsweep             141\nnmap                 73\npod                  41\nbuffer_overflow      20\nmultihop             18\nrootkit              13\nteardrop             12\nland                  7\nftp_write             3\nperl                  2\nloadmodule            2\nphf                   2\nimap                  1\nName: count, dtype: int64\nProcessed training shape: (125973, 122)\nProcessed test shape: (18794, 122)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# 1. Determine number of classes from the LabelEncoder\nnum_classes = len(label_enc.classes_)\n\n# 2. Identify column indices for cat and num features in the processed matrix\n# categorical_cols were ['protocol_type', 'service', 'flag']\ncat_cols = ['protocol_type', 'service', 'flag']\nnum_cols = [c for c in X_train_aug.columns if c not in cat_cols]\n\n# We need to know which columns in X_train_processed belong to 'num' and 'cat'\n# Based on your ColumnTransformer: num is first, then cat\nnum_feature_count = len(num_cols)\n# Categorical features are One-Hot Encoded, so we find where they start\n# X_train_processed[:, :num_feature_count] is numerical\n# X_train_processed[:, num_feature_count:] is categorical (as one-hot)\n\nclass TabDataset(Dataset):\n    def __init__(self, X, y, num_feat_count):\n        # Convert sparse to dense if necessary\n        if hasattr(X, \"toarray\"): X = X.toarray()\n        \n        # In your ColumnTransformer, numerical columns come first\n        self.x_num = torch.tensor(X[:, :num_feat_count], dtype=torch.float32)\n        \n        # For TabTransformer, categorical features usually need Label Encoding \n        # for embeddings, but since you used OneHotEncoder in the preprocessor,\n        # we will pass the numerical and encoded features as separate tensors.\n        # NOTE: To use nn.Embedding, we need the raw categorical integer codes.\n        self.x_cat = torch.tensor(X[:, num_feat_count:], dtype=torch.float32) \n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return self.x_cat[idx], self.x_num[idx], self.y[idx]\n\nBATCH_SIZE = 512\ntrain_dataset = TabDataset(X_train_processed, y_train_encoded, num_feature_count)\ntest_dataset  = TabDataset(X_test_processed, y_test_encoded, num_feature_count)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T06:57:09.883944Z","iopub.execute_input":"2026-01-17T06:57:09.884370Z","iopub.status.idle":"2026-01-17T06:57:09.959691Z","shell.execute_reply.started":"2026-01-17T06:57:09.884335Z","shell.execute_reply":"2026-01-17T06:57:09.958324Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch.nn as nn\n\nclass TabTransformer(nn.Module):\n    def __init__(self, cat_feature_count, num_features, num_classes, emb_dim=32, depth=4, heads=4, dropout=0.1):\n        super().__init__()\n\n        # Since your preprocessor used OneHotEncoder, cat_feature_count is the \n        # total number of binary columns created (e.g., ~84).\n        # We project these into the embedding space to work with the Transformer.\n        self.cat_projection = nn.Linear(cat_feature_count, emb_dim)\n\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(\n                d_model=emb_dim,\n                nhead=heads,\n                dim_feedforward=emb_dim * 4,\n                dropout=dropout,\n                batch_first=True\n            ),\n            num_layers=depth\n        )\n\n        self.mlp = nn.Sequential(\n            nn.Linear(emb_dim + num_features, 256),\n            nn.ReLU(),\n            nn.BatchNorm1d(256),\n            nn.Dropout(0.3),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x_cat, x_num):\n        # x_cat is [batch, cat_onehot_dim]\n        x = self.cat_projection(x_cat) # [batch, emb_dim]\n        x = x.unsqueeze(1)             # [batch, 1, emb_dim] for Transformer\n        x = self.transformer(x)\n        x = x.squeeze(1)               # [batch, emb_dim]\n        \n        x = torch.cat([x, x_num], dim=1)\n        return self.mlp(x)\n\n# Initialization\ncat_onehot_dim = X_train_processed.shape[1] - num_feature_count\nmodel = TabTransformer(cat_onehot_dim, num_feature_count, num_classes).to(DEVICE)\n\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR)\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T06:57:14.306668Z","iopub.execute_input":"2026-01-17T06:57:14.307106Z","iopub.status.idle":"2026-01-17T06:57:18.143446Z","shell.execute_reply.started":"2026-01-17T06:57:14.307050Z","shell.execute_reply":"2026-01-17T06:57:18.142437Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report\n\ndef train_epoch(model, loader):\n    model.train()\n    total_loss = 0\n    for x_cat, x_num, y in loader:\n        x_cat, x_num, y = x_cat.to(DEVICE), x_num.to(DEVICE), y.to(DEVICE)\n        \n        logits = model(x_cat, x_num)\n        \n        # Consistency Regularization (Optional, based on your previous code)\n        noise = torch.randn_like(x_num) * 0.05\n        logits_aug = model(x_cat, x_num + noise)\n        \n        loss_cls = criterion(logits, y)\n        loss_cons = torch.mean((logits - logits_aug)**2)\n        loss = loss_cls + LAMBDA * loss_cons\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(loader)\n\ndef evaluate(model, loader):\n    model.eval()\n    all_preds, all_true = [], []\n    with torch.no_grad():\n        for x_cat, x_num, y in loader:\n            x_cat, x_num = x_cat.to(DEVICE), x_num.to(DEVICE)\n            logits = model(x_cat, x_num)\n            preds = torch.argmax(logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_true.extend(y.numpy())\n\n    print(\"Accuracy:\", accuracy_score(all_true, all_preds))\n    print(classification_report(all_true, all_preds, target_names=label_enc.classes_))\n\n# Execution Loop\nfor epoch in range(EPOCHS):\n    loss = train_epoch(model, train_loader)\n    print(f\"Epoch {epoch+1}: Loss = {loss:.4f}\")\n\nevaluate(model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T06:58:06.430756Z","iopub.execute_input":"2026-01-17T06:58:06.431456Z","iopub.status.idle":"2026-01-17T07:05:44.234284Z","shell.execute_reply.started":"2026-01-17T06:58:06.431413Z","shell.execute_reply":"2026-01-17T07:05:44.233038Z"}},"outputs":[{"name":"stdout","text":"Epoch 1: Loss = 0.3150\nEpoch 2: Loss = 0.1244\nEpoch 3: Loss = 0.1064\nEpoch 4: Loss = 0.0969\nEpoch 5: Loss = 0.0919\nEpoch 6: Loss = 0.0887\nEpoch 7: Loss = 0.0867\nEpoch 8: Loss = 0.0829\nEpoch 9: Loss = 0.0792\nEpoch 10: Loss = 0.0775\nEpoch 11: Loss = 0.0769\nEpoch 12: Loss = 0.0751\nEpoch 13: Loss = 0.0737\nEpoch 14: Loss = 0.0723\nEpoch 15: Loss = 0.0712\nEpoch 16: Loss = 0.0723\nEpoch 17: Loss = 0.0711\nEpoch 18: Loss = 0.0696\nEpoch 19: Loss = 0.0685\nEpoch 20: Loss = 0.0677\nEpoch 21: Loss = 0.0668\nEpoch 22: Loss = 0.0663\nEpoch 23: Loss = 0.0663\nEpoch 24: Loss = 0.0658\nEpoch 25: Loss = 0.0646\nEpoch 26: Loss = 0.0629\nEpoch 27: Loss = 0.0641\nEpoch 28: Loss = 0.0648\nEpoch 29: Loss = 0.0626\nEpoch 30: Loss = 0.0633\nAccuracy: 0.8375545386825582\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/3888154234.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}: Loss = {loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_55/3888154234.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_enc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Execution Loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2691\u001b[0m             )\n\u001b[1;32m   2692\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2694\u001b[0m                 \u001b[0;34m\"Number of classes, {0}, does not match size of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m                 \u001b[0;34m\"target_names, {1}. Try specifying the labels \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Number of classes, 22, does not match size of target_names, 23. Try specifying the labels parameter"],"ename":"ValueError","evalue":"Number of classes, 22, does not match size of target_names, 23. Try specifying the labels parameter","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report\n\ndef evaluate(model, loader):\n    model.eval()\n    all_preds, all_true = [], []\n    with torch.no_grad():\n        for x_cat, x_num, y in loader:\n            x_cat, x_num = x_cat.to(DEVICE), x_num.to(DEVICE)\n            logits = model(x_cat, x_num)\n            preds = torch.argmax(logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_true.extend(y.numpy())\n\n    print(\"Accuracy:\", accuracy_score(all_true, all_preds))\n    \n    # FIX: Pass labels=range(len(label_enc.classes_)) to match target_names\n    print(classification_report(\n        all_true, \n        all_preds, \n        labels=range(len(label_enc.classes_)), \n        target_names=label_enc.classes_\n    ))\n\nevaluate(model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T07:10:23.912805Z","iopub.execute_input":"2026-01-17T07:10:23.913240Z","iopub.status.idle":"2026-01-17T07:10:24.378704Z","shell.execute_reply.started":"2026-01-17T07:10:23.913207Z","shell.execute_reply":"2026-01-17T07:10:24.377803Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.8375545386825582\n                 precision    recall  f1-score   support\n\n           back       0.99      0.92      0.96       359\nbuffer_overflow       0.82      0.45      0.58        20\n      ftp_write       0.50      0.33      0.40         3\n   guess_passwd       0.00      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.96      0.98      0.97       141\n           land       1.00      0.86      0.92         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      1.00      1.00      4657\n           nmap       0.97      0.99      0.98        73\n         normal       0.80      0.93      0.86      9711\n           perl       0.00      0.00      0.00         2\n            phf       0.00      0.00      0.00         2\n            pod       0.69      0.88      0.77        41\n      portsweep       0.79      0.96      0.86       157\n        rootkit       0.00      0.00      0.00        13\n          satan       0.50      0.83      0.62       735\n          smurf       1.00      1.00      1.00       665\n            spy       0.00      0.00      0.00         0\n       teardrop       0.21      0.83      0.34        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       1.00      0.02      0.04       944\n\n       accuracy                           0.84     18794\n      macro avg       0.49      0.48      0.45     18794\n   weighted avg       0.80      0.84      0.79     18794\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","output_type":"stream"}],"execution_count":10}]}