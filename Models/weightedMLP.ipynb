{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14523807,"sourceType":"datasetVersion","datasetId":9276062},{"sourceId":394223,"sourceType":"datasetVersion","datasetId":174616}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-17T07:37:41.823671Z","iopub.execute_input":"2026-01-17T07:37:41.823908Z","iopub.status.idle":"2026-01-17T07:37:42.111038Z","shell.execute_reply.started":"2026-01-17T07:37:41.823886Z","shell.execute_reply":"2026-01-17T07:37:42.110321Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nsl-kdd-augmented/smote_augmented.csv\n/kaggle/input/nslkdd/KDDTest+.arff\n/kaggle/input/nslkdd/KDDTest-21.arff\n/kaggle/input/nslkdd/KDDTest1.jpg\n/kaggle/input/nslkdd/KDDTrain+.txt\n/kaggle/input/nslkdd/KDDTrain+_20Percent.txt\n/kaggle/input/nslkdd/KDDTest-21.txt\n/kaggle/input/nslkdd/KDDTest+.txt\n/kaggle/input/nslkdd/KDDTrain+.arff\n/kaggle/input/nslkdd/index.html\n/kaggle/input/nslkdd/KDDTrain+_20Percent.arff\n/kaggle/input/nslkdd/KDDTrain1.jpg\n/kaggle/input/nslkdd/nsl-kdd/KDDTest+.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTest-21.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTest1.jpg\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+_20Percent.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTest-21.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTest+.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+.arff\n/kaggle/input/nslkdd/nsl-kdd/index.html\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+_20Percent.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain1.jpg\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ===========================================\n# 0️⃣ Imports\n# ===========================================\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\n# ===========================================\n# 1️⃣ Load & Correct Column Assignment\n# ===========================================\nlocal_path = \"/kaggle/input/nslkdd/\" \n\ncolumns = [\n    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land',\n    'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n    'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n    'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count',\n    'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n    'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n    'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n    'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate',\n    'outcome', 'level'  # Index 41 is the attack type (outcome)\n]\n\ndf_train = pd.read_csv(local_path + \"KDDTrain+.txt\", header=None, names=columns)\ndf_test  = pd.read_csv(local_path + \"KDDTest+.txt\", header=None, names=columns)\n\n# Standardize to string\ndf_train['outcome'] = df_train['outcome'].astype(str)\ndf_test['outcome'] = df_test['outcome'].astype(str)\n\n# Filter test set to only include classes present in training\ntrain_labels = set(df_train['outcome'].unique())\ndf_test = df_test[df_test['outcome'].isin(train_labels)].reset_index(drop=True)\n\n# ===========================================\n# 2️⃣ Preprocessing\n# ===========================================\ntarget = 'outcome'\ncat_cols = ['protocol_type', 'service', 'flag']\nnum_cols = [c for c in columns if c not in cat_cols + [target, 'level']]\n\npreprocessor = ColumnTransformer([\n    ('num', StandardScaler(), num_cols),\n    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n])\n\nX_train_proc = preprocessor.fit_transform(df_train.drop(columns=[target, 'level']))\nX_test_proc  = preprocessor.transform(df_test.drop(columns=[target, 'level']))\n\nle = LabelEncoder()\ny_train_enc = le.fit_transform(df_train[target])\ny_test_enc  = le.transform(df_test[target])\nnum_classes = len(le.classes_)\n\n# ===========================================\n# 3️⃣ Balanced Sampling & Weighting\n# ===========================================\nclass_counts = np.bincount(y_train_enc)\n# USE SQUARE ROOT SMOOTHING: 1/sqrt(n) is much more stable than 1/n\nclass_weights = 1.0 / np.sqrt(class_counts + 1)\nsample_weights = class_weights[y_train_enc]\n\nsampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(y_train_enc), replacement=True)\n\ntrain_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train_proc, dtype=torch.float32), \n                                               torch.tensor(y_train_enc, dtype=torch.long))\ntest_dataset  = torch.utils.data.TensorDataset(torch.tensor(X_test_proc, dtype=torch.float32), \n                                               torch.tensor(y_test_enc, dtype=torch.long))\n\ntrain_loader = DataLoader(train_dataset, batch_size=256, sampler=sampler)\ntest_loader  = DataLoader(test_dataset, batch_size=256, shuffle=False)\n\n# ===========================================\n# 4️⃣ Model & Loss\n# ===========================================\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Normalized weights for the Loss Function\nloss_weights = torch.tensor(class_weights / class_weights.sum() * num_classes, dtype=torch.float32).to(device)\ncriterion = nn.CrossEntropyLoss(weight=loss_weights)\n\nclass RobustMLP(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.LayerNorm(512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, num_classes)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nmodel = RobustMLP(X_train_proc.shape[1], num_classes).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n\n# ===========================================\n# 5️⃣ Training Loop\n# ===========================================\nEPOCHS = 15\nfor epoch in range(EPOCHS):\n    model.train()\n    running_loss = 0.0\n    for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(X_batch)\n        loss = criterion(outputs, y_batch)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    \n    print(f\"Epoch {epoch+1} Loss: {running_loss/len(train_loader):.4f}\")\n\n# ===========================================\n# 6️⃣ Evaluation\n# ===========================================\nmodel.eval()\nall_preds, all_labels = [], []\n\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        outputs = model(X_batch.to(device))\n        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n        all_preds.extend(preds)\n        all_labels.extend(y_batch.numpy())\n\nprint(\"\\n--- Evaluation Report ---\")\n# Use the labels parameter to ensure all classes are reported correctly\nprint(classification_report(all_labels, all_preds, \n                            labels=np.arange(len(le.classes_)), \n                            target_names=le.classes_, \n                            zero_division=0))\n\n# Quick Accuracy Check\ncorrect = (np.array(all_preds) == np.array(all_labels)).sum()\nprint(f\"Total Accuracy: {correct / len(all_labels):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T07:51:05.790120Z","iopub.execute_input":"2026-01-17T07:51:05.790822Z","iopub.status.idle":"2026-01-17T07:51:33.030563Z","shell.execute_reply.started":"2026-01-17T07:51:05.790790Z","shell.execute_reply":"2026-01-17T07:51:33.029912Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 493/493 [00:01<00:00, 295.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Loss: 0.2842\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 493/493 [00:01<00:00, 258.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Loss: 0.0885\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 493/493 [00:01<00:00, 288.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Loss: 0.0647\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 493/493 [00:01<00:00, 292.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Loss: 0.0603\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 493/493 [00:01<00:00, 290.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Loss: 0.0468\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 493/493 [00:01<00:00, 260.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 Loss: 0.0472\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 493/493 [00:01<00:00, 290.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 Loss: 0.0484\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|██████████| 493/493 [00:01<00:00, 291.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 Loss: 0.0392\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|██████████| 493/493 [00:01<00:00, 266.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 Loss: 0.0488\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|██████████| 493/493 [00:01<00:00, 289.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 Loss: 0.0436\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11: 100%|██████████| 493/493 [00:01<00:00, 287.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 Loss: 0.0421\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12: 100%|██████████| 493/493 [00:01<00:00, 289.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 Loss: 0.0356\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13: 100%|██████████| 493/493 [00:01<00:00, 264.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 Loss: 0.0345\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14: 100%|██████████| 493/493 [00:01<00:00, 290.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 Loss: 0.0402\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15: 100%|██████████| 493/493 [00:01<00:00, 286.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15 Loss: 0.0346\n\n--- Evaluation Report ---\n                 precision    recall  f1-score   support\n\n           back       0.97      1.00      0.98       359\nbuffer_overflow       0.64      0.45      0.53        20\n      ftp_write       0.01      0.67      0.02         3\n   guess_passwd       0.61      0.02      0.03      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.80      0.99      0.88       141\n           land       1.00      1.00      1.00         7\n     loadmodule       0.50      1.00      0.67         2\n       multihop       0.02      0.11      0.03        18\n        neptune       1.00      1.00      1.00      4657\n           nmap       1.00      0.99      0.99        73\n         normal       0.87      0.96      0.91      9711\n           perl       0.50      0.50      0.50         2\n            phf       0.33      0.50      0.40         2\n            pod       0.72      0.95      0.82        41\n      portsweep       0.67      0.93      0.78       157\n        rootkit       0.01      0.23      0.02        13\n          satan       0.77      0.85      0.81       735\n          smurf       0.99      1.00      0.99       665\n            spy       0.00      0.00      0.00         0\n       teardrop       0.24      1.00      0.39        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       0.71      0.15      0.25       944\n\n       accuracy                           0.86     18794\n      macro avg       0.54      0.62      0.52     18794\n   weighted avg       0.87      0.86      0.84     18794\n\nTotal Accuracy: 0.8637\n","output_type":"stream"}],"execution_count":7}]}