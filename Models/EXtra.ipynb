{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":394223,"sourceType":"datasetVersion","datasetId":174616},{"sourceId":14523807,"sourceType":"datasetVersion","datasetId":9276062}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-17T11:57:30.176866Z","iopub.execute_input":"2026-01-17T11:57:30.177192Z","iopub.status.idle":"2026-01-17T11:57:30.207055Z","shell.execute_reply.started":"2026-01-17T11:57:30.177145Z","shell.execute_reply":"2026-01-17T11:57:30.206368Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nsl-kdd-augmented/smote_augmented.csv\n/kaggle/input/nslkdd/KDDTest+.arff\n/kaggle/input/nslkdd/KDDTest-21.arff\n/kaggle/input/nslkdd/KDDTest1.jpg\n/kaggle/input/nslkdd/KDDTrain+.txt\n/kaggle/input/nslkdd/KDDTrain+_20Percent.txt\n/kaggle/input/nslkdd/KDDTest-21.txt\n/kaggle/input/nslkdd/KDDTest+.txt\n/kaggle/input/nslkdd/KDDTrain+.arff\n/kaggle/input/nslkdd/index.html\n/kaggle/input/nslkdd/KDDTrain+_20Percent.arff\n/kaggle/input/nslkdd/KDDTrain1.jpg\n/kaggle/input/nslkdd/nsl-kdd/KDDTest+.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTest-21.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTest1.jpg\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+_20Percent.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTest-21.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTest+.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+.arff\n/kaggle/input/nslkdd/nsl-kdd/index.html\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+_20Percent.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain1.jpg\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\nfrom sklearn.preprocessing import RobustScaler, LabelEncoder\nfrom sklearn.metrics import classification_report, f1_score\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom tqdm import tqdm\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {DEVICE}\")\n\ncolumns = [\n    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land',\n    'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n    'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n    'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count',\n    'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n    'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n    'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n    'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate',\n    'outcome', 'level'\n]\n\n# ===========================================\n# CRITICAL: LOAD ORIGINAL DATA FIRST\n# ===========================================\nprint(\"Loading ORIGINAL unbalanced training data...\")\n\n# You'll need to load your original training data before SMOTE\n# If you don't have it, we can use a subset of SMOTE data with duplicates removed\n# For now, I'll assume you have the original file - adjust path as needed\ntry:\n    df_train_orig = pd.read_csv(\"/kaggle/input/nslkdd/KDDTrain+.txt\", header=None)\n    df_train_orig.columns = columns\n    print(\"✓ Loaded original training data\")\n    use_original = True\nexcept:\n    print(\"⚠ Original data not found, will use SMOTE with weighting\")\n    use_original = False\n\ndf_train_smote = pd.read_csv(\"/kaggle/input/nsl-kdd-augmented/smote_augmented.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/nslkdd/KDDTest+.txt\", header=None)\ndf_test.columns = columns\n\n# Filter test to only classes in training\nif use_original:\n    train_labels = set(df_train_orig['outcome'].unique())\nelse:\n    train_labels = set(df_train_smote['outcome'].unique())\n    \ndf_test = df_test[df_test['outcome'].isin(train_labels)].reset_index(drop=True)\n\ncat_cols = ['protocol_type', 'service', 'flag']\nnum_cols = [c for c in df_train_smote.columns if c not in cat_cols + ['outcome', 'level']]\n\n# ===========================================\n# PREPROCESSING\n# ===========================================\n\ndef preprocess_data(df, le_cat_dict=None, le_target=None, scaler=None, fit=False):\n    \"\"\"Preprocess data with optional fitting\"\"\"\n    df = df.copy()\n    \n    # Categorical encoding\n    if fit:\n        le_cat_dict = {}\n        cat_dims = []\n        for col in cat_cols:\n            le_c = LabelEncoder()\n            df[col] = le_c.fit_transform(df[col].astype(str))\n            le_cat_dict[col] = le_c\n            cat_dims.append(len(le_c.classes_))\n    else:\n        cat_dims = []\n        for col in cat_cols:\n            train_classes = {cls: i for i, cls in enumerate(le_cat_dict[col].classes_)}\n            df[col] = df[col].map(lambda x: train_classes.get(str(x), 0))\n            cat_dims.append(len(le_cat_dict[col].classes_))\n    \n    # Numerical scaling\n    if fit:\n        scaler = RobustScaler()\n        X_num = scaler.fit_transform(df[num_cols]).astype(np.float32)\n    else:\n        X_num = scaler.transform(df[num_cols]).astype(np.float32)\n    \n    # Target encoding\n    if fit:\n        le_target = LabelEncoder()\n        y = le_target.fit_transform(df['outcome'])\n    else:\n        y = le_target.transform(df['outcome'])\n    \n    X_cat = df[cat_cols].values\n    \n    if fit:\n        return X_cat, X_num, y, cat_dims, le_cat_dict, le_target, scaler\n    else:\n        return X_cat, X_num, y\n\n# Fit on SMOTE data (has all classes)\nX_cat_smote, X_num_smote, y_smote, cat_dims, le_cat_dict, le_target, scaler = \\\n    preprocess_data(df_train_smote, fit=True)\n\nif use_original:\n    X_cat_orig, X_num_orig, y_orig = preprocess_data(df_train_orig, le_cat_dict, le_target, scaler, fit=False)\n    \nX_cat_test, X_num_test, y_test = preprocess_data(df_test, le_cat_dict, le_target, scaler, fit=False)\n\nnum_classes = len(le_target.classes_)\n\nif use_original:\n    orig_class_counts = np.bincount(y_orig, minlength=num_classes)\n    print(\"\\nOriginal data distribution:\")\n    for i, (name, count) in enumerate(zip(le_target.classes_, orig_class_counts)):\n        print(f\"{name:20s}: {count:8d}\")\n\ntest_class_counts = np.bincount(y_test, minlength=num_classes)\nprint(f\"\\nTest samples: {len(y_test)}\")\n\n# ===========================================\n# ENSEMBLE ARCHITECTURE\n# ===========================================\n\nclass AttentiveResidualBlock(nn.Module):\n    \"\"\"Residual block with channel attention\"\"\"\n    def __init__(self, dim, dropout=0.3):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, dim * 2),\n            nn.BatchNorm1d(dim * 2),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim * 2, dim),\n            nn.BatchNorm1d(dim)\n        )\n        # Channel attention\n        self.attention = nn.Sequential(\n            nn.Linear(dim, dim // 4),\n            nn.ReLU(),\n            nn.Linear(dim // 4, dim),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x):\n        residual = x\n        out = self.net(x)\n        # Apply attention\n        attn = self.attention(out)\n        out = out * attn\n        return F.gelu(out + residual)\n\nclass HybridNet(nn.Module):\n    \"\"\"\n    Architecture optimized for hybrid training:\n    1. Learn from real data first\n    2. Fine-tune with synthetic augmentation\n    \"\"\"\n    def __init__(self, cat_dims, num_feat_dim, num_classes, emb_dim=48, hidden_dim=384):\n        super().__init__()\n        \n        # Categorical embeddings\n        self.embeddings = nn.ModuleList([\n            nn.Embedding(d, emb_dim) for d in cat_dims\n        ])\n        cat_total = emb_dim * len(cat_dims)\n        \n        # Input layers\n        self.cat_proj = nn.Sequential(\n            nn.BatchNorm1d(cat_total),\n            nn.Linear(cat_total, hidden_dim // 2),\n            nn.BatchNorm1d(hidden_dim // 2),\n            nn.GELU(),\n            nn.Dropout(0.3)\n        )\n        \n        self.num_proj = nn.Sequential(\n            nn.BatchNorm1d(num_feat_dim),\n            nn.Linear(num_feat_dim, hidden_dim // 2),\n            nn.BatchNorm1d(hidden_dim // 2),\n            nn.GELU(),\n            nn.Dropout(0.3)\n        )\n        \n        # Fusion\n        self.fusion = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.GELU(),\n            nn.Dropout(0.3)\n        )\n        \n        # Deep blocks with attention\n        self.blocks = nn.ModuleList([\n            AttentiveResidualBlock(hidden_dim, dropout=0.35) for _ in range(5)\n        ])\n        \n        # Multi-head output\n        self.head1 = nn.Sequential(\n            nn.Dropout(0.4),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.BatchNorm1d(hidden_dim // 2),\n            nn.GELU(),\n            nn.Dropout(0.4),\n            nn.Linear(hidden_dim // 2, num_classes)\n        )\n        \n        self.head2 = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(hidden_dim, num_classes)\n        )\n        \n    def forward(self, x_cat, x_num, return_features=False):\n        # Process inputs\n        cat_emb = torch.cat([emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)], dim=1)\n        cat_feat = self.cat_proj(cat_emb)\n        num_feat = self.num_proj(x_num)\n        \n        # Fuse\n        x = torch.cat([cat_feat, num_feat], dim=1)\n        x = self.fusion(x)\n        \n        # Deep processing\n        for block in self.blocks:\n            x = block(x)\n        \n        # Two heads for ensemble\n        out1 = self.head1(x)\n        out2 = self.head2(x)\n        \n        if return_features:\n            return out1, out2, x\n        return out1, out2\n\n# ===========================================\n# ADVANCED FOCAL LOSS\n# ===========================================\n\nclass AdaptiveFocalLoss(nn.Module):\n    \"\"\"Focal loss with adaptive gamma based on class difficulty\"\"\"\n    def __init__(self, class_counts, base_gamma=2.0):\n        super().__init__()\n        # Compute class weights\n        weights = 1.0 / np.power(class_counts, 0.5)\n        weights = weights / weights.min()\n        self.weights = torch.tensor(weights, dtype=torch.float32)\n        self.base_gamma = base_gamma\n        \n    def forward(self, pred, target):\n        self.weights = self.weights.to(pred.device)\n        \n        ce_loss = F.cross_entropy(pred, target, reduction='none', weight=self.weights)\n        pt = torch.exp(-ce_loss)\n        \n        # Adaptive gamma: harder samples get higher gamma\n        gamma = self.base_gamma + (1 - pt)\n        focal_loss = ((1 - pt) ** gamma) * ce_loss\n        \n        return focal_loss.mean()\n\n# ===========================================\n# DATASET\n# ===========================================\n\nclass NSLDataset(Dataset):\n    def __init__(self, c, n, y, is_synthetic=False):\n        self.c = torch.tensor(c, dtype=torch.long)\n        self.n = torch.tensor(n, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.is_synthetic = is_synthetic\n    \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, i):\n        return self.c[i], self.n[i], self.y[i]\n\n# ===========================================\n# HYBRID TRAINING STRATEGY\n# ===========================================\n\ndef train_phase(model, train_loader, criterion, optimizer, scheduler, phase_name):\n    \"\"\"Train for one phase\"\"\"\n    model.train()\n    total_loss = 0\n    \n    pbar = tqdm(train_loader, desc=phase_name)\n    for xc, xn, y in pbar:\n        xc, xn, y = xc.to(DEVICE), xn.to(DEVICE), y.to(DEVICE)\n        \n        optimizer.zero_grad()\n        out1, out2 = model(xc, xn)\n        \n        # Loss on both heads\n        loss1 = criterion(out1, y)\n        loss2 = criterion(out2, y)\n        \n        # Consistency loss\n        loss_consist = F.kl_div(F.log_softmax(out1, dim=1), F.softmax(out2, dim=1), reduction='batchmean')\n        \n        loss = loss1 + 0.5 * loss2 + 0.2 * loss_consist\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        if scheduler is not None:\n            scheduler.step()\n        \n        total_loss += loss.item()\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n    \n    return total_loss / len(train_loader)\n\ndef evaluate(model, test_loader, le_target):\n    \"\"\"Evaluate model\"\"\"\n    model.eval()\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for xc, xn, y in test_loader:\n            xc, xn = xc.to(DEVICE), xn.to(DEVICE)\n            out1, out2 = model(xc, xn)\n            # Ensemble both heads\n            ensemble = (out1 + out2) / 2\n            preds = torch.argmax(ensemble, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(y.numpy())\n    \n    macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n    weighted_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n    \n    return macro_f1, weighted_f1, all_preds, all_labels\n\n# ===========================================\n# MAIN TRAINING\n# ===========================================\n\nmodel = HybridNet(\n    cat_dims=cat_dims,\n    num_feat_dim=X_num_smote.shape[1],\n    num_classes=num_classes,\n    emb_dim=48,\n    hidden_dim=384\n).to(DEVICE)\n\nprint(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Create datasets\ntest_dataset = NSLDataset(X_cat_test, X_num_test, y_test)\ntest_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False, num_workers=2)\n\nbest_macro_f1 = 0.0\nbest_model_state = None\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"HYBRID TRAINING STRATEGY\")\nprint(\"=\"*70)\n\nif use_original:\n    # PHASE 1: Train on original real data with extreme focal loss\n    print(\"\\n>>> PHASE 1: Learning from REAL data (30 epochs)\")\n    print(\"=\"*70)\n    \n    orig_dataset = NSLDataset(X_cat_orig, X_num_orig, y_orig, is_synthetic=False)\n    orig_loader = DataLoader(orig_dataset, batch_size=256, shuffle=True, num_workers=2)\n    \n    criterion_phase1 = AdaptiveFocalLoss(orig_class_counts, base_gamma=3.0)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-3)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30, eta_min=1e-6)\n    \n    for epoch in range(30):\n        train_loss = train_phase(model, orig_loader, criterion_phase1, optimizer, scheduler, \n                                 f\"Phase 1 - Epoch {epoch+1}/30\")\n        \n        if (epoch + 1) % 5 == 0:\n            macro_f1, weighted_f1, _, _ = evaluate(model, test_loader, le_target)\n            print(f\"  Validation - Macro F1: {macro_f1:.4f}, Weighted F1: {weighted_f1:.4f}\")\n            \n            if macro_f1 > best_macro_f1:\n                best_macro_f1 = macro_f1\n                best_model_state = model.state_dict().copy()\n\n# PHASE 2: Fine-tune with SMOTE data\nprint(\"\\n>>> PHASE 2: Fine-tuning with SMOTE augmentation (40 epochs)\")\nprint(\"=\"*70)\n\nsmote_dataset = NSLDataset(X_cat_smote, X_num_smote, y_smote, is_synthetic=True)\n\n# Mix original (if available) and SMOTE with 30-70 ratio\nif use_original and len(y_orig) > 0:\n    # Sample original data\n    orig_indices = np.random.choice(len(y_orig), size=min(len(y_orig), len(y_smote) // 2), replace=False)\n    orig_subset = NSLDataset(X_cat_orig[orig_indices], X_num_orig[orig_indices], \n                            y_orig[orig_indices], is_synthetic=False)\n    mixed_dataset = ConcatDataset([orig_subset, smote_dataset])\n    print(f\"Mixed dataset: {len(orig_subset)} original + {len(smote_dataset)} SMOTE\")\nelse:\n    mixed_dataset = smote_dataset\n\nmixed_loader = DataLoader(mixed_dataset, batch_size=512, shuffle=True, num_workers=2)\n\nsmote_class_counts = np.bincount(y_smote, minlength=num_classes)\ncriterion_phase2 = AdaptiveFocalLoss(smote_class_counts, base_gamma=2.0)\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-3)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=40, eta_min=1e-7)\n\npatience = 15\npatience_counter = 0\n\nfor epoch in range(40):\n    train_loss = train_phase(model, mixed_loader, criterion_phase2, optimizer, scheduler,\n                             f\"Phase 2 - Epoch {epoch+1}/40\")\n    \n    macro_f1, weighted_f1, all_preds, all_labels = evaluate(model, test_loader, le_target)\n    \n    per_class_f1 = f1_score(all_labels, all_preds, average=None, zero_division=0, labels=range(num_classes))\n    testable = per_class_f1[test_class_counts > 0]\n    good_classes = (testable > 0.3).sum()\n    \n    print(f\"\\n  Train Loss: {train_loss:.4f}\")\n    print(f\"  Macro F1: {macro_f1:.4f} | Weighted F1: {weighted_f1:.4f}\")\n    print(f\"  Classes F1 > 0.3: {good_classes}/{len(testable)} | Mean: {testable.mean():.4f}\")\n    \n    if macro_f1 > best_macro_f1:\n        best_macro_f1 = macro_f1\n        best_model_state = model.state_dict().copy()\n        patience_counter = 0\n        print(f\"  ✓ New best Macro F1: {best_macro_f1:.4f}\")\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(f\"\\nEarly stopping\")\n            break\n\n# ===========================================\n# FINAL EVALUATION\n# ===========================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"FINAL EVALUATION\")\nprint(\"=\"*70)\n\nmodel.load_state_dict(best_model_state)\n_, _, all_preds, all_labels = evaluate(model, test_loader, le_target)\n\nlabels_in_test = sorted(list(set(all_labels)))\ntarget_names_filtered = [le_target.classes_[i] for i in labels_in_test]\n\nprint(\"\\nCLASSIFICATION REPORT:\")\nprint(\"=\"*70)\nprint(classification_report(all_labels, all_preds, labels=labels_in_test,\n                          target_names=target_names_filtered, zero_division=0, digits=4))\n\nprint(\"\\n\" + \"=\"*70)\nprint(f\"BEST MACRO F1: {best_macro_f1:.4f}\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T11:57:30.368136Z","iopub.execute_input":"2026-01-17T11:57:30.368411Z","iopub.status.idle":"2026-01-17T12:07:39.640660Z","shell.execute_reply.started":"2026-01-17T11:57:30.368388Z","shell.execute_reply":"2026-01-17T12:07:39.639708Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading ORIGINAL unbalanced training data...\n✓ Loaded original training data\n\nOriginal data distribution:\nback                :      956\nbuffer_overflow     :       30\nftp_write           :        8\nguess_passwd        :       53\nimap                :       11\nipsweep             :     3599\nland                :       18\nloadmodule          :        9\nmultihop            :        7\nneptune             :    41214\nnmap                :     1493\nnormal              :    67343\nperl                :        3\nphf                 :        4\npod                 :      201\nportsweep           :     2931\nrootkit             :       10\nsatan               :     3633\nsmurf               :     2646\nspy                 :        2\nteardrop            :      892\nwarezclient         :      890\nwarezmaster         :       20\n\nTest samples: 18794\n\nModel parameters: 3,614,138\n\n======================================================================\nHYBRID TRAINING STRATEGY\n======================================================================\n\n>>> PHASE 1: Learning from REAL data (30 epochs)\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"Phase 1 - Epoch 1/30: 100%|██████████| 493/493 [00:08<00:00, 60.21it/s, loss=0.0538]\nPhase 1 - Epoch 2/30: 100%|██████████| 493/493 [00:08<00:00, 60.32it/s, loss=0.3026]\nPhase 1 - Epoch 3/30: 100%|██████████| 493/493 [00:08<00:00, 59.55it/s, loss=1.3090]\nPhase 1 - Epoch 4/30: 100%|██████████| 493/493 [00:08<00:00, 60.12it/s, loss=0.3785]\nPhase 1 - Epoch 5/30: 100%|██████████| 493/493 [00:08<00:00, 60.89it/s, loss=0.1711]\n","output_type":"stream"},{"name":"stdout","text":"  Validation - Macro F1: 0.4791, Weighted F1: 0.7989\n","output_type":"stream"},{"name":"stderr","text":"Phase 1 - Epoch 6/30: 100%|██████████| 493/493 [00:08<00:00, 60.02it/s, loss=0.0279]\nPhase 1 - Epoch 7/30: 100%|██████████| 493/493 [00:08<00:00, 59.63it/s, loss=0.0178]\nPhase 1 - Epoch 8/30: 100%|██████████| 493/493 [00:08<00:00, 60.05it/s, loss=0.0084]\nPhase 1 - Epoch 9/30: 100%|██████████| 493/493 [00:08<00:00, 60.22it/s, loss=0.0265]\nPhase 1 - Epoch 10/30: 100%|██████████| 493/493 [00:08<00:00, 60.28it/s, loss=nan]   \n","output_type":"stream"},{"name":"stdout","text":"  Validation - Macro F1: 0.0018, Weighted F1: 0.0007\n","output_type":"stream"},{"name":"stderr","text":"Phase 1 - Epoch 11/30: 100%|██████████| 493/493 [00:08<00:00, 59.65it/s, loss=nan]\nPhase 1 - Epoch 12/30: 100%|██████████| 493/493 [00:08<00:00, 60.07it/s, loss=nan]\nPhase 1 - Epoch 13/30: 100%|██████████| 493/493 [00:08<00:00, 60.10it/s, loss=nan]\nPhase 1 - Epoch 14/30: 100%|██████████| 493/493 [00:08<00:00, 59.41it/s, loss=nan]\nPhase 1 - Epoch 15/30: 100%|██████████| 493/493 [00:08<00:00, 60.60it/s, loss=nan]\n","output_type":"stream"},{"name":"stdout","text":"  Validation - Macro F1: 0.0018, Weighted F1: 0.0007\n","output_type":"stream"},{"name":"stderr","text":"Phase 1 - Epoch 16/30: 100%|██████████| 493/493 [00:08<00:00, 59.44it/s, loss=nan]\nPhase 1 - Epoch 17/30: 100%|██████████| 493/493 [00:08<00:00, 60.91it/s, loss=nan]\nPhase 1 - Epoch 18/30: 100%|██████████| 493/493 [00:08<00:00, 59.84it/s, loss=nan]\nPhase 1 - Epoch 19/30: 100%|██████████| 493/493 [00:08<00:00, 60.08it/s, loss=nan]\nPhase 1 - Epoch 20/30: 100%|██████████| 493/493 [00:08<00:00, 60.09it/s, loss=nan]\n","output_type":"stream"},{"name":"stdout","text":"  Validation - Macro F1: 0.0018, Weighted F1: 0.0007\n","output_type":"stream"},{"name":"stderr","text":"Phase 1 - Epoch 21/30: 100%|██████████| 493/493 [00:08<00:00, 59.84it/s, loss=nan]\nPhase 1 - Epoch 22/30: 100%|██████████| 493/493 [00:08<00:00, 60.85it/s, loss=nan]\nPhase 1 - Epoch 23/30: 100%|██████████| 493/493 [00:08<00:00, 60.25it/s, loss=nan]\nPhase 1 - Epoch 24/30: 100%|██████████| 493/493 [00:08<00:00, 60.43it/s, loss=nan]\nPhase 1 - Epoch 25/30: 100%|██████████| 493/493 [00:08<00:00, 59.15it/s, loss=nan]\n","output_type":"stream"},{"name":"stdout","text":"  Validation - Macro F1: 0.0018, Weighted F1: 0.0007\n","output_type":"stream"},{"name":"stderr","text":"Phase 1 - Epoch 26/30: 100%|██████████| 493/493 [00:08<00:00, 60.19it/s, loss=nan]\nPhase 1 - Epoch 27/30: 100%|██████████| 493/493 [00:08<00:00, 60.47it/s, loss=nan]\nPhase 1 - Epoch 28/30: 100%|██████████| 493/493 [00:08<00:00, 60.50it/s, loss=nan]\nPhase 1 - Epoch 29/30: 100%|██████████| 493/493 [00:08<00:00, 59.66it/s, loss=nan]\nPhase 1 - Epoch 30/30: 100%|██████████| 493/493 [00:08<00:00, 60.43it/s, loss=nan]\n","output_type":"stream"},{"name":"stdout","text":"  Validation - Macro F1: 0.0018, Weighted F1: 0.0007\n\n>>> PHASE 2: Fine-tuning with SMOTE augmentation (40 epochs)\n======================================================================\nMixed dataset: 125973 original + 557934 SMOTE\n","output_type":"stream"},{"name":"stderr","text":"Phase 2 - Epoch 1/40: 100%|██████████| 1336/1336 [00:23<00:00, 56.82it/s, loss=nan]\n","output_type":"stream"},{"name":"stdout","text":"\n  Train Loss: nan\n  Macro F1: 0.0018 | Weighted F1: 0.0007\n  Classes F1 > 0.3: 0/21 | Mean: 0.0018\n","output_type":"stream"},{"name":"stderr","text":"Phase 2 - Epoch 2/40: 100%|██████████| 1336/1336 [00:23<00:00, 56.48it/s, loss=nan]\n","output_type":"stream"},{"name":"stdout","text":"\n  Train Loss: nan\n  Macro F1: 0.0018 | Weighted F1: 0.0007\n  Classes F1 > 0.3: 0/21 | Mean: 0.0018\n","output_type":"stream"},{"name":"stderr","text":"Phase 2 - Epoch 3/40: 100%|██████████| 1336/1336 [00:23<00:00, 56.07it/s, loss=nan]\n","output_type":"stream"},{"name":"stdout","text":"\n  Train Loss: nan\n  Macro F1: 0.0018 | Weighted F1: 0.0007\n  Classes F1 > 0.3: 0/21 | Mean: 0.0018\n","output_type":"stream"},{"name":"stderr","text":"Phase 2 - Epoch 4/40: 100%|██████████| 1336/1336 [00:23<00:00, 56.59it/s, loss=nan]\n","output_type":"stream"},{"name":"stdout","text":"\n  Train Loss: nan\n  Macro F1: 0.0018 | Weighted F1: 0.0007\n  Classes F1 > 0.3: 0/21 | Mean: 0.0018\n","output_type":"stream"},{"name":"stderr","text":"Phase 2 - Epoch 5/40: 100%|██████████| 1336/1336 [00:23<00:00, 57.65it/s, loss=nan]\n","output_type":"stream"},{"name":"stdout","text":"\n  Train Loss: nan\n  Macro F1: 0.0018 | Weighted F1: 0.0007\n  Classes F1 > 0.3: 0/21 | Mean: 0.0018\n","output_type":"stream"},{"name":"stderr","text":"Phase 2 - Epoch 6/40: 100%|██████████| 1336/1336 [00:23<00:00, 56.32it/s, loss=nan]\n","output_type":"stream"},{"name":"stdout","text":"\n  Train Loss: nan\n  Macro F1: 0.0018 | Weighted F1: 0.0007\n  Classes F1 > 0.3: 0/21 | Mean: 0.0018\n","output_type":"stream"},{"name":"stderr","text":"Phase 2 - Epoch 7/40: 100%|██████████| 1336/1336 [00:23<00:00, 57.04it/s, loss=nan]\n","output_type":"stream"},{"name":"stdout","text":"\n  Train Loss: nan\n  Macro F1: 0.0018 | Weighted F1: 0.0007\n  Classes F1 > 0.3: 0/21 | Mean: 0.0018\n","output_type":"stream"},{"name":"stderr","text":"Phase 2 - Epoch 8/40: 100%|██████████| 1336/1336 [00:23<00:00, 56.93it/s, loss=nan]\n","output_type":"stream"},{"name":"stdout","text":"\n  Train Loss: nan\n  Macro F1: 0.0018 | Weighted F1: 0.0007\n  Classes F1 > 0.3: 0/21 | Mean: 0.0018\n","output_type":"stream"},{"name":"stderr","text":"Phase 2 - Epoch 9/40: 100%|██████████| 1336/1336 [00:23<00:00, 56.53it/s, loss=nan]\n","output_type":"stream"},{"name":"stdout","text":"\n  Train Loss: nan\n  Macro F1: 0.0018 | Weighted F1: 0.0007\n  Classes F1 > 0.3: 0/21 | Mean: 0.0018\n","output_type":"stream"},{"name":"stderr","text":"Phase 2 - Epoch 10/40: 100%|██████████| 1336/1336 [00:23<00:00, 57.06it/s, loss=nan]\n","output_type":"stream"},{"name":"stdout","text":"\n  Train Loss: nan\n  Macro F1: 0.0018 | Weighted F1: 0.0007\n  Classes F1 > 0.3: 0/21 | Mean: 0.0018\n","output_type":"stream"},{"name":"stderr","text":"Phase 2 - Epoch 11/40: 100%|██████████| 1336/1336 [00:23<00:00, 57.40it/s, loss=nan]\n","output_type":"stream"},{"name":"stdout","text":"\n  Train Loss: nan\n  Macro F1: 0.0018 | Weighted F1: 0.0007\n  Classes F1 > 0.3: 0/21 | Mean: 0.0018\n","output_type":"stream"},{"name":"stderr","text":"Phase 2 - Epoch 12/40: 100%|██████████| 1336/1336 [00:23<00:00, 57.41it/s, loss=nan]\n","output_type":"stream"},{"name":"stdout","text":"\n  Train Loss: nan\n  Macro F1: 0.0018 | Weighted F1: 0.0007\n  Classes F1 > 0.3: 0/21 | Mean: 0.0018\n","output_type":"stream"},{"name":"stderr","text":"Phase 2 - Epoch 13/40: 100%|██████████| 1336/1336 [00:23<00:00, 57.39it/s, loss=nan]\n","output_type":"stream"},{"name":"stdout","text":"\n  Train Loss: nan\n  Macro F1: 0.0018 | Weighted F1: 0.0007\n  Classes F1 > 0.3: 0/21 | Mean: 0.0018\n","output_type":"stream"},{"name":"stderr","text":"Phase 2 - Epoch 14/40: 100%|██████████| 1336/1336 [00:23<00:00, 56.50it/s, loss=nan]\n","output_type":"stream"},{"name":"stdout","text":"\n  Train Loss: nan\n  Macro F1: 0.0018 | Weighted F1: 0.0007\n  Classes F1 > 0.3: 0/21 | Mean: 0.0018\n","output_type":"stream"},{"name":"stderr","text":"Phase 2 - Epoch 15/40: 100%|██████████| 1336/1336 [00:23<00:00, 56.67it/s, loss=nan]\n","output_type":"stream"},{"name":"stdout","text":"\n  Train Loss: nan\n  Macro F1: 0.0018 | Weighted F1: 0.0007\n  Classes F1 > 0.3: 0/21 | Mean: 0.0018\n\nEarly stopping\n\n======================================================================\nFINAL EVALUATION\n======================================================================\n\nCLASSIFICATION REPORT:\n======================================================================\n                 precision    recall  f1-score   support\n\n           back     0.0191    1.0000    0.0375       359\nbuffer_overflow     0.0000    0.0000    0.0000        20\n      ftp_write     0.0000    0.0000    0.0000         3\n   guess_passwd     0.0000    0.0000    0.0000      1231\n           imap     0.0000    0.0000    0.0000         1\n        ipsweep     0.0000    0.0000    0.0000       141\n           land     0.0000    0.0000    0.0000         7\n     loadmodule     0.0000    0.0000    0.0000         2\n       multihop     0.0000    0.0000    0.0000        18\n        neptune     0.0000    0.0000    0.0000      4657\n           nmap     0.0000    0.0000    0.0000        73\n         normal     0.0000    0.0000    0.0000      9711\n           perl     0.0000    0.0000    0.0000         2\n            phf     0.0000    0.0000    0.0000         2\n            pod     0.0000    0.0000    0.0000        41\n      portsweep     0.0000    0.0000    0.0000       157\n        rootkit     0.0000    0.0000    0.0000        13\n          satan     0.0000    0.0000    0.0000       735\n          smurf     0.0000    0.0000    0.0000       665\n       teardrop     0.0000    0.0000    0.0000        12\n    warezmaster     0.0000    0.0000    0.0000       944\n\n       accuracy                         0.0191     18794\n      macro avg     0.0009    0.0476    0.0018     18794\n   weighted avg     0.0004    0.0191    0.0007     18794\n\n\n======================================================================\nBEST MACRO F1: 0.4791\n======================================================================\n","output_type":"stream"}],"execution_count":14}]}