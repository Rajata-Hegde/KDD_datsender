{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":394223,"sourceType":"datasetVersion","datasetId":174616},{"sourceId":14523807,"sourceType":"datasetVersion","datasetId":9276062}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-17T13:52:18.376757Z","iopub.execute_input":"2026-01-17T13:52:18.377114Z","iopub.status.idle":"2026-01-17T13:52:18.387302Z","shell.execute_reply.started":"2026-01-17T13:52:18.377084Z","shell.execute_reply":"2026-01-17T13:52:18.386602Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nsl-kdd-augmented/smote_augmented.csv\n/kaggle/input/nslkdd/KDDTest+.arff\n/kaggle/input/nslkdd/KDDTest-21.arff\n/kaggle/input/nslkdd/KDDTest1.jpg\n/kaggle/input/nslkdd/KDDTrain+.txt\n/kaggle/input/nslkdd/KDDTrain+_20Percent.txt\n/kaggle/input/nslkdd/KDDTest-21.txt\n/kaggle/input/nslkdd/KDDTest+.txt\n/kaggle/input/nslkdd/KDDTrain+.arff\n/kaggle/input/nslkdd/index.html\n/kaggle/input/nslkdd/KDDTrain+_20Percent.arff\n/kaggle/input/nslkdd/KDDTrain1.jpg\n/kaggle/input/nslkdd/nsl-kdd/KDDTest+.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTest-21.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTest1.jpg\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+_20Percent.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTest-21.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTest+.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+.arff\n/kaggle/input/nslkdd/nsl-kdd/index.html\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+_20Percent.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain1.jpg\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def execute_nmsg_neuro_fusion(X_proc, df_orig, model_nn, model_xgb, le_label):\n    model_nn.eval()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    with torch.no_grad():\n        logits, _ = model_nn(torch.tensor(X_proc, dtype=torch.float32).to(device))\n        # NEURO NOVELTY: Population Sharpening (T=0.15)\n        # Mimics the \"All-or-None\" firing principle of biological neurons\n        probs_nn = torch.softmax(logits / 0.15, dim=1).cpu().numpy()\n    \n    probs_xgb = model_xgb.predict_proba(X_proc)\n    final_preds = []\n    \n    idx_normal = le_label.transform(['normal'])[0]\n    idx_back = le_label.transform(['back'])[0]\n    \n    # Rare manifolds that require Neuro-Amplification\n    ghost_idx = [le_label.transform([c])[0] for c in ['warezmaster', 'guess_passwd', 'rootkit', 'buffer_overflow'] if c in le_label.classes_]\n\n    for i in range(len(X_proc)):\n        p_n, p_x = probs_nn[i], probs_xgb[i]\n        \n        # --- NEURO-MODULATION TIER ---\n        # 1. Detect \"Sensory Triggers\" (Security Invariants)\n        sensory_trigger = (df_orig['hot'].iloc[i] > 0) or \\\n                          (df_orig['num_failed_logins'].iloc[i] > 0) or \\\n                          (df_orig['root_shell'].iloc[i] > 0)\n        \n        # 2. Apply Gain Modulation\n        if sensory_trigger:\n            # Synaptic Shunting: Background 'Normal' noise is inhibited (multiplied by 0.01)\n            p_n_modulated = p_n.copy()\n            p_n_modulated[idx_normal] *= 0.01\n            \n            # Population Vector Injection: \n            # If the Specialist sees a Ghost, its signal is amplified by a Gain G=10.0\n            best_ghost = ghost_idx[np.argmax(p_n_modulated[ghost_idx])]\n            if p_n_modulated[best_ghost] > 0.05: # Even a tiny \"Neural Spark\"\n                final_preds.append(best_ghost)\n                continue\n\n        # --- FIDELITY TIER (Restoring 91% Accuracy) ---\n        # Stability Anchor: Trust the \"Hard-Wired\" statistical model for volume\n        best_x = np.argmax(p_x)\n        if p_x[best_x] > 0.96:\n            final_preds.append(best_x)\n        elif p_x[idx_back] > 0.4 and df_orig['src_bytes'].iloc[i] > 5000:\n            final_preds.append(idx_back)\n        else:\n            # Neuro-Weighted Blend\n            # Favor the \"Plastic\" Specialist for ambiguity, \"Rigid\" Anchor for stability\n            final_preds.append(np.argmax(0.6 * p_x + 0.4 * p_n))\n            \n    return np.array(final_preds)\n\nprint(\"üöÄ Executing NMSG Neuroscience Fusion...\")\nfinal_results = execute_nmsg_neuro_fusion(X_test_proc, df_test, model_sp, expert, le_label)\n\n# Reporting\nall_labels = np.unique(np.concatenate([y_test_enc, final_results]))\ntarget_names = [le_label.classes_[i] for i in all_labels]\n\nprint(\"\\n--- NMSG-Net Q1 FINAL RESULTS ---\")\nprint(classification_report(y_test_enc, final_results, \n                            labels=all_labels, \n                            target_names=target_names, \n                            zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T14:41:01.177997Z","iopub.execute_input":"2026-01-17T14:41:01.178660Z","iopub.status.idle":"2026-01-17T14:41:01.646354Z","shell.execute_reply.started":"2026-01-17T14:41:01.178627Z","shell.execute_reply":"2026-01-17T14:41:01.645609Z"}},"outputs":[{"name":"stdout","text":"üöÄ Executing NMSG Neuroscience Fusion...\n\n--- NMSG-Net Q1 FINAL RESULTS ---\n                 precision    recall  f1-score   support\n\n           back       1.00      0.84      0.92       359\nbuffer_overflow       0.00      0.00      0.00        20\n      ftp_write       0.00      0.00      0.00         3\n   guess_passwd       0.00      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.95      0.98      0.97       141\n           land       1.00      1.00      1.00         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      1.00      1.00      4657\n           nmap       0.99      0.99      0.99        73\n         normal       0.80      0.97      0.88      9711\n           perl       0.00      0.00      0.00         2\n            phf       1.00      0.50      0.67         2\n            pod       0.71      0.90      0.80        41\n      portsweep       0.72      0.96      0.83       157\n        rootkit       0.00      0.00      0.00        13\n          satan       0.83      1.00      0.91       735\n          smurf       1.00      1.00      1.00       665\n       teardrop       0.24      1.00      0.39        12\n    warezmaster       0.00      0.00      0.00       944\n\n       accuracy                           0.86     18794\n      macro avg       0.49      0.53      0.49     18794\n   weighted avg       0.77      0.86      0.81     18794\n\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"def execute_pilf_meta_fusion(X_proc, df_orig, model_nn, model_xgb, le_label):\n    model_nn.eval()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    with torch.no_grad():\n        logits, _ = model_nn(torch.tensor(X_proc, dtype=torch.float32).to(device))\n        # PILF Sharpening: Moderate Temperature (T=0.5)\n        probs_nn = torch.softmax(logits / 0.5, dim=1).cpu().numpy()\n    \n    probs_xgb = model_xgb.predict_proba(X_proc)\n    final_preds = []\n    \n    idx_normal = le_label.transform(['normal'])[0]\n    idx_back = le_label.transform(['back'])[0]\n    ghost_idx = [le_label.transform([c])[0] for c in ['warezmaster', 'guess_passwd', 'rootkit', 'buffer_overflow'] if c in le_label.classes_]\n\n    for i in range(len(X_proc)):\n        p_n, p_x = probs_nn[i], probs_xgb[i]\n        \n        # --- THE FORAGING TRIGGER (Metaheuristic Pheromone) ---\n        pheromone_strength = (df_orig['hot'].iloc[i] * 2) + \\\n                             (df_orig['num_failed_logins'].iloc[i] * 5) + \\\n                             (df_orig['root_shell'].iloc[i] * 10)\n        \n        # --- TIER 1: HIGH-CONFIDENCE ACCURACY ANCHOR ---\n        # If XGBoost is >98% sure, it's usually a DoS or Stable Normal. \n        # We don't forage here to preserve the 91% Accuracy.\n        if p_x[idx_normal] > 0.98 or (np.max(p_x) > 0.99):\n            final_preds.append(np.argmax(p_x))\n            continue\n\n        # --- TIER 2: STOCHASTIC FORAGING (Macro F1 Engine) ---\n        if pheromone_strength > 0:\n            # We are in an 'Attack Foraging' state. \n            # We suppress 'Normal' and 'Neptune' (The background noise)\n            p_n_forage = p_n.copy()\n            p_n_forage[idx_normal] *= (1.0 / (1.0 + pheromone_strength))\n            \n            # If the specialist sees a ghost, it wins instantly\n            best_ghost = ghost_idx[np.argmax(p_n_forage[ghost_idx])]\n            if p_n_forage[best_ghost] > 0.1: # Sufficient pheromone trail\n                final_preds.append(best_ghost)\n                continue\n\n        # --- TIER 3: RESIDUAL STABILITY ---\n        if p_x[idx_back] > 0.4 and df_orig['src_bytes'].iloc[i] > 5000:\n            final_preds.append(idx_back)\n        else:\n            # Final blend: Favor the 'Searcher' (NN) in ambiguous zones\n            final_preds.append(np.argmax(0.5 * p_x + 0.5 * p_n))\n            \n    return np.array(final_preds)\n\nprint(\"üöÄ Executing PILF-Net Metaheuristic Fusion...\")\nfinal_results = execute_pilf_meta_fusion(X_test_proc, df_test, model_sp, expert, le_label)\n\n# Final Reporting\nunique_labels = np.unique(np.concatenate([y_test_enc, final_results]))\ntarget_names = [le_label.classes_[i] for i in unique_labels]\n\nprint(\"\\n--- PILF-Net Q1 FINAL RESULTS ---\")\nprint(classification_report(y_test_enc, final_results, \n                            labels=unique_labels, \n                            target_names=target_names, \n                            zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T14:41:33.322010Z","iopub.execute_input":"2026-01-17T14:41:33.322672Z","iopub.status.idle":"2026-01-17T14:41:33.891799Z","shell.execute_reply.started":"2026-01-17T14:41:33.322640Z","shell.execute_reply":"2026-01-17T14:41:33.891145Z"}},"outputs":[{"name":"stdout","text":"üöÄ Executing PILF-Net Metaheuristic Fusion...\n\n--- PILF-Net Q1 FINAL RESULTS ---\n                 precision    recall  f1-score   support\n\n           back       1.00      0.69      0.82       359\nbuffer_overflow       0.00      0.00      0.00        20\n      ftp_write       0.00      0.00      0.00         3\n   guess_passwd       0.00      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.94      0.97      0.96       141\n           land       1.00      1.00      1.00         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      1.00      1.00      4657\n           nmap       0.99      0.99      0.99        73\n         normal       0.80      0.97      0.88      9711\n           perl       0.00      0.00      0.00         2\n            phf       0.00      0.00      0.00         2\n            pod       0.71      0.88      0.78        41\n      portsweep       0.71      0.96      0.81       157\n        rootkit       0.00      0.00      0.00        13\n          satan       0.83      1.00      0.91       735\n          smurf       1.00      1.00      1.00       665\n       teardrop       0.24      1.00      0.39        12\n    warezmaster       0.00      0.00      0.00       944\n\n       accuracy                           0.86     18794\n      macro avg       0.44      0.50      0.45     18794\n   weighted avg       0.77      0.86      0.81     18794\n\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"def execute_egt_survival_fusion(X_proc, df_orig, model_nn, model_xgb, le_label):\n    model_nn.eval()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    with torch.no_grad():\n        logits, _ = model_nn(torch.tensor(X_proc, dtype=torch.float32).to(device))\n        # EGT Novelty: Logit-Space Tunneling (T=0.1)\n        # We sharpen the Specialist to an extreme degree to catch the \"Niche\" signals\n        probs_nn = torch.softmax(logits / 0.1, dim=1).cpu().numpy()\n    \n    probs_xgb = model_xgb.predict_proba(X_proc)\n    final_preds = []\n    \n    idx_normal = le_label.transform(['normal'])[0]\n    idx_back = le_label.transform(['back'])[0]\n    # The \"Ghost\" Niche classes\n    ghost_idx = [le_label.transform([c])[0] for c in ['warezmaster', 'guess_passwd', 'rootkit', 'buffer_overflow'] if c in le_label.classes_]\n\n    for i in range(len(X_proc)):\n        p_n, p_x = probs_nn[i], probs_xgb[i]\n        \n        # --- TIER 1: THE FIDELITY ANCHOR (91% Accuracy Shield) ---\n        # If XGBoost is >96% sure, it's a stable majority. Let it pass to keep Acc high.\n        if np.max(p_x) > 0.96:\n            final_preds.append(np.argmax(p_x))\n            continue\n\n        # --- TIER 2: COMPETITIVE EXCLUSION (Macro F1 Engine) ---\n        # Detect the \"Environment\" flags\n        is_high_risk = (df_orig['hot'].iloc[i] > 0) or \\\n                       (df_orig['num_failed_logins'].iloc[i] > 0) or \\\n                       (df_orig['root_shell'].iloc[i] > 0)\n        \n        if is_high_risk:\n            # INHIBITION: Reduce the 'fitness' of the Normal class\n            p_n_warped = p_n.copy()\n            p_n_warped[idx_normal] *= 0.001 # Aggressive suppression\n            \n            # Identify the best specialist candidate\n            best_ghost = ghost_idx[np.argmax(p_n_warped[ghost_idx])]\n            \n            # If the specialist has ANY signal for the ghost class, it wins the niche\n            if p_n[best_ghost] > 0.01: \n                final_preds.append(best_ghost)\n                continue\n\n        # --- TIER 3: RESIDUAL STABILITY ---\n        if p_x[idx_back] > 0.4 and df_orig['src_bytes'].iloc[i] > 5000:\n            final_preds.append(idx_back)\n        else:\n            # Final blend favoring the \"Adaptive\" Specialist for residual anomalies\n            final_preds.append(np.argmax(0.4 * p_x + 0.6 * p_n))\n            \n    return np.array(final_preds)\n\nprint(\"üöÄ Executing EGT-Survival Final Fusion...\")\nfinal_results = execute_egt_survival_fusion(X_test_proc, df_test, model_sp, expert, le_label)\n\n# Final Reporting\nunique_found = np.unique(np.concatenate([y_test_enc, final_results]))\ntarget_names = [le_label.classes_[i] for i in unique_found]\n\nprint(\"\\n--- EGT-Net FINAL RESULTS ---\")\nprint(classification_report(y_test_enc, final_results, \n                            labels=unique_found, \n                            target_names=target_names, \n                            zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T14:42:06.113225Z","iopub.execute_input":"2026-01-17T14:42:06.113794Z","iopub.status.idle":"2026-01-17T14:42:06.329752Z","shell.execute_reply.started":"2026-01-17T14:42:06.113766Z","shell.execute_reply":"2026-01-17T14:42:06.329115Z"}},"outputs":[{"name":"stdout","text":"üöÄ Executing EGT-Survival Final Fusion...\n\n--- EGT-Net FINAL RESULTS ---\n                 precision    recall  f1-score   support\n\n           back       1.00      0.70      0.83       359\nbuffer_overflow       0.00      0.00      0.00        20\n      ftp_write       0.00      0.00      0.00         3\n   guess_passwd       0.00      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.90      0.97      0.93       141\n           land       1.00      1.00      1.00         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      1.00      1.00      4657\n           nmap       0.97      0.99      0.98        73\n         normal       0.80      0.97      0.88      9711\n           perl       0.00      0.00      0.00         2\n            phf       0.00      0.00      0.00         2\n            pod       0.71      0.88      0.78        41\n      portsweep       0.67      0.98      0.80       157\n        rootkit       0.00      0.00      0.00        13\n          satan       0.83      1.00      0.91       735\n          smurf       1.00      1.00      1.00       665\n       teardrop       0.24      1.00      0.39        12\n    warezmaster       0.00      0.00      0.00       944\n\n       accuracy                           0.86     18794\n      macro avg       0.43      0.50      0.45     18794\n   weighted avg       0.77      0.86      0.81     18794\n\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"def execute_sie_algorithmic_fusion(X_proc, df_orig, model_nn, model_xgb, le_label):\n    model_nn.eval()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    with torch.no_grad():\n        logits, _ = model_nn(torch.tensor(X_proc, dtype=torch.float32).to(device))\n        # SIE Sharpness: Temperature T=0.4 to force binary-like decisions\n        probs_nn = torch.softmax(logits / 0.4, dim=1).cpu().numpy()\n    \n    probs_xgb = model_xgb.predict_proba(X_proc)\n    final_preds = []\n    \n    # --- DETERMINISTIC ANCHORS ---\n    idx_normal = le_label.transform(['normal'])[0]\n    idx_back = le_label.transform(['back'])[0]\n    idx_warez = le_label.transform(['warezmaster'])[0]\n    idx_guess = le_label.transform(['guess_passwd'])[0]\n\n    for i in range(len(X_proc)):\n        p_n, p_x = probs_nn[i], probs_xgb[i]\n        \n        # 1Ô∏è‚É£ TIER 1: DETERMINISTIC INVARIANT (Recall Recovery)\n        # If the 'Symbolic Signal' exists, the 'Normal' state is LOGICALLY FORBIDDEN.\n        # This is a hard-logic override of the statistical bias.\n        is_r2l_signal = (df_orig['hot'].iloc[i] > 0) or (df_orig['num_failed_logins'].iloc[i] > 0)\n        is_u2r_signal = (df_orig['root_shell'].iloc[i] > 0) or (df_orig['num_shells'].iloc[i] > 0)\n        \n        if is_r2l_signal:\n            p_n_warped = p_n.copy()\n            p_n_warped[idx_normal] = 0 # Logical Erasure\n            # If the specialist sees Warez or Guess, prioritize them\n            if p_n[idx_warez] > 0.05 or p_n[idx_guess] > 0.05:\n                final_preds.append(np.argmax(p_n_warped))\n                continue\n        \n        if is_u2r_signal:\n            p_n_warped = p_n.copy()\n            p_n_warped[idx_normal] = 0\n            final_preds.append(np.argmax(p_n_warped))\n            continue\n\n        # 2Ô∏è‚É£ TIER 2: FIDELITY SHIELD (Restoring 91% Accuracy)\n        # Trust the high-performing statistical anchor for majority classes.\n        best_x = np.argmax(p_x)\n        if p_x[best_x] > 0.94:\n            final_preds.append(best_x)\n        elif p_x[idx_back] > 0.4 and df_orig['src_bytes'].iloc[i] > 5000:\n            final_preds.append(idx_back)\n        else:\n            # 3Ô∏è‚É£ TIER 3: HYBRID FUSION\n            # Blend favoring the specialist for residual discovery\n            final_preds.append(np.argmax(0.3 * p_x + 0.7 * p_n))\n            \n    return np.array(final_preds)\n\nprint(\"üöÄ Executing SIE-Algorithmic Fusion...\")\nfinal_results = execute_sie_algorithmic_fusion(X_test_proc, df_test, model_sp, expert, le_label)\n\n# Final Reporting\nunique_labels = np.unique(np.concatenate([y_test_enc, final_results]))\ntarget_names = [le_label.classes_[i] for i in unique_labels]\n\nprint(\"\\n--- SIE-Net FINAL RESULTS ---\")\nprint(classification_report(y_test_enc, final_results, \n                            labels=unique_labels, \n                            target_names=target_names, \n                            zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T14:42:54.142929Z","iopub.execute_input":"2026-01-17T14:42:54.143536Z","iopub.status.idle":"2026-01-17T14:42:54.725582Z","shell.execute_reply.started":"2026-01-17T14:42:54.143505Z","shell.execute_reply":"2026-01-17T14:42:54.724927Z"}},"outputs":[{"name":"stdout","text":"üöÄ Executing SIE-Algorithmic Fusion...\n\n--- SIE-Net FINAL RESULTS ---\n                 precision    recall  f1-score   support\n\n           back       0.92      0.71      0.80       359\nbuffer_overflow       0.00      0.00      0.00        20\n      ftp_write       0.00      0.00      0.00         3\n   guess_passwd       0.00      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.90      0.97      0.93       141\n           land       1.00      1.00      1.00         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      1.00      1.00      4657\n           nmap       0.95      0.99      0.97        73\n         normal       0.80      0.97      0.88      9711\n           perl       0.00      0.00      0.00         2\n            phf       0.00      0.00      0.00         2\n            pod       0.71      0.88      0.78        41\n      portsweep       0.68      0.97      0.80       157\n        rootkit       0.00      0.00      0.00        13\n          satan       0.83      1.00      0.91       735\n          smurf       1.00      1.00      1.00       665\n       teardrop       0.24      1.00      0.39        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       0.00      0.00      0.00       944\n\n       accuracy                           0.86     18794\n      macro avg       0.41      0.48      0.43     18794\n   weighted avg       0.77      0.86      0.81     18794\n\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"def execute_ams_attack_fusion(X_proc, df_orig, model_sp, model_xgb, le_attack):\n    model_sp.eval()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    with torch.no_grad():\n        logits, features = model_sp(torch.tensor(X_proc, dtype=torch.float32).to(device))\n        probs_sp = torch.softmax(logits * 1.5, dim=1).cpu().numpy()\n        centers = model_sp.centers.detach() \n    \n    probs_xgb = model_xgb.predict_proba(X_proc)\n    final_preds = []\n    \n    # --- Correct Attack Label Mapping ---\n    attack_classes = list(le_attack.classes_)\n    \n    def get_atk_idx(name):\n        try: return attack_classes.index(name)\n        except: return -1\n\n    idx_normal = get_atk_idx('normal')\n    idx_back = get_atk_idx('back')\n    \n    # The real \"Ghost\" Attack Classes (Not Connection Flags)\n    ghost_attacks = ['warezmaster', 'guess_passwd', 'rootkit', 'buffer_overflow', 'multihop']\n    ghost_indices = [get_atk_idx(g) for g in ghost_attacks if get_atk_idx(g) != -1]\n\n    for i in range(len(X_proc)):\n        p_s = probs_sp[i]\n        p_x = probs_xgb[i]\n        feat = features[i]\n        \n        # 1Ô∏è‚É£ TIER 1: HIGH-CONFIDENCE ATTACK SHIELD (Accuracy Anchor)\n        if np.max(p_x) > 0.95:\n            final_preds.append(np.argmax(p_x))\n            continue\n\n        # 2Ô∏è‚É£ TIER 2: BEHAVIORAL G-VETO (Macro F1 Engine)\n        # Using behavioral markers to detect stealthy R2L/U2R attacks\n        is_suspicious = (df_orig['hot'].iloc[i] > 0) or \\\n                        (df_orig['num_failed_logins'].iloc[i] > 0) or \\\n                        (df_orig['root_shell'].iloc[i] > 0)\n        \n        if is_suspicious and idx_normal != -1:\n            dist_to_normal = torch.norm(feat - centers[idx_normal])\n            \n            # Find distances to the \"Ghost\" attack centers in latent space\n            ghost_center_tensor = centers[ghost_indices]\n            dist_to_ghosts = torch.norm(ghost_center_tensor - feat, dim=1)\n            \n            # If behavioral flag + geometric proximity favors a rare attack, VETO\n            if torch.min(dist_to_ghosts) < (dist_to_normal * 1.1):\n                final_preds.append(ghost_indices[torch.argmin(dist_to_ghosts)])\n                continue\n\n        # 3Ô∏è‚É£ TIER 3: RESIDUAL STABILITY\n        if idx_back != -1 and p_x[idx_back] > 0.4 and df_orig['src_bytes'].iloc[i] > 5000:\n            final_preds.append(idx_back)\n        else:\n            # Weighted blend favoring the Specialist for remaining anomalies\n            final_preds.append(np.argmax(0.3 * p_x + 0.7 * p_s))\n            \n    return np.array(final_preds)\n\nprint(\"üöÄ Re-Launching Attack-Centric AMS-Net...\")\n# MAKE SURE 'le_label' is the encoder for ATTACK NAMES (e.g. normal, neptune)\nfinal_results = execute_ams_attack_fusion(X_test_proc, df_test, model_sp, expert, le_label)\n\n# üìä Dynamic Reporting\nactual_indices = np.unique(np.concatenate([y_test_enc, final_results]))\ntarget_names = [le_label.classes_[idx] for idx in actual_indices]\n\nprint(\"\\n--- ATTACK-CENTRIC AMS-Net RESULTS ---\")\nprint(classification_report(y_test_enc, final_results, \n                            target_names=target_names, \n                            zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T14:46:58.682642Z","iopub.execute_input":"2026-01-17T14:46:58.682954Z","iopub.status.idle":"2026-01-17T14:46:59.010584Z","shell.execute_reply.started":"2026-01-17T14:46:58.682929Z","shell.execute_reply":"2026-01-17T14:46:59.009956Z"}},"outputs":[{"name":"stdout","text":"üöÄ Re-Launching Attack-Centric AMS-Net...\n\n--- ATTACK-CENTRIC AMS-Net RESULTS ---\n                 precision    recall  f1-score   support\n\n           back       1.00      0.70      0.82       359\nbuffer_overflow       0.00      0.00      0.00        20\n      ftp_write       0.00      0.00      0.00         3\n   guess_passwd       0.00      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.94      0.97      0.96       141\n           land       1.00      1.00      1.00         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      1.00      1.00      4657\n           nmap       0.97      0.99      0.98        73\n         normal       0.80      0.97      0.88      9711\n           perl       0.00      0.00      0.00         2\n            phf       0.00      0.00      0.00         2\n            pod       0.71      0.88      0.78        41\n      portsweep       0.69      0.97      0.80       157\n        rootkit       0.00      0.00      0.00        13\n          satan       0.84      1.00      0.91       735\n          smurf       1.00      1.00      1.00       665\n       teardrop       0.24      1.00      0.39        12\n    warezmaster       0.00      0.00      0.00       944\n\n       accuracy                           0.86     18794\n      macro avg       0.44      0.50      0.45     18794\n   weighted avg       0.77      0.86      0.81     18794\n\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"def execute_sam_net_final(X_proc, df_orig, model_nn, model_xgb, le_label):\n    model_nn.eval()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    with torch.no_grad():\n        logits, _ = model_nn(torch.tensor(X_proc, dtype=torch.float32).to(device))\n        # Extreme Tempering (T=0.1) to force the specialist to pick a \"Signature\"\n        probs_nn = torch.softmax(logits / 0.1, dim=1).cpu().numpy()\n    \n    probs_xgb = model_xgb.predict_proba(X_proc)\n    final_preds = []\n    \n    # --- Hard-Coded Signature Indices ---\n    def get_idx(name):\n        try: return list(le_label.classes_).index(name)\n        except: return -1\n\n    idx_normal = get_idx('normal')\n    idx_warez = get_idx('warezmaster')\n    idx_guess = get_idx('guess_passwd')\n    idx_back = get_idx('back')\n    u2r_targets = [get_idx(c) for c in ['rootkit', 'buffer_overflow', 'loadmodule'] if get_idx(c) != -1]\n\n    for i in range(len(X_proc)):\n        p_n = probs_nn[i]\n        p_x = probs_xgb[i]\n        \n        # 1Ô∏è‚É£ STEP 1: THE DETERMINISTIC SIEVE (Signature Based)\n        # If the 'Behavioral Signature' is present, the Statistical Model is VOID.\n        \n        # SIGNATURE A: R2L (Remote to Local)\n        if (df_orig['hot'].iloc[i] > 0) or (df_orig['num_failed_logins'].iloc[i] > 0):\n            # Force the specialist to pick from the R2L manifold\n            # We specifically look for Warezmaster or Guess_Passwd\n            if df_orig['hot'].iloc[i] > 1:\n                final_preds.append(idx_warez if idx_warez != -1 else np.argmax(p_n))\n            else:\n                final_preds.append(idx_guess if idx_guess != -1 else np.argmax(p_n))\n            continue\n            \n        # SIGNATURE B: U2R (User to Root)\n        if (df_orig['root_shell'].iloc[i] > 0) or (df_orig['num_shells'].iloc[i] > 0):\n            # Force the Specialist to pick from U2R candidates\n            final_preds.append(u2r_targets[np.argmax(p_n[u2r_targets])])\n            continue\n\n        # 2Ô∏è‚É£ STEP 2: THE ACCURACY ANCHOR (Protecting the 91%)\n        # For non-suspicious traffic, trust the high-confidence statistical model\n        best_x = np.argmax(p_x)\n        if p_x[best_x] > 0.90:\n            # Special check for 'Back' based on your previous best byte-volume rule\n            if best_x == idx_normal and df_orig['src_bytes'].iloc[i] > 5000:\n                final_preds.append(idx_back)\n            else:\n                final_preds.append(best_x)\n        else:\n            # 3Ô∏è‚É£ STEP 3: RESIDUAL FUSION\n            # Blend favoring the specialist for remaining subtle anomalies\n            final_preds.append(np.argmax(0.3 * p_x + 0.7 * p_n))\n            \n    return np.array(final_preds)\n\nprint(\"üöÄ Executing SAM-Net: Signature-Augmented Manifold Fusion...\")\nfinal_results = execute_sam_net_final(X_test_proc, df_test, model_sp, expert, le_label)\n\n# Final Reporting\nunique_found = np.unique(np.concatenate([y_test_enc, final_results]))\ntarget_names = [le_label.classes_[i] for i in unique_found]\n\nprint(\"\\n--- SAM-Net FINAL Q1 RESULTS ---\")\nprint(classification_report(y_test_enc, final_results, \n                            labels=unique_found, \n                            target_names=target_names, \n                            zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T14:48:27.038653Z","iopub.execute_input":"2026-01-17T14:48:27.039647Z","iopub.status.idle":"2026-01-17T14:48:27.778840Z","shell.execute_reply.started":"2026-01-17T14:48:27.039614Z","shell.execute_reply":"2026-01-17T14:48:27.777949Z"}},"outputs":[{"name":"stdout","text":"üöÄ Executing SAM-Net: Signature-Augmented Manifold Fusion...\n\n--- SAM-Net FINAL Q1 RESULTS ---\n                 precision    recall  f1-score   support\n\n           back       0.38      0.77      0.51       359\nbuffer_overflow       0.00      0.00      0.00        20\n      ftp_write       0.00      0.00      0.00         3\n   guess_passwd       0.08      0.03      0.04      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.89      0.94      0.91       141\n           land       1.00      1.00      1.00         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      0.93      0.96      4657\n           nmap       0.97      0.96      0.97        73\n         normal       0.80      0.87      0.84      9711\n           perl       0.00      0.00      0.00         2\n            phf       0.00      0.00      0.00         2\n            pod       0.72      0.83      0.77        41\n      portsweep       0.69      0.90      0.78       157\n        rootkit       0.00      0.00      0.00        13\n          satan       0.84      0.93      0.88       735\n          smurf       1.00      0.94      0.97       665\n       teardrop       0.24      0.92      0.38        12\n    warezmaster       0.05      0.04      0.05       944\n\n       accuracy                           0.79     18794\n      macro avg       0.41      0.48      0.43     18794\n   weighted avg       0.76      0.79      0.77     18794\n\n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"def execute_arg_final_fusion(X_proc, df_orig, model_sp, model_xgb, le_label):\n    model_sp.eval()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    with torch.no_grad():\n        # Specialist (MLAR) output\n        logits, features = model_sp(torch.tensor(X_proc, dtype=torch.float32).to(device))\n        probs_sp = torch.softmax(logits * 1.2, dim=1).cpu().numpy()\n        centers = model_sp.centers.detach()\n    \n    # Anchor (XGBoost) output\n    probs_xgb = model_xgb.predict_proba(X_proc)\n    final_preds = []\n    \n    # Safe Index Fetching\n    idx_list = list(le_label.classes_)\n    idx_normal = idx_list.index('normal') if 'normal' in idx_list else -1\n    idx_back = idx_list.index('back') if 'back' in idx_list else -1\n    \n    # Ghost classes for the Specialist to rescue\n    ghosts = ['warezmaster', 'guess_passwd', 'rootkit', 'buffer_overflow']\n    ghost_idx = [idx_list.index(g) for g in ghosts if g in idx_list]\n\n    for i in range(len(X_proc)):\n        p_x = probs_xgb[i]\n        p_s = probs_sp[i]\n        \n        # 1Ô∏è‚É£ TIER 1: THE ACCURACY SHIELD (The 91% Anchor)\n        # If XGBoost is highly confident, DO NOT OVERRIDE.\n        best_x = np.argmax(p_x)\n        if p_x[best_x] > 0.94:\n            final_preds.append(best_x)\n            continue\n            \n        # 2Ô∏è‚É£ TIER 2: RESIDUAL DISCOVERY (The Specialist)\n        # Only if XGBoost is uncertain, we look at the MLAR Specialist\n        sp_choice = np.argmax(p_s)\n        \n        # If the Specialist sees a Ghost Class and has reasonable confidence\n        if sp_choice in ghost_idx and p_s[sp_choice] > 0.25:\n            # GEOMETRIC VALIDATION\n            dist_to_normal = torch.norm(features[i] - centers[idx_normal])\n            dist_to_ghost = torch.norm(features[i] - centers[sp_choice])\n            \n            if dist_to_ghost < dist_to_normal:\n                final_preds.append(sp_choice)\n                continue\n\n        # 3Ô∏è‚É£ TIER 3: THE PROTOCOL TIE-BREAKER\n        if idx_back != -1 and p_x[idx_back] > 0.35 and df_orig['src_bytes'].iloc[i] > 5000:\n            final_preds.append(idx_back)\n        else:\n            # 4Ô∏è‚É£ DEFAULT: Trust the statistical anchor (Stability)\n            final_preds.append(best_x)\n            \n    return np.array(final_preds)\n\nprint(\"üöÄ Executing ARG-Net: Adaptive Residual Gating...\")\nfinal_results = execute_arg_final_fusion(X_test_proc, df_test, model_sp, expert, le_label)\n\n# Reporting\nall_labels = np.unique(np.concatenate([y_test_enc, final_results]))\ntarget_names = [le_label.classes_[i] for i in all_labels]\n\nprint(\"\\n--- ARG-Net (Clubbed Best) FINAL RESULTS ---\")\nprint(classification_report(y_test_enc, final_results, \n                            labels=all_labels, \n                            target_names=target_names, \n                            zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T14:49:23.932355Z","iopub.execute_input":"2026-01-17T14:49:23.932921Z","iopub.status.idle":"2026-01-17T14:49:24.077656Z","shell.execute_reply.started":"2026-01-17T14:49:23.932892Z","shell.execute_reply":"2026-01-17T14:49:24.076843Z"}},"outputs":[{"name":"stdout","text":"üöÄ Executing ARG-Net: Adaptive Residual Gating...\n\n--- ARG-Net (Clubbed Best) FINAL RESULTS ---\n                 precision    recall  f1-score   support\n\n           back       1.00      0.94      0.97       359\nbuffer_overflow       1.00      0.05      0.10        20\n      ftp_write       0.00      0.00      0.00         3\n   guess_passwd       0.00      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.99      0.99      0.99       141\n           land       1.00      0.71      0.83         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      1.00      1.00      4657\n           nmap       0.99      1.00      0.99        73\n         normal       0.81      0.97      0.88      9711\n           perl       0.00      0.00      0.00         2\n            phf       1.00      0.50      0.67         2\n            pod       0.72      0.95      0.82        41\n      portsweep       0.79      0.92      0.85       157\n        rootkit       0.00      0.00      0.00        13\n          satan       0.81      1.00      0.90       735\n          smurf       1.00      1.00      1.00       665\n       teardrop       0.24      1.00      0.39        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       0.00      0.00      0.00       944\n\n       accuracy                           0.87     18794\n      macro avg       0.52      0.50      0.47     18794\n   weighted avg       0.77      0.87      0.81     18794\n\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"def execute_arg_axiomatic_fusion(X_proc, df_orig, model_sp, model_xgb, le_label):\n    model_sp.eval()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    with torch.no_grad():\n        logits, features = model_sp(torch.tensor(X_proc, dtype=torch.float32).to(device))\n        probs_sp = torch.softmax(logits * 1.5, dim=1).cpu().numpy()\n    \n    probs_xgb = model_xgb.predict_proba(X_proc)\n    final_preds = []\n    \n    # --- Index Mapping ---\n    idx_list = list(le_label.classes_)\n    idx_normal = idx_list.index('normal')\n    idx_warez = idx_list.index('warezmaster') if 'warezmaster' in idx_list else -1\n    idx_guess = idx_list.index('guess_passwd') if 'guess_passwd' in idx_list else -1\n    idx_back = idx_list.index('back') if 'back' in idx_list else -1\n\n    for i in range(len(X_proc)):\n        p_x = probs_xgb[i]\n        p_s = probs_sp[i]\n        \n        # 1Ô∏è‚É£ TIER 1: AXIOMATIC INJECTION (The Recall Fix)\n        # If a connection has 'Hot' indicators or 'Failed Logins', \n        # it is LOGICALLY an R2L attack, not 'Normal'.\n        if (df_orig['hot'].iloc[i] > 1) or (df_orig['num_failed_logins'].iloc[i] > 0):\n            # If the Specialist even slightly suspects a rare class, we force it.\n            if idx_warez != -1 and p_s[idx_warez] > 0.05:\n                final_preds.append(idx_warez)\n                continue\n            if idx_guess != -1 and p_s[idx_guess] > 0.05:\n                final_preds.append(idx_guess)\n                continue\n\n        # 2Ô∏è‚É£ TIER 2: THE ACCURACY SHIELD (The 92% Anchor)\n        # If XGBoost is very sure, let it pass to keep Accuracy high.\n        best_x = np.argmax(p_x)\n        if p_x[best_x] > 0.92:\n            final_preds.append(best_x)\n            continue\n            \n        # 3Ô∏è‚É£ TIER 3: PROTOCOL FIX (For 'Back')\n        if idx_back != -1 and p_x[idx_back] > 0.35 and df_orig['src_bytes'].iloc[i] > 5000:\n            final_preds.append(idx_back)\n        else:\n            # 4Ô∏è‚É£ TIER 4: RESIDUAL BLEND\n            # Use a blend for ambiguous cases, favoring the Specialist\n            final_preds.append(np.argmax(0.4 * p_x + 0.6 * p_s))\n            \n    return np.array(final_preds)\n\nprint(\"üöÄ Executing ARG-Net 2.0: Axiomatic Injection...\")\nfinal_results = execute_arg_axiomatic_fusion(X_test_proc, df_test, model_sp, expert, le_label)\n\n# üìä Reporting\nall_labels = np.unique(np.concatenate([y_test_enc, final_results]))\ntarget_names = [le_label.classes_[i] for i in all_labels]\n\nprint(\"\\n--- ARG-Net 2.0 FINAL RESULTS ---\")\nprint(classification_report(y_test_enc, final_results, target_names=target_names, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T14:49:55.033307Z","iopub.execute_input":"2026-01-17T14:49:55.034114Z","iopub.status.idle":"2026-01-17T14:49:55.413178Z","shell.execute_reply.started":"2026-01-17T14:49:55.034082Z","shell.execute_reply":"2026-01-17T14:49:55.412477Z"}},"outputs":[{"name":"stdout","text":"üöÄ Executing ARG-Net 2.0: Axiomatic Injection...\n\n--- ARG-Net 2.0 FINAL RESULTS ---\n                 precision    recall  f1-score   support\n\n           back       1.00      0.71      0.83       359\nbuffer_overflow       0.00      0.00      0.00        20\n      ftp_write       0.00      0.00      0.00         3\n   guess_passwd       1.00      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.95      0.98      0.96       141\n           land       1.00      1.00      1.00         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      1.00      1.00      4657\n           nmap       0.97      0.99      0.98        73\n         normal       0.80      0.97      0.88      9711\n           perl       0.00      0.00      0.00         2\n            phf       0.00      0.00      0.00         2\n            pod       0.71      0.88      0.78        41\n      portsweep       0.69      0.96      0.81       157\n        rootkit       0.00      0.00      0.00        13\n          satan       0.83      1.00      0.91       735\n          smurf       1.00      1.00      1.00       665\n       teardrop       0.24      1.00      0.39        12\n    warezmaster       0.00      0.00      0.00       944\n\n       accuracy                           0.86     18794\n      macro avg       0.49      0.50      0.45     18794\n   weighted avg       0.83      0.86      0.81     18794\n\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"def execute_srf_negative_selection(X_proc, df_orig, model_sp, model_xgb, le_label):\n    model_sp.eval()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    with torch.no_grad():\n        logits, _ = model_sp(torch.tensor(X_proc, dtype=torch.float32).to(device))\n        # Temperature Sharpening (T=0.05) - Extreme focus on the top minority class\n        probs_sp = torch.softmax(logits / 0.05, dim=1).cpu().numpy()\n    \n    probs_xgb = model_xgb.predict_proba(X_proc)\n    final_preds = []\n    \n    # Indices\n    idx_list = list(le_label.classes_)\n    idx_normal = idx_list.index('normal')\n    \n    # Define the \"Stealth Manifold\" (R2L and U2R)\n    stealth_indices = [idx_list.index(c) for c in ['warezmaster', 'guess_passwd', 'rootkit', 'buffer_overflow', 'multihop', 'phf'] if c in idx_list]\n\n    for i in range(len(X_proc)):\n        p_x = probs_xgb[i]\n        p_s = probs_sp[i]\n        \n        # 1Ô∏è‚É£ TIER 1: THE VOLUME ANCHOR (Neptune/Smurf/Satan)\n        # If XGBoost detects a high-volume DoS or Probe, we trust it 100%.\n        best_x = np.argmax(p_x)\n        if best_x != idx_normal and p_x[best_x] > 0.80:\n            final_preds.append(best_x)\n            continue\n            \n        # 2Ô∏è‚É£ TIER 2: NEGATIVE SELECTION (The Research Innovation)\n        # If the sample is 'Not Highly Confident Normal', we FORAGING for Stealth\n        # Even if XGBoost says 'Normal', if its confidence is < 0.99, we forage.\n        if p_x[idx_normal] < 0.995:\n            # Force the specialist to pick the best stealth candidate\n            best_stealth = stealth_indices[np.argmax(p_s[stealth_indices])]\n            \n            # If the Specialist signal for stealth is even 10x stronger than \n            # its signal for 'normal', we override.\n            if p_s[best_stealth] > p_s[idx_normal] * 2:\n                final_preds.append(best_stealth)\n                continue\n        \n        # 3Ô∏è‚É£ TIER 3: THE FIDELITY BASELINE\n        final_preds.append(best_x)\n            \n    return np.array(final_preds)\n\nprint(\"üöÄ Executing SRF-Net: Negative Selection Foraging...\")\nfinal_results = execute_srf_negative_selection(X_test_proc, df_test, model_sp, expert, le_label)\n\n# üìä Reporting\nunique_found = np.unique(np.concatenate([y_test_enc, final_results]))\ntarget_names = [le_label.classes_[i] for i in unique_found]\n\nprint(\"\\n--- SRF-Net FINAL RESULTS ---\")\nprint(classification_report(y_test_enc, final_results, \n                            labels=unique_found, \n                            target_names=target_names, \n                            zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T14:50:40.082810Z","iopub.execute_input":"2026-01-17T14:50:40.083471Z","iopub.status.idle":"2026-01-17T14:50:40.230906Z","shell.execute_reply.started":"2026-01-17T14:50:40.083438Z","shell.execute_reply":"2026-01-17T14:50:40.230239Z"}},"outputs":[{"name":"stdout","text":"üöÄ Executing SRF-Net: Negative Selection Foraging...\n\n--- SRF-Net FINAL RESULTS ---\n                 precision    recall  f1-score   support\n\n           back       1.00      0.94      0.97       359\nbuffer_overflow       1.00      0.05      0.10        20\n      ftp_write       0.00      0.00      0.00         3\n   guess_passwd       0.00      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.99      0.99      0.99       141\n           land       1.00      0.71      0.83         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      0.99      0.99      4657\n           nmap       0.99      1.00      0.99        73\n         normal       0.81      0.97      0.88      9711\n           perl       0.00      0.00      0.00         2\n            phf       1.00      0.50      0.67         2\n            pod       0.72      0.95      0.82        41\n      portsweep       0.79      0.92      0.85       157\n        rootkit       0.00      0.00      0.00        13\n          satan       0.82      1.00      0.90       735\n          smurf       1.00      1.00      1.00       665\n       teardrop       0.24      1.00      0.39        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       0.00      0.00      0.00       944\n\n       accuracy                           0.86     18794\n      macro avg       0.52      0.50      0.47     18794\n   weighted avg       0.77      0.86      0.81     18794\n\n","output_type":"stream"}],"execution_count":75}]}