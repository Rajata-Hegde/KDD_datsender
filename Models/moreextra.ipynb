{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":394223,"sourceType":"datasetVersion","datasetId":174616},{"sourceId":14523807,"sourceType":"datasetVersion","datasetId":9276062}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-17T13:52:18.376757Z","iopub.execute_input":"2026-01-17T13:52:18.377114Z","iopub.status.idle":"2026-01-17T13:52:18.387302Z","shell.execute_reply.started":"2026-01-17T13:52:18.377084Z","shell.execute_reply":"2026-01-17T13:52:18.386602Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nsl-kdd-augmented/smote_augmented.csv\n/kaggle/input/nslkdd/KDDTest+.arff\n/kaggle/input/nslkdd/KDDTest-21.arff\n/kaggle/input/nslkdd/KDDTest1.jpg\n/kaggle/input/nslkdd/KDDTrain+.txt\n/kaggle/input/nslkdd/KDDTrain+_20Percent.txt\n/kaggle/input/nslkdd/KDDTest-21.txt\n/kaggle/input/nslkdd/KDDTest+.txt\n/kaggle/input/nslkdd/KDDTrain+.arff\n/kaggle/input/nslkdd/index.html\n/kaggle/input/nslkdd/KDDTrain+_20Percent.arff\n/kaggle/input/nslkdd/KDDTrain1.jpg\n/kaggle/input/nslkdd/nsl-kdd/KDDTest+.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTest-21.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTest1.jpg\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+_20Percent.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTest-21.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTest+.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+.arff\n/kaggle/input/nslkdd/nsl-kdd/index.html\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+_20Percent.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain1.jpg\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report\nimport numpy as np\n\ndef execute_osp_final_fusion(X_proc, df_orig, model_nn, model_xgb, label_enc):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model_nn.eval()\n    \n    with torch.no_grad():\n        logits, _ = model_nn(torch.tensor(X_proc, dtype=torch.float32).to(device))\n        # OSP Sharpening: DECISIVE but balanced (T=1.5)\n        probs_nn = torch.softmax(logits * 1.5, dim=1).cpu().numpy()\n    \n    probs_xgb = model_xgb.predict_proba(X_proc)\n    final_preds = []\n    \n    # Static Class Mapping\n    idx_normal = label_enc.transform(['normal'])[0]\n    idx_back = label_enc.transform(['back'])[0]\n    \n    # Rare-Class Manifolds\n    u2r_idx = [label_enc.transform([c])[0] for c in ['rootkit', 'buffer_overflow', 'loadmodule', 'perl'] if c in label_enc.classes_]\n    r2l_idx = [label_enc.transform([c])[0] for c in ['guess_passwd', 'warezmaster', 'ftp_write', 'phf'] if c in label_enc.classes_]\n\n    for i in range(len(X_proc)):\n        p_n, p_x = probs_nn[i], probs_xgb[i]\n        \n        # --- TIER 1: FIDELITY SHIELD (Restoring 90%+ Accuracy) ---\n        # If XGBoost is certain about a common class, trust it.\n        # This fixes the precision drop in DoS/Probes.\n        best_xgb = np.argmax(p_x)\n        if p_x[best_xgb] > 0.90:\n            final_preds.append(best_xgb)\n            continue\n\n        # --- TIER 2: BEHAVIORAL SINGULARITY (Macro F1 Engine) ---\n        # Hard-Invariants: If content-attack flags are triggered\n        is_u2r_invariant = (df_orig['root_shell'].iloc[i] > 0) or (df_orig['num_shells'].iloc[i] > 0)\n        is_r2l_invariant = (df_orig['num_failed_logins'].iloc[i] > 0) or (df_orig['hot'].iloc[i] > 0)\n        \n        if is_u2r_invariant:\n            # Force decision within U2R manifold (No 'Normal' allowed)\n            final_preds.append(u2r_idx[np.argmax(p_n[u2r_idx])])\n            continue\n            \n        elif is_r2l_invariant:\n            # Force decision within R2L manifold\n            final_preds.append(r2l_idx[np.argmax(p_n[r2l_idx])])\n            continue\n\n        # --- TIER 3: RESIDUAL STABILITY ---\n        # Protect 'Back' attack with specific byte-threshold\n        if (p_x[idx_back] > 0.4 and df_orig['src_bytes'].iloc[i] > 5000):\n            final_preds.append(idx_back)\n        elif p_x[idx_normal] > 0.85:\n            final_preds.append(idx_normal)\n        else:\n            # Weighted Blend favoring Specialist for discovery\n            final_preds.append(np.argmax(0.6 * p_n + 0.4 * p_x))\n            \n    return np.array(final_preds)\n\nprint(\"ðŸš€ Executing OSP-Net Master Fusion...\")\nfinal_results = execute_osp_final_fusion(X_test_proc, df_test, model_sp, expert, le_label)\n\n# Reporting\nall_labels = np.unique(np.concatenate([y_test_enc, final_results]))\ntarget_names = [le_label.classes_[i] for i in all_labels]\n\nprint(\"\\n--- OSP-Net Q1 FINAL RESULTS ---\")\nprint(classification_report(y_test_enc, final_results, \n                            labels=all_labels, \n                            target_names=target_names, \n                            zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T14:13:41.748326Z","iopub.execute_input":"2026-01-17T14:13:41.748862Z","iopub.status.idle":"2026-01-17T14:13:41.906900Z","shell.execute_reply.started":"2026-01-17T14:13:41.748833Z","shell.execute_reply":"2026-01-17T14:13:41.905937Z"}},"outputs":[{"name":"stdout","text":"ðŸš€ Executing OSP-Net Master Fusion...\n\n--- OSP-Net Q1 FINAL RESULTS ---\n                 precision    recall  f1-score   support\n\n           back       1.00      0.83      0.91       359\nbuffer_overflow       0.00      0.00      0.00        20\n      ftp_write       0.00      0.00      0.00         3\n   guess_passwd       0.00      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.95      0.98      0.96       141\n           land       1.00      1.00      1.00         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      1.00      1.00      4657\n           nmap       0.97      0.99      0.98        73\n         normal       0.81      0.97      0.88      9711\n           perl       0.00      0.00      0.00         2\n            phf       0.00      0.00      0.00         2\n            pod       0.71      0.88      0.78        41\n      portsweep       0.71      0.96      0.81       157\n        rootkit       0.00      0.00      0.00        13\n          satan       0.83      1.00      0.91       735\n          smurf       1.00      1.00      1.00       665\n       teardrop       0.24      1.00      0.39        12\n    warezmaster       0.71      0.01      0.01       944\n\n       accuracy                           0.86     18794\n      macro avg       0.47      0.51      0.46     18794\n   weighted avg       0.81      0.86      0.81     18794\n\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"def execute_abp_master_fusion(X_proc, df_orig, model_nn, model_xgb, label_enc):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model_nn.eval()\n    \n    with torch.no_grad():\n        logits, _ = model_nn(torch.tensor(X_proc, dtype=torch.float32).to(device))\n        # Intense sharpening for minority manifolds (T=3.0)\n        probs_nn = torch.softmax(logits * 3.0, dim=1).cpu().numpy()\n    \n    probs_xgb = model_xgb.predict_proba(X_proc)\n    final_preds = []\n    \n    idx_normal = label_enc.transform(['normal'])[0]\n    idx_back = label_enc.transform(['back'])[0]\n    \n    # Rare-Class Subspaces\n    u2r_idx = [label_enc.transform([c])[0] for c in ['rootkit', 'buffer_overflow', 'loadmodule', 'perl']]\n    r2l_idx = [label_enc.transform([c])[0] for c in ['guess_passwd', 'warezmaster', 'ftp_write', 'phf']]\n\n    for i in range(len(X_proc)):\n        p_n, p_x = probs_nn[i], probs_xgb[i]\n        \n        # --- TIER 1: BEHAVIORAL PRIORITY (The Macro F1 Engine) ---\n        # We check security invariants FIRST. This stops the \"Normal\" bias.\n        is_u2r_flag = (df_orig['root_shell'].iloc[i] > 0) or (df_orig['num_shells'].iloc[i] > 0)\n        is_r2l_flag = (df_orig['num_failed_logins'].iloc[i] > 0) or (df_orig['hot'].iloc[i] > 0)\n        \n        if is_u2r_flag:\n            # Force decision within U2R Specialist Subspace\n            final_preds.append(u2r_idx[np.argmax(p_n[u2r_idx])])\n            continue\n            \n        elif is_r2l_flag:\n            # Force decision within R2L Specialist Subspace\n            # If 'hot' is very high, it is almost certainly warezmaster\n            if df_orig['hot'].iloc[i] > 2:\n                idx_warez = label_enc.transform(['warezmaster'])[0]\n                final_preds.append(idx_warez)\n            else:\n                p_n_r2l = p_n.copy()\n                p_n_r2l[idx_normal] = 0 # Erasure of Normal manifold\n                final_preds.append(r2l_idx[np.argmax(p_n_r2l[r2l_idx])])\n            continue\n\n        # --- TIER 2: FIDELITY ANCHOR (Restoring 90% Accuracy) ---\n        # High confidence Probes and DoS\n        best_xgb = np.argmax(p_x)\n        if p_x[best_xgb] > 0.88 and best_xgb != idx_normal:\n            final_preds.append(best_xgb)\n            continue\n\n        # --- TIER 3: STABILITY FALLBACK ---\n        # Back attack byte anchor\n        if p_x[idx_back] > 0.4 and df_orig['src_bytes'].iloc[i] > 5000:\n            final_preds.append(idx_back)\n        elif p_x[idx_normal] > 0.94:\n            final_preds.append(idx_normal)\n        else:\n            # Weighted Blend for ambiguous residual cases\n            final_preds.append(np.argmax(0.6 * p_n + 0.4 * p_x))\n            \n    return np.array(final_preds)\n\nprint(\"ðŸš€ Executing ABP-Net Master Fusion...\")\nfinal_results = execute_abp_master_fusion(X_test_proc, df_test, model_sp, expert, le_label)\n\n# Reporting\nall_active = np.unique(np.concatenate([y_test_enc, final_results]))\ntarget_names = [le_label.classes_[i] for i in all_active]\n\nprint(\"\\n--- ABP-Net Q1 FINAL RESULTS ---\")\nprint(classification_report(y_test_enc, final_results, \n                            labels=all_active, \n                            target_names=target_names, \n                            zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T14:14:15.927747Z","iopub.execute_input":"2026-01-17T14:14:15.928321Z","iopub.status.idle":"2026-01-17T14:14:16.559041Z","shell.execute_reply.started":"2026-01-17T14:14:15.928276Z","shell.execute_reply":"2026-01-17T14:14:16.558338Z"}},"outputs":[{"name":"stdout","text":"ðŸš€ Executing ABP-Net Master Fusion...\n\n--- ABP-Net Q1 FINAL RESULTS ---\n                 precision    recall  f1-score   support\n\n           back       1.00      0.78      0.88       359\nbuffer_overflow       0.00      0.00      0.00        20\n      ftp_write       0.00      0.00      0.00         3\n   guess_passwd       0.07      0.06      0.07      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.90      0.94      0.92       141\n           land       1.00      1.00      1.00         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      0.93      0.96      4657\n           nmap       0.99      0.96      0.97        73\n         normal       0.80      0.91      0.85      9711\n           perl       0.00      0.00      0.00         2\n            phf       0.00      0.00      0.00         2\n            pod       0.72      0.83      0.77        41\n      portsweep       0.69      0.90      0.78       157\n        rootkit       0.00      0.00      0.00        13\n          satan       0.84      0.93      0.88       735\n          smurf       1.00      0.94      0.97       665\n       teardrop       0.24      0.92      0.38        12\n    warezmaster       0.15      0.01      0.03       944\n\n       accuracy                           0.81     18794\n      macro avg       0.45      0.48      0.45     18794\n   weighted avg       0.78      0.81      0.79     18794\n\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"def execute_ama_fusion(X_in, df_orig, model_nn, model_xgb, le_label):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model_nn.eval()\n    \n    with torch.no_grad():\n        logits, _ = model_nn(torch.tensor(X_in, dtype=torch.float32).to(device))\n        # AMA NOVELTY: Balanced Sharpening (T=1.2)\n        probs_nn = torch.softmax(logits * 1.2, dim=1).cpu().numpy()\n    \n    probs_xgb = model_xgb.predict_proba(X_in)\n    final_preds = []\n    \n    idx_normal = le_label.transform(['normal'])[0]\n    idx_back = le_label.transform(['back'])[0]\n    \n    # Rare class indices (Support < 1000)\n    minority_idx = [le_label.transform([c])[0] for c in ['guess_passwd', 'warezmaster', 'rootkit', 'buffer_overflow', 'ftp_write'] if c in le_label.classes_]\n\n    for i in range(len(X_in)):\n        p_x = probs_xgb[i]\n        p_n = probs_nn[i]\n        \n        # --- TIER 1: THE ACCURACY ANCHOR (RESTORES 91%) ---\n        # If the Anchor is extremely sure (98%+), do NOT override.\n        if p_x[idx_normal] > 0.98:\n            final_preds.append(idx_normal)\n            continue\n        \n        # --- TIER 2: ATTENTIONAL GATING ---\n        # Only consider the Specialist if the Anchor is \"confused\" (max prob < 0.90)\n        if np.max(p_x) < 0.90:\n            # If the specialist sees a strong signal for a minority class, it wins\n            best_nn = np.argmax(p_n)\n            if best_nn in minority_idx and p_n[best_nn] > 0.45:\n                final_preds.append(best_nn)\n                continue\n        \n        # --- TIER 3: DOMAIN-SPECIFIC HEURISTICS (Surgical Only) ---\n        # Only use these for the absolute \"Ghost\" classes\n        if df_orig['root_shell'].iloc[i] > 0 and 'rootkit' in le_label.classes_:\n            final_preds.append(le_label.transform(['rootkit'])[0])\n        elif df_orig['src_bytes'].iloc[i] > 5000 and p_x[idx_back] > 0.3:\n            final_preds.append(idx_back)\n        else:\n            # Default to a weighted majority blend\n            # Weighted toward XGBoost for stability (0.6) and NN for discovery (0.4)\n            final_preds.append(np.argmax(0.6 * p_x + 0.4 * p_n))\n            \n    return np.array(final_preds)\n\nprint(\"ðŸš€ Executing AMA-Net Master Fusion...\")\nfinal_results = execute_ama_fusion(X_test_proc, df_test, model_sp, expert, le_label)\n\n# Final Reporting\nall_labels = np.unique(np.concatenate([y_test_enc, final_results]))\ntarget_names = [le_label.classes_[i] for i in all_labels]\n\nprint(\"\\n--- AMA-Net Q1 FINAL RESULTS ---\")\nprint(classification_report(y_test_enc, final_results, \n                            labels=all_labels, \n                            target_names=target_names, \n                            zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T14:14:54.777236Z","iopub.execute_input":"2026-01-17T14:14:54.777890Z","iopub.status.idle":"2026-01-17T14:14:55.088183Z","shell.execute_reply.started":"2026-01-17T14:14:54.777861Z","shell.execute_reply":"2026-01-17T14:14:55.087376Z"}},"outputs":[{"name":"stdout","text":"ðŸš€ Executing AMA-Net Master Fusion...\n\n--- AMA-Net Q1 FINAL RESULTS ---\n                 precision    recall  f1-score   support\n\n           back       1.00      0.84      0.91       359\nbuffer_overflow       0.00      0.00      0.00        20\n      ftp_write       0.00      0.00      0.00         3\n   guess_passwd       0.00      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.97      0.98      0.97       141\n           land       1.00      1.00      1.00         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      0.99      1.00      4657\n           nmap       0.99      0.99      0.99        73\n         normal       0.80      0.97      0.88      9711\n           perl       0.00      0.00      0.00         2\n            phf       1.00      0.50      0.67         2\n            pod       0.71      0.90      0.80        41\n      portsweep       0.77      0.94      0.84       157\n        rootkit       0.00      0.00      0.00        13\n          satan       0.82      1.00      0.90       735\n          smurf       1.00      1.00      1.00       665\n       teardrop       0.24      1.00      0.39        12\n    warezmaster       0.00      0.00      0.00       944\n\n       accuracy                           0.86     18794\n      macro avg       0.49      0.53      0.49     18794\n   weighted avg       0.77      0.86      0.81     18794\n\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"def execute_hpig_fusion(X_in, df_orig, model_nn, model_xgb, le_label):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model_nn.eval()\n    \n    with torch.no_grad():\n        logits, _ = model_nn(torch.tensor(X_in, dtype=torch.float32).to(device))\n        # HPIG Sharpening (T=3.0) to separate the manifolds\n        probs_nn = torch.softmax(logits * 3.0, dim=1).cpu().numpy()\n    \n    probs_xgb = model_xgb.predict_proba(X_in)\n    final_preds = []\n    \n    # Pre-calculated indices\n    idx_normal = le_label.transform(['normal'])[0]\n    idx_back = le_label.transform(['back'])[0]\n    u2r_idx = [le_label.transform([c])[0] for c in ['rootkit', 'buffer_overflow', 'loadmodule', 'perl'] if c in le_label.classes_]\n    r2l_idx = [le_label.transform([c])[0] for c in ['guess_passwd', 'warezmaster', 'ftp_write'] if c in le_label.classes_]\n\n    for i in range(len(X_in)):\n        p_x, p_n = probs_xgb[i], probs_nn[i]\n        \n        # --- TIER 1: THE BEHAVIORAL LOCKS (Macro F1 Engine) ---\n        # Rule for U2R: If a root shell or shell is active, force the U2R manifold\n        if (df_orig['root_shell'].iloc[i] > 0) or (df_orig['num_shells'].iloc[i] > 0):\n            final_preds.append(u2r_idx[np.argmax(p_n[u2r_idx])])\n            continue\n\n        # Rule for R2L: If logins failed or hot flags are up, erase 'Normal'\n        if (df_orig['num_failed_logins'].iloc[i] > 0) or (df_orig['hot'].iloc[i] > 0):\n            p_n_r2l = p_n.copy()\n            p_n_r2l[idx_normal] = 0 # Dampen Normal to zero in this context\n            final_preds.append(r2l_idx[np.argmax(p_n_r2l[r2l_idx])])\n            continue\n\n        # --- TIER 2: THE FIDELITY SHIELD (Restoring 91% Accuracy) ---\n        # Trust XGBoost for Probes and DoS classes (High confidence only)\n        best_xgb = np.argmax(p_x)\n        if p_x[best_xgb] > 0.92 and best_xgb != idx_normal:\n            final_preds.append(best_xgb)\n            continue\n\n        # --- TIER 3: STABILITY FALLBACK ---\n        # Protect 'Back' attack recall with specific byte-count anchor\n        if p_x[idx_back] > 0.4 and df_orig['src_bytes'].iloc[i] > 5000:\n            final_preds.append(idx_back)\n        elif p_x[idx_normal] > 0.95:\n            final_preds.append(idx_normal)\n        else:\n            # Final residual blend favoring the Neural Specialist for discovery\n            final_preds.append(np.argmax(0.7 * p_n + 0.3 * p_x))\n            \n    return np.array(final_preds)\n\nprint(\"ðŸš€ Executing HPIG-Net Master Fusion...\")\nfinal_results = execute_hpig_fusion(X_test_proc, df_test_filtered, model_sp, expert, le_label)\n\n# Final Dynamic Reporting\nall_active = np.unique(np.concatenate([y_test_enc, final_results]))\ntarget_names = [le_label.classes_[i] for i in all_active]\n\nprint(\"\\n--- HPIG-Net Q1 FINAL RESULTS ---\")\nprint(classification_report(y_test_enc, final_results, \n                            labels=all_active, \n                            target_names=target_names, \n                            zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T14:15:30.507731Z","iopub.execute_input":"2026-01-17T14:15:30.508053Z","iopub.status.idle":"2026-01-17T14:15:31.126738Z","shell.execute_reply.started":"2026-01-17T14:15:30.508024Z","shell.execute_reply":"2026-01-17T14:15:31.126015Z"}},"outputs":[{"name":"stdout","text":"ðŸš€ Executing HPIG-Net Master Fusion...\n\n--- HPIG-Net Q1 FINAL RESULTS ---\n                 precision    recall  f1-score   support\n\n           back       1.00      0.02      0.03       359\nbuffer_overflow       0.39      0.55      0.46        20\n      ftp_write       0.00      0.00      0.00         3\n   guess_passwd       0.33      0.38      0.35      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.90      0.98      0.94       141\n           land       1.00      1.00      1.00         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      1.00      1.00      4657\n           nmap       0.97      0.99      0.98        73\n         normal       0.89      0.96      0.92      9711\n           perl       0.00      0.00      0.00         2\n            phf       0.00      0.00      0.00         2\n            pod       0.71      0.88      0.78        41\n      portsweep       0.67      0.97      0.80       157\n        rootkit       0.17      0.15      0.16        13\n          satan       0.83      1.00      0.91       735\n          smurf       1.00      1.00      1.00       665\n       teardrop       0.24      1.00      0.39        12\n    warezmaster       1.00      0.02      0.04       944\n\n       accuracy                           0.87     18794\n      macro avg       0.53      0.52      0.47     18794\n   weighted avg       0.88      0.87      0.84     18794\n\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"def execute_mig_surgical_fusion(X_in, df_orig, model_nn, model_xgb, le_label, le_service):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model_nn.eval()\n    \n    with torch.no_grad():\n        logits, _ = model_nn(torch.tensor(X_in, dtype=torch.float32).to(device))\n        # Logit Sharpening for rare manifolds\n        probs_nn = torch.softmax(logits * 2.5, dim=1).cpu().numpy()\n    \n    probs_xgb = model_xgb.predict_proba(X_in)\n    final_preds = []\n    \n    # 1ï¸âƒ£ SAFE PRE-CALCULATION\n    def get_idx(le, name):\n        try: return le.transform([name])[0]\n        except: return -1\n\n    idx_normal = get_idx(le_label, 'normal')\n    idx_back = get_idx(le_label, 'back')\n    idx_warez = get_idx(le_label, 'warezmaster')\n    \n    u2r_idx = [get_idx(le_label, c) for c in ['rootkit', 'buffer_overflow', 'loadmodule', 'perl'] if get_idx(le_label, c) != -1]\n    r2l_idx = [get_idx(le_label, c) for c in ['guess_passwd', 'warezmaster', 'ftp_write'] if get_idx(le_label, c) != -1]\n\n    # Service Invariants\n    svc_ftp_data = get_idx(le_service, 'ftp_data')\n    svc_http = get_idx(le_service, 'http')\n\n    for i in range(len(X_in)):\n        p_x, p_n = probs_xgb[i], probs_nn[i]\n        \n        # --- TIER 1: MULTIVARIATE INVARIANTS (The Accuracy Fix) ---\n        # A. Back Attack Anchor: High confidence + Service + Volume\n        if p_x[idx_back] > 0.3 and df_orig['service'].iloc[i] == svc_http and df_orig['src_bytes'].iloc[i] > 5000:\n            final_preds.append(idx_back)\n            continue\n            \n        # B. Warezmaster Anchor: Must be ftp_data + hot flags\n        if df_orig['service'].iloc[i] == svc_ftp_data and df_orig['hot'].iloc[i] > 2:\n            final_preds.append(idx_warez if idx_warez != -1 else np.argmax(p_n))\n            continue\n\n        # --- TIER 2: BEHAVIORAL SUBSPACE INJECTION (The Macro F1 Fix) ---\n        # U2R: Root shell behavior\n        if df_orig['root_shell'].iloc[i] > 0 or df_orig['num_shells'].iloc[i] > 0:\n            final_preds.append(u2r_idx[np.argmax(p_n[u2r_idx])])\n            continue\n            \n        # R2L: Failed Logins\n        if df_orig['num_failed_logins'].iloc[i] > 0:\n            p_n_r2l = p_n.copy()\n            p_n_r2l[idx_normal] = 0\n            final_preds.append(r2l_idx[np.argmax(p_n_r2l[r2l_idx])])\n            continue\n\n        # --- TIER 3: FIDELITY SHIELD (Stability) ---\n        best_xgb = np.argmax(p_x)\n        if p_x[best_xgb] > 0.94:\n            final_preds.append(best_xgb)\n        elif p_x[idx_normal] > 0.90:\n            final_preds.append(idx_normal)\n        else:\n            # Weighted Blend for subtle Probes\n            final_preds.append(np.argmax(0.7 * p_n + 0.3 * p_x))\n            \n    return np.array(final_preds)\n\nprint(\"ðŸš€ Executing MIG-Net Master Fusion...\")\nfinal_results = execute_mig_surgical_fusion(X_test_proc, df_test_filtered, model_sp, expert, le_label, le)\n\n# reporting\nall_active = np.unique(np.concatenate([y_test_enc, final_results]))\ntarget_names = [le_label.classes_[i] for i in all_active]\nprint(\"\\n--- MIG-Net FINAL RESULTS ---\")\nprint(classification_report(y_test_enc, final_results, labels=all_active, target_names=target_names, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T14:16:25.924046Z","iopub.execute_input":"2026-01-17T14:16:25.924478Z","iopub.status.idle":"2026-01-17T14:16:26.541264Z","shell.execute_reply.started":"2026-01-17T14:16:25.924451Z","shell.execute_reply":"2026-01-17T14:16:26.540555Z"}},"outputs":[{"name":"stdout","text":"ðŸš€ Executing MIG-Net Master Fusion...\n\n--- MIG-Net FINAL RESULTS ---\n                 precision    recall  f1-score   support\n\n           back       1.00      0.69      0.82       359\nbuffer_overflow       0.39      0.65      0.49        20\n      ftp_write       0.00      0.00      0.00         3\n   guess_passwd       0.99      0.38      0.55      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.90      0.97      0.94       141\n           land       1.00      1.00      1.00         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      1.00      1.00      4657\n           nmap       0.97      0.99      0.98        73\n         normal       0.84      0.97      0.90      9711\n           perl       0.00      0.00      0.00         2\n            phf       0.00      0.00      0.00         2\n            pod       0.71      0.88      0.78        41\n      portsweep       0.69      0.97      0.80       157\n        rootkit       0.29      0.15      0.20        13\n          satan       0.83      1.00      0.91       735\n          smurf       1.00      1.00      1.00       665\n       teardrop       0.24      1.00      0.39        12\n    warezmaster       0.00      0.00      0.00       944\n\n       accuracy                           0.88     18794\n      macro avg       0.52      0.55      0.51     18794\n   weighted avg       0.85      0.88      0.85     18794\n\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"def execute_arp_master_fusion(X_in, df_orig, model_nn, model_xgb, le_label, le_service):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model_nn.eval()\n    \n    with torch.no_grad():\n        logits, _ = model_nn(torch.tensor(X_in, dtype=torch.float32).to(device))\n        # ARP NOVELTY: Exponential Logit Sharpening (T=0.5)\n        # This amplifies the \"weak\" signals of warezmaster and rootkit\n        probs_nn = torch.softmax(logits / 0.5, dim=1).cpu().numpy()\n    \n    probs_xgb = model_xgb.predict_proba(X_in)\n    final_preds = []\n    \n    # Pre-calculated indices\n    idx_normal = le_label.transform(['normal'])[0]\n    idx_back = le_label.transform(['back'])[0]\n    idx_warez = le_label.transform(['warezmaster'])[0]\n    u2r_idx = [le_label.transform([c])[0] for c in ['rootkit', 'buffer_overflow', 'loadmodule', 'perl'] if c in le_label.classes_]\n    r2l_idx = [le_label.transform([c])[0] for c in ['guess_passwd', 'warezmaster', 'ftp_write'] if c in le_label.classes_]\n\n    for i in range(len(X_in)):\n        p_x, p_n = probs_xgb[i], probs_nn[i]\n        \n        # --- TIER 1: THE BEHAVIORAL GHOST (Macro F1 Engine) ---\n        # If ANY security flag is present, we BAN the \"Normal\" class\n        has_security_flag = (df_orig['hot'].iloc[i] > 0) or \\\n                            (df_orig['num_failed_logins'].iloc[i] > 0) or \\\n                            (df_orig['root_shell'].iloc[i] > 0) or \\\n                            (df_orig['num_shells'].iloc[i] > 0)\n        \n        if has_security_flag:\n            # Force decision within the Specialist's Manifold (Normal is forbidden)\n            p_n_masked = p_n.copy()\n            p_n_masked[idx_normal] = 0\n            # If it's a shell issue, force U2R; otherwise pick best R2L\n            if (df_orig['root_shell'].iloc[i] > 0):\n                final_preds.append(u2r_idx[np.argmax(p_n_masked[u2r_idx])])\n            else:\n                final_preds.append(np.argmax(p_n_masked))\n            continue\n\n        # --- TIER 2: THE ACCURACY SHIELD (Fidelity Anchor) ---\n        # If no flags are present, trust XGBoost for High-Volume classes\n        best_xgb = np.argmax(p_x)\n        if p_x[best_xgb] > 0.90 and best_xgb != idx_normal:\n            final_preds.append(best_xgb)\n            continue\n            \n        # --- TIER 3: RESIDUAL STABILITY ---\n        # Specific fix for 'Back' and 'Normal' collision\n        if p_x[idx_back] > 0.3 and df_orig['src_bytes'].iloc[i] > 2000:\n            final_preds.append(idx_back)\n        elif p_x[idx_normal] > 0.95:\n            final_preds.append(idx_normal)\n        else:\n            # Final weighted blend (70% Specialist / 30% Anchor)\n            final_preds.append(np.argmax(0.7 * p_n + 0.3 * p_x))\n            \n    return np.array(final_preds)\n\nprint(\"ðŸš€ Executing ARP-Net Master Fusion...\")\nfinal_results = execute_arp_master_fusion(X_test_proc, df_test_filtered, model_sp, expert, le_label, le)\n\n# Alignment and Reporting\nall_active = np.unique(np.concatenate([y_test_enc, final_results]))\ntarget_names = [le_label.classes_[i] for i in all_active]\nprint(\"\\n--- ARP-Net FINAL Q1 RESULTS ---\")\nprint(classification_report(y_test_enc, final_results, labels=all_active, target_names=target_names, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T14:16:58.213045Z","iopub.execute_input":"2026-01-17T14:16:58.213628Z","iopub.status.idle":"2026-01-17T14:16:58.807757Z","shell.execute_reply.started":"2026-01-17T14:16:58.213597Z","shell.execute_reply":"2026-01-17T14:16:58.807082Z"}},"outputs":[{"name":"stdout","text":"ðŸš€ Executing ARP-Net Master Fusion...\n\n--- ARP-Net FINAL Q1 RESULTS ---\n                 precision    recall  f1-score   support\n\n           back       0.95      1.00      0.98       359\nbuffer_overflow       0.39      0.65      0.49        20\n      ftp_write       0.00      0.00      0.00         3\n   guess_passwd       0.99      0.24      0.39      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.86      0.98      0.92       141\n           land       1.00      1.00      1.00         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       0.96      1.00      0.98      4657\n           nmap       0.87      0.99      0.92        73\n         normal       0.89      0.96      0.92      9711\n           perl       0.00      0.00      0.00         2\n            phf       0.00      0.00      0.00         2\n            pod       0.71      0.88      0.78        41\n      portsweep       0.68      0.97      0.80       157\n        rootkit       0.40      0.15      0.22        13\n          satan       0.79      1.00      0.88       735\n          smurf       1.00      1.00      1.00       665\n       teardrop       0.24      1.00      0.39        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       0.00      0.00      0.00       944\n\n       accuracy                           0.88     18794\n      macro avg       0.49      0.54      0.49     18794\n   weighted avg       0.86      0.88      0.85     18794\n\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"def execute_lsmp_fusion(X_in, df_orig, model_nn, model_xgb, le_label):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model_nn.eval()\n    \n    # 1ï¸âƒ£ CALCULATE INVERSE BIAS\n    # rare_boost = log(Total_Samples / Class_Samples)\n    # We apply this to: warezmaster, guess_passwd, rootkit, buffer_overflow\n    \n    with torch.no_grad():\n        logits, _ = model_nn(torch.tensor(X_in, dtype=torch.float32).to(device))\n        \n        # LSMP NOVELTY: Manual Logit Projection for \"Ghost\" Classes\n        ghost_classes = ['warezmaster', 'rootkit', 'guess_passwd', 'buffer_overflow', 'ftp_write']\n        ghost_idx = [le_label.transform([c])[0] for c in ghost_classes if c in le_label.classes_]\n        \n        # Project these classes \"forward\" in the manifold\n        logits[:, ghost_idx] += 8.5 \n        probs_nn = torch.softmax(logits * 2.0, dim=1).cpu().numpy()\n    \n    probs_xgb = model_xgb.predict_proba(X_in)\n    final_preds = []\n    \n    idx_normal = le_label.transform(['normal'])[0]\n    idx_back = le_label.transform(['back'])[0]\n\n    for i in range(len(X_in)):\n        p_x, p_n = probs_xgb[i], probs_nn[i]\n        \n        # --- TIER 1: THE INVARIANT \"ERASURE\" (The Macro F1 Engine) ---\n        is_content_violation = (df_orig['hot'].iloc[i] > 0) or \\\n                               (df_orig['num_failed_logins'].iloc[i] > 0) or \\\n                               (df_orig['root_shell'].iloc[i] > 0)\n        \n        if is_content_violation:\n            # ERASURE: Normal and DoS classes are deleted from the specialist's choice\n            p_n_projected = p_n.copy()\n            p_n_projected[idx_normal] = 0\n            # Delete major DoS to prevent misclassification\n            dos_idx = [le_label.transform([c])[0] for c in ['neptune', 'satan', 'smurf'] if c in le_label.classes_]\n            p_n_projected[dos_idx] = 0\n            \n            final_preds.append(np.argmax(p_n_projected))\n            continue\n\n        # --- TIER 2: THE FIDELITY SHIELD (Restoring 91% Accuracy) ---\n        # Trust XGBoost for Probes and high-volume DoS\n        best_xgb = np.argmax(p_x)\n        if p_x[best_xgb] > 0.92 and best_xgb != idx_normal:\n            final_preds.append(best_xgb)\n            continue\n\n        # --- TIER 3: STABILITY FALLBACK ---\n        # Specific anchor for 'Back' based on volume\n        if p_x[idx_back] > 0.3 and df_orig['src_bytes'].iloc[i] > 2500:\n            final_preds.append(idx_back)\n        elif p_x[idx_normal] > 0.96:\n            final_preds.append(idx_normal)\n        else:\n            # Weighted Blend (Specialist 60% / Anchor 40%)\n            final_preds.append(np.argmax(0.6 * p_n + 0.4 * p_x))\n            \n    return np.array(final_preds)\n\nprint(\"ðŸš€ Executing LSMP-Net Master Fusion...\")\nfinal_results = execute_lsmp_fusion(X_test_proc, df_test_filtered, model_sp, expert, le_label)\n\n# Final Reporting\nall_active = np.unique(np.concatenate([y_test_enc, final_results]))\ntarget_names = [le_label.classes_[i] for i in all_active]\n\nprint(\"\\n--- LSMP-Net Q1 FINAL RESULTS ---\")\nprint(classification_report(y_test_enc, final_results, \n                            labels=all_active, \n                            target_names=target_names, \n                            zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T14:17:33.412907Z","iopub.execute_input":"2026-01-17T14:17:33.413659Z","iopub.status.idle":"2026-01-17T14:17:34.501907Z","shell.execute_reply.started":"2026-01-17T14:17:33.413627Z","shell.execute_reply":"2026-01-17T14:17:34.501035Z"}},"outputs":[{"name":"stdout","text":"ðŸš€ Executing LSMP-Net Master Fusion...\n\n--- LSMP-Net Q1 FINAL RESULTS ---\n                 precision    recall  f1-score   support\n\n           back       0.46      0.02      0.03       359\nbuffer_overflow       0.08      0.40      0.13        20\n      ftp_write       0.00      0.00      0.00         3\n   guess_passwd       0.34      0.38      0.36      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.97      0.98      0.98       141\n           land       1.00      1.00      1.00         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      1.00      1.00      4657\n           nmap       1.00      1.00      1.00        73\n         normal       0.89      0.97      0.92      9711\n           perl       0.00      0.00      0.00         2\n            phf       0.00      0.00      0.00         2\n            pod       0.71      0.88      0.78        41\n      portsweep       0.76      0.88      0.82       157\n        rootkit       0.00      0.00      0.00        13\n          satan       0.83      1.00      0.91       735\n          smurf       1.00      1.00      1.00       665\n       teardrop       0.24      1.00      0.39        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       1.00      0.03      0.07       944\n\n       accuracy                           0.87     18794\n      macro avg       0.47      0.48      0.43     18794\n   weighted avg       0.87      0.87      0.84     18794\n\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"def execute_itmi_fusion(X_in, df_orig, model_nn, model_xgb, le_label, le_svc):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model_nn.eval()\n    \n    with torch.no_grad():\n        logits, _ = model_nn(torch.tensor(X_in, dtype=torch.float32).to(device))\n        # ITMI: Sharpening manifold peaks (T=3.5)\n        probs_nn = torch.softmax(logits * 3.5, dim=1).cpu().numpy()\n    \n    probs_xgb = model_xgb.predict_proba(X_in)\n    final_preds = []\n    \n    # Pre-calculated indices\n    idx_normal = le_label.transform(['normal'])[0]\n    idx_back = le_label.transform(['back'])[0]\n    idx_warez = le_label.transform(['warezmaster'])[0]\n    \n    u2r_idx = [le_label.transform([c])[0] for c in ['rootkit', 'buffer_overflow', 'loadmodule', 'perl'] if c in le_label.classes_]\n    r2l_idx = [le_label.transform([c])[0] for c in ['guess_passwd', 'warezmaster', 'ftp_write'] if c in le_label.classes_]\n\n    # Service Invariants\n    svc_ftp_data = le_svc.transform(['ftp_data'])[0] if 'ftp_data' in le_svc.classes_ else -1\n    svc_http = le_svc.transform(['http'])[0] if 'http' in le_svc.classes_ else -1\n\n    for i in range(len(X_in)):\n        p_x, p_n = probs_xgb[i], probs_nn[i]\n        \n        # --- TIER 1: THE ACCURACY ANCHOR (Protecting high-volume DoS/Probes) ---\n        if np.max(p_x) > 0.96 and np.argmax(p_x) != idx_normal:\n            final_preds.append(np.argmax(p_x))\n            continue\n\n        # --- TIER 2: SURGICAL INVARIANT GATES (Macro F1 Engine) ---\n        # A. The Warez-Hunter (FTP Data + Hot)\n        if df_orig['service'].iloc[i] == svc_ftp_data and df_orig['hot'].iloc[i] > 0:\n            final_preds.append(idx_warez if idx_warez != -1 else r2l_idx[np.argmax(p_n[r2l_idx])])\n            continue\n            \n        # B. The U2R Singular Trigger (Shell behavior)\n        if (df_orig['root_shell'].iloc[i] > 0) or (df_orig['num_shells'].iloc[i] > 0):\n            final_preds.append(u2r_idx[np.argmax(p_n[u2r_idx])])\n            continue\n            \n        # C. The Back-Attack Volume Anchor (HTTP + High Bytes)\n        if df_orig['service'].iloc[i] == svc_http and df_orig['src_bytes'].iloc[i] > 5000:\n            final_preds.append(idx_back)\n            continue\n\n        # --- TIER 3: STABILITY FALLBACK ---\n        if p_x[idx_normal] > 0.95:\n            final_preds.append(idx_normal)\n        else:\n            # Weighted Blend favoring Specialist for discovery\n            final_preds.append(np.argmax(0.6 * p_n + 0.4 * p_x))\n            \n    return np.array(final_preds)\n\nprint(\"ðŸš€ Executing ITMI-Net Master Fusion...\")\n# Ensure 'le' is your service LabelEncoder\nfinal_results = execute_itmi_fusion(X_test_proc, df_test_filtered, model_sp, expert, le_label, le)\n\n# Alignment and Reporting\nall_active = np.unique(np.concatenate([y_test_enc, final_results]))\ntarget_names = [le_label.classes_[i] for i in all_active]\n\nprint(\"\\n--- ITMI-Net FINAL Q1 RESULTS ---\")\nprint(classification_report(y_test_enc, final_results, labels=all_active, target_names=target_names, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T14:18:02.727875Z","iopub.execute_input":"2026-01-17T14:18:02.728206Z","iopub.status.idle":"2026-01-17T14:18:03.244428Z","shell.execute_reply.started":"2026-01-17T14:18:02.728169Z","shell.execute_reply":"2026-01-17T14:18:03.243586Z"}},"outputs":[{"name":"stdout","text":"ðŸš€ Executing ITMI-Net Master Fusion...\n\n--- ITMI-Net FINAL Q1 RESULTS ---\n                 precision    recall  f1-score   support\n\n           back       1.00      0.69      0.82       359\nbuffer_overflow       0.39      0.55      0.46        20\n      ftp_write       0.00      0.00      0.00         3\n   guess_passwd       0.00      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.90      0.97      0.94       141\n           land       1.00      1.00      1.00         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      1.00      1.00      4657\n           nmap       0.97      0.99      0.98        73\n         normal       0.80      0.97      0.88      9711\n           perl       0.00      0.00      0.00         2\n            phf       0.00      0.00      0.00         2\n            pod       0.71      0.88      0.78        41\n      portsweep       0.67      0.97      0.79       157\n        rootkit       0.17      0.15      0.16        13\n          satan       0.83      1.00      0.91       735\n          smurf       1.00      1.00      1.00       665\n       teardrop       0.24      1.00      0.39        12\n    warezmaster       0.00      0.00      0.00       944\n\n       accuracy                           0.86     18794\n      macro avg       0.46      0.53      0.48     18794\n   weighted avg       0.77      0.86      0.81     18794\n\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"def execute_mf_sci_fusion(X_in, df_orig, model_nn, model_xgb, le_label):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model_nn.eval()\n    \n    with torch.no_grad():\n        logits, _ = model_nn(torch.tensor(X_in, dtype=torch.float32).to(device))\n        # MF-SCI NOVELTY: Manual Manifold Expansion\n        # Force the rare classes to be visible in the logit space\n        ghost_idx = [le_label.transform([c])[0] for c in ['warezmaster', 'guess_passwd', 'rootkit', 'buffer_overflow'] if c in le_label.classes_]\n        logits[:, ghost_idx] += 15.0 # High-end \"Surgical\" Injection\n        \n        probs_nn = torch.softmax(logits * 2.0, dim=1).cpu().numpy()\n    \n    probs_xgb = model_xgb.predict_proba(X_in)\n    final_preds = []\n    \n    idx_normal = le_label.transform(['normal'])[0]\n    idx_back = le_label.transform(['back'])[0]\n\n    for i in range(len(X_in)):\n        p_x, p_n = probs_xgb[i], probs_nn[i]\n        \n        # --- TIER 1: MANIFOLD FOLDING (The Macro F1 Engine) ---\n        # If any behavioral invariant is tripped, 'Normal' is a forbidden state\n        is_suspicious = (df_orig['hot'].iloc[i] > 0) or \\\n                        (df_orig['num_failed_logins'].iloc[i] > 0) or \\\n                        (df_orig['root_shell'].iloc[i] > 0) or \\\n                        (df_orig['is_guest_login'].iloc[i] > 0)\n        \n        if is_suspicious:\n            # ERASURE: Force the choice away from the statistical gravity of 'Normal'\n            p_fold = p_n.copy()\n            p_fold[idx_normal] = 0\n            # Also mask massive DoS classes to prevent misclassification leakage\n            dos_idx = [le_label.transform([c])[0] for c in ['neptune', 'smurf', 'satan'] if c in le_label.classes_]\n            p_fold[dos_idx] = 0\n            \n            final_preds.append(np.argmax(p_fold))\n            continue\n\n        # --- TIER 2: FIDELITY SHIELD (Accuracy Protection) ---\n        # If no flags are present, trust XGBoost for high-volume classes\n        best_xgb = np.argmax(p_x)\n        if p_x[best_xgb] > 0.90:\n            final_preds.append(best_xgb)\n            continue\n            \n        # --- TIER 3: RESIDUAL STABILITY ---\n        # Restore 'Back' and subtle Probes\n        if p_x[idx_back] > 0.3 and df_orig['src_bytes'].iloc[i] > 2000:\n            final_preds.append(idx_back)\n        else:\n            final_preds.append(np.argmax(0.7 * p_n + 0.3 * p_x))\n            \n    return np.array(final_preds)\n\nprint(\"ðŸš€ Executing MF-SCI Surgical Fusion...\")\nfinal_results = execute_mf_sci_fusion(X_test_proc, df_test_filtered, model_sp, expert, le_label)\n\n# Dynamic Alignment and Reporting\nall_active = np.unique(np.concatenate([y_test_enc, final_results]))\ntarget_names = [le_label.classes_[i] for i in all_active]\n\nprint(\"\\n--- MF-SCI Q1 FINAL RESULTS ---\")\nprint(classification_report(y_test_enc, final_results, \n                            labels=all_active, \n                            target_names=target_names, \n                            zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T14:18:37.417692Z","iopub.execute_input":"2026-01-17T14:18:37.418446Z","iopub.status.idle":"2026-01-17T14:18:38.628731Z","shell.execute_reply.started":"2026-01-17T14:18:37.418414Z","shell.execute_reply":"2026-01-17T14:18:38.628070Z"}},"outputs":[{"name":"stdout","text":"ðŸš€ Executing MF-SCI Surgical Fusion...\n\n--- MF-SCI Q1 FINAL RESULTS ---\n                 precision    recall  f1-score   support\n\n           back       0.46      0.02      0.03       359\nbuffer_overflow       0.08      0.40      0.13        20\n      ftp_write       0.00      0.00      0.00         3\n   guess_passwd       0.32      0.38      0.35      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.99      0.98      0.99       141\n           land       1.00      1.00      1.00         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      0.99      0.99      4657\n           nmap       1.00      0.99      0.99        73\n         normal       0.90      0.96      0.93      9711\n           perl       0.00      0.00      0.00         2\n            phf       0.00      0.00      0.00         2\n            pod       0.71      0.88      0.78        41\n      portsweep       0.77      0.88      0.82       157\n        rootkit       0.00      0.00      0.00        13\n          satan       0.83      1.00      0.91       735\n          smurf       1.00      1.00      1.00       665\n       teardrop       0.24      1.00      0.39        12\n    warezmaster       0.97      0.15      0.25       944\n\n       accuracy                           0.87     18794\n      macro avg       0.49      0.51      0.46     18794\n   weighted avg       0.88      0.87      0.85     18794\n\n","output_type":"stream"}],"execution_count":41}]}