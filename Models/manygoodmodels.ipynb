{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":394223,"sourceType":"datasetVersion","datasetId":174616},{"sourceId":14523807,"sourceType":"datasetVersion","datasetId":9276062}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-17T11:57:30.176866Z","iopub.execute_input":"2026-01-17T11:57:30.177192Z","iopub.status.idle":"2026-01-17T11:57:30.207055Z","shell.execute_reply.started":"2026-01-17T11:57:30.177145Z","shell.execute_reply":"2026-01-17T11:57:30.206368Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nsl-kdd-augmented/smote_augmented.csv\n/kaggle/input/nslkdd/KDDTest+.arff\n/kaggle/input/nslkdd/KDDTest-21.arff\n/kaggle/input/nslkdd/KDDTest1.jpg\n/kaggle/input/nslkdd/KDDTrain+.txt\n/kaggle/input/nslkdd/KDDTrain+_20Percent.txt\n/kaggle/input/nslkdd/KDDTest-21.txt\n/kaggle/input/nslkdd/KDDTest+.txt\n/kaggle/input/nslkdd/KDDTrain+.arff\n/kaggle/input/nslkdd/index.html\n/kaggle/input/nslkdd/KDDTrain+_20Percent.arff\n/kaggle/input/nslkdd/KDDTrain1.jpg\n/kaggle/input/nslkdd/nsl-kdd/KDDTest+.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTest-21.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTest1.jpg\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+_20Percent.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTest-21.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTest+.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+.arff\n/kaggle/input/nslkdd/nsl-kdd/index.html\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+_20Percent.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain1.jpg\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report\nimport numpy as np\n\n# ===========================================\n# 1️⃣ Novelty: Center-Loss Expansion Specialist\n# ===========================================\nclass MLAR_Specialist(nn.Module):\n    def __init__(self, input_dim, num_classes, embed_dim=2048):\n        super().__init__()\n        self.feat = nn.Sequential(\n            nn.Linear(input_dim, embed_dim),\n            nn.BatchNorm1d(embed_dim),\n            nn.SiLU(),\n            nn.Dropout(0.4)\n        )\n        self.classifier = nn.Linear(embed_dim, num_classes)\n        # Learnable Anchors (Centers) for each class\n        self.centers = nn.Parameter(torch.randn(num_classes, embed_dim))\n\n    def forward(self, x):\n        features = self.feat(x)\n        logits = self.classifier(features)\n        return logits, features\n\n# ===========================================\n# 2️⃣ Custom Loss: Soft-Margin + Center Penalty\n# ===========================================\ndef mlar_criterion(logits, features, targets, centers, weight, lambd=0.01):\n    # Standard Weighted Cross Entropy\n    ce_loss = F.cross_entropy(logits, targets, weight=weight)\n    \n    # Center Loss: Forces features to cluster around class anchors\n    batch_centers = centers[targets]\n    center_loss = F.mse_loss(features, batch_centers)\n    \n    return ce_loss + lambd * center_loss\n\n# ===========================================\n# 3️⃣ Training the Specialist\n# ===========================================\nmodel_sp = MLAR_Specialist(X_train_proc.shape[1], num_classes).to(device)\noptimizer = torch.optim.AdamW(model_sp.parameters(), lr=3e-4, weight_decay=1e-2)\n\n# Specific weights for classes with poor precision/recall balance\nsp_weights = torch.ones(num_classes).to(device)\nfor idx in hard_indices: sp_weights[idx] = 40.0 # Extreme focus\n\nfor epoch in range(30):\n    model_sp.train()\n    for xb, yb in sp_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        logits, feats = model_sp(xb)\n        loss = mlar_criterion(logits, feats, yb, model_sp.centers, sp_weights)\n        loss.backward()\n        optimizer.step()\n\n# ===========================================\n# 4️⃣ Expert: XGBoost (Optimized for Macro F1)\n# ===========================================\nexpert = XGBClassifier(tree_method='hist', device='cuda', \n                       n_estimators=300, max_depth=10, \n                       learning_rate=0.05, colsample_bytree=0.8)\nexpert.fit(X_train_proc, y_train_enc)\n\n# ===========================================\n# 5️⃣ Final Inference: Dynamic Bayesian Fusion\n# ===========================================\ndef final_q1_fusion(X_proc):\n    model_sp.eval()\n    with torch.no_grad():\n        logits, _ = model_sp(torch.tensor(X_proc, dtype=torch.float32).to(device))\n        probs_sp = torch.softmax(logits * 1.2, dim=1).cpu().numpy()\n    \n    probs_ex = expert.predict_proba(X_proc)\n    \n    final_preds = []\n    for i in range(len(X_proc)):\n        # Calculate confidence gap\n        conf_sp = np.max(probs_sp[i])\n        conf_ex = np.max(probs_ex[i])\n        \n        sp_choice = np.argmax(probs_sp[i])\n        \n        # If Specialist is confident in a HARD class, it wins\n        if sp_choice in hard_indices and probs_sp[i][sp_choice] > 0.35:\n            final_preds.append(sp_choice)\n        else:\n            final_preds.append(np.argmax(probs_ex[i]))\n            \n    return np.array(final_preds)\n\nfinal_preds = final_q1_fusion(X_test_proc)\nunique_labels = np.unique(np.concatenate([y_test_enc, final_preds]))\ntarget_names = [le.classes_[i] for i in unique_labels]\n\nprint(classification_report(y_test_enc, final_preds, labels=unique_labels, target_names=target_names, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T13:08:38.839582Z","iopub.execute_input":"2026-01-17T13:08:38.840232Z","iopub.status.idle":"2026-01-17T13:08:59.960960Z","shell.execute_reply.started":"2026-01-17T13:08:38.840200Z","shell.execute_reply":"2026-01-17T13:08:59.960219Z"}},"outputs":[{"name":"stdout","text":"                 precision    recall  f1-score   support\n\n           back       1.00      1.00      1.00       359\nbuffer_overflow       0.34      0.55      0.42        20\n      ftp_write       0.02      0.67      0.04         3\n   guess_passwd       0.99      0.27      0.42      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.99      0.98      0.99       141\n           land       1.00      0.43      0.60         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      1.00      1.00      4657\n           nmap       1.00      1.00      1.00        73\n         normal       0.87      0.97      0.92      9711\n           perl       0.50      0.50      0.50         2\n            phf       1.00      0.50      0.67         2\n            pod       0.69      0.88      0.77        41\n      portsweep       0.79      0.95      0.86       157\n        rootkit       0.01      0.08      0.01        13\n          satan       0.82      1.00      0.90       735\n          smurf       1.00      1.00      1.00       665\n       teardrop       0.24      1.00      0.39        12\n    warezmaster       0.89      0.26      0.41       944\n\n       accuracy                           0.89     18794\n      macro avg       0.63      0.62      0.57     18794\n   weighted avg       0.91      0.89      0.88     18794\n\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report\nimport numpy as np\n\n# ===========================================\n# 1️⃣ Novelty: Hierarchical Residual Anchor Network\n# ===========================================\nclass HRAN_Specialist(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        # Context Stream: Captures global protocol behavior\n        self.context_stream = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.LayerNorm(512),\n            nn.SiLU()\n        )\n        \n        # Detail Stream: Captures subtle R2L/U2R anomalies\n        self.detail_stream = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.SiLU(),\n            nn.Dropout(0.4),\n            nn.Linear(1024, 512),\n            nn.SiLU()\n        )\n        \n        # Fusion and Anchoring\n        self.fusion = nn.Linear(1024, 512)\n        self.classifier = nn.Linear(512, num_classes)\n        # Learnable Class Anchors for Contrastive separation\n        self.anchors = nn.Parameter(torch.randn(num_classes, 512))\n\n    def forward(self, x):\n        c = self.context_stream(x)\n        d = self.detail_stream(x)\n        combined = torch.cat([c, d], dim=1)\n        latent = self.fusion(combined)\n        logits = self.classifier(latent)\n        return logits, latent\n\n# ===========================================\n# 2️⃣ Novelty: Orthogonal Anchor Loss\n# ===========================================\ndef orthogonal_anchor_loss(logits, latent, targets, anchors, weight, alpha=0.1):\n    # Standard Cross-Entropy with CB Weights\n    ce_loss = F.cross_entropy(logits, targets, weight=weight)\n    \n    # Anchor Distance: Minimizing distance to the correct attack anchor\n    correct_anchors = anchors[targets]\n    dist_loss = F.mse_loss(latent, correct_anchors)\n    \n    # Orthogonality: Forcing attack anchors to stay away from the 'Normal' anchor\n    # Assuming le.transform(['normal'])[0] is the index for normal\n    normal_idx = le.transform(['normal'])[0]\n    normal_anchor = anchors[normal_idx].unsqueeze(0)\n    cos_sim = F.cosine_similarity(anchors, normal_anchor)\n    ortho_loss = torch.mean(cos_sim**2) # Minimize squared similarity\n    \n    return ce_loss + alpha * (dist_loss + ortho_loss)\n\n# ===========================================\n# 3️⃣ Training & Fine-Tuning\n# ===========================================\nmodel_hran = HRAN_Specialist(X_train_proc.shape[1], num_classes).to(device)\noptimizer = torch.optim.AdamW(model_hran.parameters(), lr=4e-4, weight_decay=2e-2)\n\n# Higher priority weights for content attacks that are still struggling\nhran_weights = torch.ones(num_classes).to(device)\nfor idx in hard_indices: hran_weights[idx] = 50.0 \n\nfor epoch in range(25):\n    model_hran.train()\n    for xb, yb in sp_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        logits, latent = model_hran(xb)\n        loss = orthogonal_anchor_loss(logits, latent, yb, model_hran.anchors, hran_weights)\n        loss.backward()\n        optimizer.step()\n\n# ===========================================\n# 4️⃣ Expert: Heavyweight XGBoost\n# ===========================================\nexpert = XGBClassifier(tree_method='hist', device='cuda', \n                       n_estimators=500, max_depth=12, \n                       learning_rate=0.03, subsample=0.8)\nexpert.fit(X_train_proc, y_train_enc)\n\n# ===========================================\n# 5️⃣ Final Inference: Semantic Gated Fusion\n# ===========================================\ndef semantic_fusion(X_proc):\n    model_hran.eval()\n    with torch.no_grad():\n        logits, _ = model_hran(torch.tensor(X_proc, dtype=torch.float32).to(device))\n        probs_hran = torch.softmax(logits, dim=1).cpu().numpy()\n    \n    probs_ex = expert.predict_proba(X_proc)\n    \n    final_preds = []\n    for i in range(len(X_proc)):\n        # Calculate max confidences\n        idx_hran = np.argmax(probs_hran[i])\n        \n        # Q1 HEURISTIC: Focus on pushing 'warezmaster' and 'guess_passwd'\n        # If HRAN identifies a content attack with >20% confidence, we listen\n        if idx_hran in hard_indices and probs_hran[i][idx_hran] > 0.20:\n            final_preds.append(idx_hran)\n        else:\n            final_preds.append(np.argmax(probs_ex[i]))\n            \n    return np.array(final_preds)\n\nfinal_preds = semantic_fusion(X_test_proc)\nunique_labels = np.unique(np.concatenate([y_test_enc, final_preds]))\ntarget_names = [le.classes_[i] for i in unique_labels]\n\nprint(\"\\n--- HRAN-Net Q1 RESULTS ---\")\nprint(classification_report(y_test_enc, final_preds, labels=unique_labels, target_names=target_names, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T13:10:49.764265Z","iopub.execute_input":"2026-01-17T13:10:49.764963Z","iopub.status.idle":"2026-01-17T13:11:23.623641Z","shell.execute_reply.started":"2026-01-17T13:10:49.764934Z","shell.execute_reply":"2026-01-17T13:11:23.622858Z"}},"outputs":[{"name":"stdout","text":"\n--- HRAN-Net Q1 RESULTS ---\n                 precision    recall  f1-score   support\n\n           back       1.00      1.00      1.00       359\nbuffer_overflow       0.43      0.50      0.47        20\n      ftp_write       0.01      0.67      0.01         3\n   guess_passwd       0.95      0.10      0.18      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.98      0.89      0.93       141\n           land       1.00      0.29      0.44         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      1.00      1.00      4657\n           nmap       1.00      1.00      1.00        73\n         normal       0.87      0.97      0.92      9711\n           perl       0.50      0.50      0.50         2\n            phf       1.00      0.50      0.67         2\n            pod       0.76      0.46      0.58        41\n      portsweep       0.78      0.94      0.85       157\n        rootkit       0.00      0.08      0.01        13\n          satan       0.81      1.00      0.90       735\n          smurf       0.99      1.00      1.00       665\n       teardrop       0.24      1.00      0.39        12\n    warezmaster       0.87      0.08      0.15       944\n\n       accuracy                           0.87     18794\n      macro avg       0.63      0.57      0.52     18794\n   weighted avg       0.91      0.87      0.85     18794\n\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report\nimport numpy as np\n\n# ===========================================\n# 1️⃣ Novelty: Residual Gated Attention Stem\n# ===========================================\nclass ARGF_Specialist(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        # Gating mechanism to sieve out minority signals\n        self.gate = nn.Sequential(\n            nn.Linear(input_dim, input_dim),\n            nn.Sigmoid()\n        )\n        \n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.LayerNorm(1024),\n            nn.SiLU(),\n            nn.Linear(1024, 512),\n            nn.SiLU()\n        )\n        \n        self.classifier = nn.Linear(512, num_classes)\n        # Learnable Class Sharpness (Temperature)\n        self.temp = nn.Parameter(torch.ones(num_classes))\n\n    def forward(self, x):\n        # Gating: Selective feature amplification\n        g = self.gate(x)\n        x_gated = x * g + x # Residual connection\n        \n        feat = self.encoder(x_gated)\n        logits = self.classifier(feat)\n        # Apply learnable temperature to sharpen minority class predictions\n        return logits / torch.clamp(self.temp, min=0.1)\n\n# ===========================================\n# 2️⃣ Execution: Focused Training on Content Attacks\n# ===========================================\nmodel_argf = ARGF_Specialist(X_train_proc.shape[1], num_classes).to(device)\noptimizer = torch.optim.AdamW(model_argf.parameters(), lr=8e-4, weight_decay=1e-2)\n\n# Specific weights to fix 'guess_passwd' and 'warezmaster' without hurting accuracy\nargf_weights = torch.ones(num_classes).to(device)\nargf_weights[le.transform(['guess_passwd'])[0]] = 60.0\nargf_weights[le.transform(['warezmaster'])[0]] = 40.0\nargf_weights[le.transform(['rootkit'])[0]] = 20.0\n\ncriterion = nn.CrossEntropyLoss(weight=argf_weights)\n\nprint(\"Training ARGF Specialist...\")\nfor epoch in range(25):\n    model_argf.train()\n    for xb, yb in sp_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        loss = criterion(model_argf(xb), yb)\n        loss.backward()\n        optimizer.step()\n\n# ===========================================\n# 3️⃣ Expert: XGBoost (Base Stability)\n# ===========================================\nexpert = XGBClassifier(tree_method='hist', device='cuda', \n                       n_estimators=300, max_depth=10, \n                       learning_rate=0.05)\nexpert.fit(X_train_proc, y_train_enc)\n\n# ===========================================\n# 4️⃣ NOVELTY: Adaptive Meta-Selection\n# ===========================================\ndef meta_selection_fusion(X_proc, df_original):\n    model_argf.eval()\n    with torch.no_grad():\n        logits = model_argf(torch.tensor(X_proc, dtype=torch.float32).to(device))\n        probs_argf = torch.softmax(logits, dim=1).cpu().numpy()\n    \n    probs_ex = expert.predict_proba(X_proc)\n    \n    # Handpicked Gating Rule: If these flags are active, trust the Specialist\n    content_flags = (df_original['num_failed_logins'] > 0) | \\\n                    (df_original['is_guest_login'] > 0) | \\\n                    (df_original['hot'] > 0)\n    \n    final_preds = []\n    for i in range(len(X_proc)):\n        idx_argf = np.argmax(probs_argf[i])\n        idx_ex = np.argmax(probs_ex[i])\n        \n        # Q1 Logic: If 'Normal' is guessed by XGBoost but 'Attack' is guessed by ARGF\n        # and the Content Sieve is triggered, we follow the ARGF\n        if content_flags.iloc[i]:\n            if idx_argf in hard_indices:\n                final_preds.append(idx_argf)\n            else:\n                final_preds.append(idx_ex)\n        else:\n            # Otherwise, use a soft confidence fusion\n            if np.max(probs_argf[i]) > 0.9 and idx_argf in hard_indices:\n                final_preds.append(idx_argf)\n            else:\n                final_preds.append(idx_ex)\n                \n    return np.array(final_preds)\n\nprint(\"Inference with Meta-Selection...\")\nfinal_preds = meta_selection_fusion(X_test_proc, df_test)\n\nunique_labels = np.unique(np.concatenate([y_test_enc, final_preds]))\ntarget_names = [le.classes_[i] for i in unique_labels]\n\nprint(\"\\n--- ARGF-Net Q1 RESULTS ---\")\nprint(classification_report(y_test_enc, final_preds, labels=unique_labels, target_names=target_names, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T13:12:49.673808Z","iopub.execute_input":"2026-01-17T13:12:49.674783Z","iopub.status.idle":"2026-01-17T13:13:10.324776Z","shell.execute_reply.started":"2026-01-17T13:12:49.674747Z","shell.execute_reply":"2026-01-17T13:13:10.323864Z"}},"outputs":[{"name":"stdout","text":"Training ARGF Specialist...\nInference with Meta-Selection...\n\n--- ARGF-Net Q1 RESULTS ---\n                 precision    recall  f1-score   support\n\n           back       1.00      1.00      1.00       359\nbuffer_overflow       0.78      0.35      0.48        20\n      ftp_write       0.05      0.33      0.08         3\n   guess_passwd       0.98      0.24      0.39      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.98      0.99      0.98       141\n           land       1.00      0.57      0.73         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      1.00      1.00      4657\n           nmap       1.00      1.00      1.00        73\n         normal       0.86      0.97      0.91      9711\n           perl       0.00      0.00      0.00         2\n            phf       1.00      0.50      0.67         2\n            pod       0.70      0.93      0.80        41\n      portsweep       0.78      0.94      0.85       157\n        rootkit       0.01      0.08      0.01        13\n          satan       0.82      1.00      0.90       735\n          smurf       1.00      1.00      1.00       665\n       teardrop       0.24      1.00      0.39        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       0.90      0.06      0.11       944\n\n       accuracy                           0.88     18794\n      macro avg       0.60      0.54      0.51     18794\n   weighted avg       0.91      0.88      0.86     18794\n\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report\nfrom torch.utils.data import DataLoader, TensorDataset\nimport numpy as np\nfrom tqdm import tqdm\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# ===========================================\n# 1️⃣ Novelty: Manifold Amplification Specialist\n# ===========================================\nclass SLMA_Specialist(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        self.stem = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.SiLU(),\n            nn.LayerNorm(1024)\n        )\n        self.bottleneck = nn.Linear(1024, 512)\n        self.classifier = nn.Linear(512, num_classes)\n\n    def forward(self, x, target_labels=None, is_training=False):\n        x = self.stem(x)\n        latent = self.bottleneck(x)\n        \n        # SLMA NOVELTY: Perturb latent space for rare classes during training\n        if is_training and target_labels is not None:\n            # hard_indices must be defined globally from previous steps\n            rare_mask = torch.isin(target_labels, torch.tensor(hard_indices).to(device))\n            if rare_mask.any():\n                noise = torch.randn_like(latent[rare_mask]) * 0.05\n                latent[rare_mask] += noise\n        \n        logits = self.classifier(latent)\n        return logits, latent\n\n# ===========================================\n# 2️⃣ Novelty: Orthogonal Repulsion Loss\n# ===========================================\ndef slma_loss(logits, latent, targets, weight):\n    ce_loss = F.cross_entropy(logits, targets, weight=weight)\n    \n    # Orthogonal Penalty: Ensure Attack manifolds don't collapse into Normal manifolds\n    normal_idx = le.transform(['normal'])[0]\n    normal_latent = latent[targets == normal_idx]\n    attack_latent = latent[targets != normal_idx]\n    \n    if len(normal_latent) > 0 and len(attack_latent) > 0:\n        sim = F.cosine_similarity(normal_latent.mean(0, keepdim=True), \n                                 attack_latent.mean(0, keepdim=True))\n        return ce_loss + 0.1 * sim.pow(2)\n    return ce_loss\n\n# ===========================================\n# 3️⃣ Strategic Training \n# ===========================================\nmodel_slma = SLMA_Specialist(X_train_proc.shape[1], num_classes).to(device)\noptimizer = torch.optim.AdamW(model_slma.parameters(), lr=5e-4, weight_decay=2e-2)\n\n# High-Intensity weights for the 'Silent Killers'\nslma_weights = torch.ones(num_classes).to(device)\nslma_weights[le.transform(['warezmaster'])[0]] = 80.0\nslma_weights[le.transform(['rootkit'])[0]] = 50.0\nslma_weights[le.transform(['guess_passwd'])[0]] = 40.0\n\n# Using your existing sp_loader or similar filtered dataset\nfor epoch in range(25):\n    model_slma.train()\n    for xb, yb in tqdm(sp_loader, desc=f\"Epoch {epoch+1}\"):\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        logits, latent = model_slma(xb, target_labels=yb, is_training=True)\n        loss = slma_loss(logits, latent, yb, slma_weights)\n        loss.backward()\n        optimizer.step()\n\n# ===========================================\n# 4️⃣ Expert & Switchboard Fusion\n# ===========================================\nexpert = XGBClassifier(tree_method='hist', device='cuda', n_estimators=400, max_depth=10)\nexpert.fit(X_train_proc, y_train_enc)\n\ndef switchboard_fusion(X_proc, df_original):\n    model_slma.eval()\n    with torch.no_grad():\n        logits, _ = model_slma(torch.tensor(X_proc, dtype=torch.float32).to(device))\n        probs_slma = torch.softmax(logits, dim=1).cpu().numpy()\n    \n    probs_ex = expert.predict_proba(X_proc)\n    \n    # Domain Heuristic: Content-based switchboard\n    extreme_anomaly = (df_original['hot'] > 1) | (df_original['num_failed_logins'] > 0)\n    \n    final_preds = []\n    for i in range(len(X_proc)):\n        idx_slma = np.argmax(probs_slma[i])\n        idx_ex = np.argmax(probs_ex[i])\n        \n        if extreme_anomaly.iloc[i] or (idx_slma in hard_indices and probs_slma[i][idx_slma] > 0.15):\n            final_preds.append(idx_slma)\n        else:\n            final_preds.append(idx_ex)\n    return np.array(final_preds)\n\n# ===========================================\n# 5️⃣ Error-Proof Evaluation\n# ===========================================\nprint(\"Final SLMA Inference...\")\nfinal_preds = switchboard_fusion(X_test_proc, df_test)\n\n# FIX: Filter target_names to only include classes present in the test predictions/truth\nunique_test_classes = np.unique(np.concatenate([y_test_enc, final_preds]))\ntarget_names_filtered = [le.classes_[i] for i in unique_test_classes]\n\nprint(\"\\n--- SLMA-Net Q1 RESULTS ---\")\nprint(classification_report(y_test_enc, final_preds, \n                            labels=unique_test_classes, \n                            target_names=target_names_filtered, \n                            zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T13:15:37.078015Z","iopub.execute_input":"2026-01-17T13:15:37.078836Z","iopub.status.idle":"2026-01-17T13:15:59.120223Z","shell.execute_reply.started":"2026-01-17T13:15:37.078803Z","shell.execute_reply":"2026-01-17T13:15:59.119317Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 13/13 [00:00<00:00, 167.99it/s]\nEpoch 2: 100%|██████████| 13/13 [00:00<00:00, 184.65it/s]\nEpoch 3: 100%|██████████| 13/13 [00:00<00:00, 188.31it/s]\nEpoch 4: 100%|██████████| 13/13 [00:00<00:00, 189.24it/s]\nEpoch 5: 100%|██████████| 13/13 [00:00<00:00, 185.03it/s]\nEpoch 6: 100%|██████████| 13/13 [00:00<00:00, 184.61it/s]\nEpoch 7: 100%|██████████| 13/13 [00:00<00:00, 184.45it/s]\nEpoch 8: 100%|██████████| 13/13 [00:00<00:00, 181.06it/s]\nEpoch 9: 100%|██████████| 13/13 [00:00<00:00, 181.69it/s]\nEpoch 10: 100%|██████████| 13/13 [00:00<00:00, 182.33it/s]\nEpoch 11: 100%|██████████| 13/13 [00:00<00:00, 187.98it/s]\nEpoch 12: 100%|██████████| 13/13 [00:00<00:00, 192.78it/s]\nEpoch 13: 100%|██████████| 13/13 [00:00<00:00, 185.55it/s]\nEpoch 14: 100%|██████████| 13/13 [00:00<00:00, 186.88it/s]\nEpoch 15: 100%|██████████| 13/13 [00:00<00:00, 185.12it/s]\nEpoch 16: 100%|██████████| 13/13 [00:00<00:00, 186.52it/s]\nEpoch 17: 100%|██████████| 13/13 [00:00<00:00, 187.91it/s]\nEpoch 18: 100%|██████████| 13/13 [00:00<00:00, 185.21it/s]\nEpoch 19: 100%|██████████| 13/13 [00:00<00:00, 184.78it/s]\nEpoch 20: 100%|██████████| 13/13 [00:00<00:00, 189.13it/s]\nEpoch 21: 100%|██████████| 13/13 [00:00<00:00, 185.88it/s]\nEpoch 22: 100%|██████████| 13/13 [00:00<00:00, 184.52it/s]\nEpoch 23: 100%|██████████| 13/13 [00:00<00:00, 183.66it/s]\nEpoch 24: 100%|██████████| 13/13 [00:00<00:00, 175.97it/s]\nEpoch 25: 100%|██████████| 13/13 [00:00<00:00, 183.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Final SLMA Inference...\n\n--- SLMA-Net Q1 RESULTS ---\n                 precision    recall  f1-score   support\n\n           back       1.00      0.12      0.22       359\nbuffer_overflow       0.70      0.35      0.47        20\n      ftp_write       0.04      0.67      0.07         3\n   guess_passwd       0.65      0.10      0.18      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.99      0.99      0.99       141\n           land       1.00      0.43      0.60         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      0.70      0.82      4657\n           nmap       0.96      1.00      0.98        73\n         normal       0.85      0.97      0.90      9711\n           perl       1.00      0.50      0.67         2\n            phf       0.00      0.00      0.00         2\n            pod       0.70      0.93      0.80        41\n      portsweep       0.79      0.96      0.87       157\n        rootkit       0.00      0.15      0.01        13\n          satan       0.82      1.00      0.90       735\n          smurf       0.98      1.00      0.99       665\n       teardrop       0.60      0.25      0.35        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       0.17      0.29      0.22       944\n\n       accuracy                           0.80     18794\n      macro avg       0.56      0.47      0.46     18794\n   weighted avg       0.84      0.80      0.79     18794\n\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report\nimport numpy as np\nfrom torch.utils.data import DataLoader, TensorDataset\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# ===========================================\n# 1️⃣ Path A: The High-Stability Anchor (XGBoost)\n# ===========================================\n# Optimized to protect the majority classes (Normal, Neptune, Smurf)\nprint(\"Phase 1: Training Stability Anchor...\")\nanchor_model = XGBClassifier(\n    n_estimators=300, \n    max_depth=10, \n    learning_rate=0.05,\n    tree_method='hist',\n    device='cuda' if torch.cuda.is_available() else 'cpu'\n)\nanchor_model.fit(X_train_proc, y_train_enc)\n\n# ===========================================\n# 2️⃣ Path B: The Residual Hunter (Deep Gated MLP)\n# ===========================================\n# This model ONLY cares about the classes the Anchor misses\nclass ResidualHunter(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.SiLU(),\n            nn.LayerNorm(1024),\n            nn.Dropout(0.4),\n            nn.Linear(1024, 512),\n            nn.SiLU(),\n            nn.Linear(512, num_classes)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nhunter_model = ResidualHunter(X_train_proc.shape[1], num_classes).to(device)\noptimizer = torch.optim.AdamW(hunter_model.parameters(), lr=5e-4)\n\n# Weighting: Only focus on the 'Hard' classes\nhunter_weights = torch.ones(num_classes).to(device)\nhard_list = ['guess_passwd', 'warezmaster', 'rootkit', 'buffer_overflow', 'ftp_write', 'teardrop']\nfor cls in hard_list:\n    idx = le.transform([cls])[0]\n    hunter_weights[idx] = 100.0 # Aggressive focus\n\ncriterion = nn.CrossEntropyLoss(weight=hunter_weights)\n\nprint(\"Phase 2: Training Residual Hunter...\")\n# Train on the full dataset but with hunter_weights\nfor epoch in range(20):\n    hunter_model.train()\n    for xb, yb in sp_loader: # Using your specialist loader\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        loss = criterion(hunter_model(xb), yb)\n        loss.backward()\n        optimizer.step()\n\n# ===========================================\n# 3️⃣ NOVELTY: Cross-Distillation Fusion\n# ===========================================\ndef cross_distillation_inference(X_proc, df_orig):\n    hunter_model.eval()\n    with torch.no_grad():\n        logits = hunter_model(torch.tensor(X_proc, dtype=torch.float32).to(device))\n        probs_h = torch.softmax(logits, dim=1).cpu().numpy()\n    \n    probs_a = anchor_model.predict_proba(X_proc)\n    \n    final_preds = []\n    for i in range(len(X_proc)):\n        # Calculate Confidence Gap\n        conf_a = np.max(probs_a[i])\n        pred_a = np.argmax(probs_a[i])\n        \n        conf_h = np.max(probs_h[i])\n        pred_h = np.argmax(probs_h[i])\n        \n        # LOGIC: If Anchor is highly confident (>0.9) in a major class, DO NOT OVERRULE\n        if conf_a > 0.92:\n            final_preds.append(pred_a)\n        # If Hunter detects a Hard Class with decent confidence, LISTEN\n        elif pred_h in [le.transform([c])[0] for c in hard_list] and conf_h > 0.15:\n            final_preds.append(pred_h)\n        else:\n            final_preds.append(pred_a)\n            \n    return np.array(final_preds)\n\n# ===========================================\n# 4️⃣ Evaluation\n# ===========================================\nprint(\"Phase 3: Final Fusion Inference...\")\nfinal_preds = cross_distillation_inference(X_test_proc, df_test)\n\nunique_test_classes = np.unique(np.concatenate([y_test_enc, final_preds]))\ntarget_names = [le.classes_[i] for i in unique_test_classes]\n\nprint(\"\\n--- DPCD-Net Q1 RESULTS ---\")\nprint(classification_report(y_test_enc, final_preds, \n                            labels=unique_test_classes, \n                            target_names=target_names, \n                            zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T13:16:41.761728Z","iopub.execute_input":"2026-01-17T13:16:41.762061Z","iopub.status.idle":"2026-01-17T13:17:02.376507Z","shell.execute_reply.started":"2026-01-17T13:16:41.762034Z","shell.execute_reply":"2026-01-17T13:17:02.375829Z"}},"outputs":[{"name":"stdout","text":"Phase 1: Training Stability Anchor...\nPhase 2: Training Residual Hunter...\nPhase 3: Final Fusion Inference...\n\n--- DPCD-Net Q1 RESULTS ---\n                 precision    recall  f1-score   support\n\n           back       1.00      1.00      1.00       359\nbuffer_overflow       0.50      0.10      0.17        20\n      ftp_write       0.00      0.00      0.00         3\n   guess_passwd       1.00      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.99      0.99      0.99       141\n           land       1.00      0.14      0.25         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      1.00      1.00      4657\n           nmap       1.00      1.00      1.00        73\n         normal       0.82      0.98      0.89      9711\n           perl       0.00      0.00      0.00         2\n            phf       0.00      0.00      0.00         2\n            pod       0.69      0.88      0.77        41\n      portsweep       0.78      0.94      0.85       157\n        rootkit       0.00      0.00      0.00        13\n          satan       0.82      1.00      0.90       735\n          smurf       1.00      1.00      1.00       665\n       teardrop       0.24      1.00      0.39        12\n    warezmaster       0.93      0.10      0.17       944\n\n       accuracy                           0.87     18794\n      macro avg       0.56      0.48      0.45     18794\n   weighted avg       0.89      0.87      0.83     18794\n\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report\nimport numpy as np\nfrom tqdm import tqdm\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# ===========================================\n# 1️⃣ Novelty: Neural Feature Expansion Network\n# ===========================================\nclass NFENet(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        # Expansion to decouple overlapping features\n        self.expand = nn.Sequential(\n            nn.Linear(input_dim, 4096),\n            nn.SiLU(),\n            nn.BatchNorm1d(4096),\n            nn.Dropout(0.5)\n        )\n        self.compress = nn.Sequential(\n            nn.Linear(4096, 512),\n            nn.SiLU()\n        )\n        self.classifier = nn.Linear(512, num_classes)\n        # Learnable Class Centroids\n        self.centroids = nn.Parameter(torch.randn(num_classes, 512))\n\n    def forward(self, x):\n        features = self.expand(x)\n        latent = self.compress(features)\n        logits = self.classifier(latent)\n        return logits, latent\n\n# ===========================================\n# 2️⃣ Novelty: Contrastive Anchoring Loss\n# ===========================================\ndef contrastive_anchor_loss(logits, latent, targets, centroids, weights):\n    ce_loss = F.cross_entropy(logits, targets, weight=weights)\n    # Centroid Distance (Minimize distance to own class)\n    target_centroids = centroids[targets]\n    dist_loss = F.mse_loss(latent, target_centroids)\n    # Push-Away Loss (Maximize distance to 'Normal' centroid)\n    normal_idx = le.transform(['normal'])[0]\n    normal_centroid = centroids[normal_idx].detach()\n    push_loss = -torch.mean(F.pairwise_distance(latent, normal_centroid))\n    return ce_loss + 0.1 * dist_loss + 0.05 * push_loss\n\n# ===========================================\n# 3️⃣ Training Loop\n# ===========================================\nnfe_model = NFENet(X_train_proc.shape[1], num_classes).to(device)\noptimizer = torch.optim.AdamW(nfe_model.parameters(), lr=3e-4, weight_decay=1e-2)\n\nweights_nfe = torch.ones(num_classes).to(device)\nhard_attacks = ['guess_passwd', 'warezmaster', 'rootkit', 'buffer_overflow', 'ftp_write']\nfor attack in hard_attacks:\n    weights_nfe[le.transform([attack])[0]] = 150.0 \n\nprint(\"Training Expansion Specialist...\")\n# Use your previously defined specialist loader (sp_loader)\nfor epoch in range(25):\n    nfe_model.train()\n    for xb, yb in tqdm(sp_loader, desc=f\"Epoch {epoch+1}\"):\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        logits, latent = nfe_model(xb)\n        loss = contrastive_anchor_loss(logits, latent, yb, nfe_model.centroids, weights_nfe)\n        loss.backward()\n        optimizer.step()\n\n# ===========================================\n# 4️⃣ Inference & Error-Proof Report\n# ===========================================\ndef pgf_inference(X_proc):\n    nfe_model.eval()\n    with torch.no_grad():\n        logits_n, _ = nfe_model(torch.tensor(X_proc, dtype=torch.float32).to(device))\n        probs_n = torch.softmax(logits_n, dim=1).cpu().numpy()\n    \n    # Use existing XGBoost anchor model from previous steps\n    probs_x = anchor_model.predict_proba(X_proc) \n    \n    final_preds = []\n    hard_indices = [le.transform([a])[0] for a in hard_attacks]\n    \n    for i in range(len(X_proc)):\n        p_x = probs_x[i]\n        p_n = probs_n[i]\n        \n        # Calculate Probability Gap in XGBoost\n        top2 = np.sort(p_x)[-2:]\n        gap = top2[1] - top2[0]\n        \n        # Trigger Specialist if XGBoost is unsure or Specialist is confident in hard class\n        if gap < 0.6 or (np.argmax(p_n) in hard_indices and np.max(p_n) > 0.3):\n            final_preds.append(np.argmax(p_n))\n        else:\n            final_preds.append(np.argmax(p_x))\n    return np.array(final_preds)\n\nprint(\"Running Final Inference...\")\nfinal_preds = pgf_inference(X_test_proc)\n\n# ERROR FIX: Dynamically align target names with actual predicted classes\nunique_labels = np.unique(np.concatenate([y_test_enc, final_preds]))\ntarget_names = [le.classes_[i] for i in unique_labels]\n\nprint(\"\\n--- CME-PGG Q1 Final Results ---\")\nprint(classification_report(y_test_enc, final_preds, \n                            labels=unique_labels, \n                            target_names=target_names, \n                            zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T13:21:43.372146Z","iopub.execute_input":"2026-01-17T13:21:43.373057Z","iopub.status.idle":"2026-01-17T13:21:45.165712Z","shell.execute_reply.started":"2026-01-17T13:21:43.373022Z","shell.execute_reply":"2026-01-17T13:21:45.164979Z"}},"outputs":[{"name":"stdout","text":"Training Expansion Specialist...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 13/13 [00:00<00:00, 228.44it/s]\nEpoch 2: 100%|██████████| 13/13 [00:00<00:00, 241.94it/s]\nEpoch 3: 100%|██████████| 13/13 [00:00<00:00, 241.36it/s]\nEpoch 4: 100%|██████████| 13/13 [00:00<00:00, 238.20it/s]\nEpoch 5: 100%|██████████| 13/13 [00:00<00:00, 240.70it/s]\nEpoch 6: 100%|██████████| 13/13 [00:00<00:00, 247.16it/s]\nEpoch 7: 100%|██████████| 13/13 [00:00<00:00, 235.87it/s]\nEpoch 8: 100%|██████████| 13/13 [00:00<00:00, 246.60it/s]\nEpoch 9: 100%|██████████| 13/13 [00:00<00:00, 243.93it/s]\nEpoch 10: 100%|██████████| 13/13 [00:00<00:00, 239.87it/s]\nEpoch 11: 100%|██████████| 13/13 [00:00<00:00, 243.29it/s]\nEpoch 12: 100%|██████████| 13/13 [00:00<00:00, 245.51it/s]\nEpoch 13: 100%|██████████| 13/13 [00:00<00:00, 242.53it/s]\nEpoch 14: 100%|██████████| 13/13 [00:00<00:00, 242.93it/s]\nEpoch 15: 100%|██████████| 13/13 [00:00<00:00, 244.26it/s]\nEpoch 16: 100%|██████████| 13/13 [00:00<00:00, 252.25it/s]\nEpoch 17: 100%|██████████| 13/13 [00:00<00:00, 248.02it/s]\nEpoch 18: 100%|██████████| 13/13 [00:00<00:00, 248.18it/s]\nEpoch 19: 100%|██████████| 13/13 [00:00<00:00, 246.48it/s]\nEpoch 20: 100%|██████████| 13/13 [00:00<00:00, 253.10it/s]\nEpoch 21: 100%|██████████| 13/13 [00:00<00:00, 246.56it/s]\nEpoch 22: 100%|██████████| 13/13 [00:00<00:00, 241.49it/s]\nEpoch 23: 100%|██████████| 13/13 [00:00<00:00, 242.60it/s]\nEpoch 24: 100%|██████████| 13/13 [00:00<00:00, 245.10it/s]\nEpoch 25: 100%|██████████| 13/13 [00:00<00:00, 241.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Running Final Inference...\n\n--- CME-PGG Q1 Final Results ---\n                 precision    recall  f1-score   support\n\n           back       1.00      0.95      0.97       359\nbuffer_overflow       0.14      0.10      0.12        20\n      ftp_write       0.01      0.67      0.02         3\n   guess_passwd       0.97      0.09      0.17      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.50      0.01      0.01       141\n           land       1.00      0.14      0.25         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      0.99      0.99      4657\n           nmap       1.00      1.00      1.00        73\n         normal       0.90      0.97      0.93      9711\n           perl       0.00      0.00      0.00         2\n            phf       0.00      0.00      0.00         2\n            pod       0.77      0.41      0.54        41\n      portsweep       0.79      0.91      0.85       157\n        rootkit       0.01      0.38      0.02        13\n          satan       0.83      1.00      0.90       735\n          smurf       1.00      1.00      1.00       665\n       teardrop       0.24      1.00      0.39        12\n    warezmaster       0.93      0.68      0.79       944\n\n       accuracy                           0.89     18794\n      macro avg       0.53      0.49      0.43     18794\n   weighted avg       0.92      0.89      0.88     18794\n\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report\nimport numpy as np\n\n# ===========================================\n# 1️⃣ Path A: Statistical Expert (XGBoost)\n# ===========================================\n# Re-running to ensure the global boundaries are clean\nexpert_xgb = XGBClassifier(tree_method='hist', device='cuda', n_estimators=300, max_depth=10)\nexpert_xgb.fit(X_train_proc, y_train_enc)\n\n# ===========================================\n# 2️⃣ Path B: Manifold Specialist (CME-Net)\n# ===========================================\n# Use the nfe_model trained in the previous step\n# (Assuming nfe_model is already trained and in memory)\n\n# ===========================================\n# 3️⃣ NOVELTY: Protocol-Aware Decision Logic\n# ===========================================\ndef tri_stream_fusion(X_proc, df_orig):\n    nfe_model.eval()\n    with torch.no_grad():\n        logits_n, _ = nfe_model(torch.tensor(X_proc, dtype=torch.float32).to(device))\n        probs_n = torch.softmax(logits_n, dim=1).cpu().numpy()\n    \n    probs_x = expert_xgb.predict_proba(X_proc)\n    \n    final_preds = []\n    hard_indices = [le.transform([a])[0] for a in ['guess_passwd', 'warezmaster', 'rootkit', 'buffer_overflow']]\n    \n    # Get protocol indices from df_orig\n    # Logic: R2L attacks are almost exclusively TCP/UDP\n    for i in range(len(X_proc)):\n        p_x = probs_x[i]\n        p_n = probs_n[i]\n        \n        # Determine protocol context\n        protocol = df_orig['protocol_type'].iloc[i]\n        \n        # TRISTREAM LOGIC:\n        # 1. If it's a Probing/DoS protocol (ICMP), trust XGBoost 100%\n        if protocol == 'icmp':\n            final_preds.append(np.argmax(p_x))\n            \n        # 2. If it's TCP/UDP and the Specialist is confident in an R2L attack, trust Specialist\n        elif np.argmax(p_n) in hard_indices and np.max(p_n) > 0.4:\n            final_preds.append(np.argmax(p_n))\n            \n        # 3. Otherwise, use a Confidence-Weighted Average\n        else:\n            # Weighted blend: give more weight to XGBoost for 'normal' and DoS\n            blended = (0.7 * p_x) + (0.3 * p_n)\n            final_preds.append(np.argmax(blended))\n            \n    return np.array(final_preds)\n\n# ===========================================\n# 4️⃣ Execution & Final Report\n# ===========================================\nprint(\"Running Tri-Stream Fusion...\")\nfinal_preds = tri_stream_fusion(X_test_proc, df_test)\n\nunique_labels = np.unique(np.concatenate([y_test_enc, final_preds]))\ntarget_names = [le.classes_[i] for i in unique_labels]\n\nprint(\"\\n--- TSOF Q1 FINAL RESULTS ---\")\nprint(classification_report(y_test_enc, final_preds, \n                            labels=unique_labels, \n                            target_names=target_names, \n                            zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T13:22:21.928950Z","iopub.execute_input":"2026-01-17T13:22:21.929385Z","iopub.status.idle":"2026-01-17T13:22:37.696463Z","shell.execute_reply.started":"2026-01-17T13:22:21.929352Z","shell.execute_reply":"2026-01-17T13:22:37.695623Z"}},"outputs":[{"name":"stdout","text":"Running Tri-Stream Fusion...\n\n--- TSOF Q1 FINAL RESULTS ---\n                 precision    recall  f1-score   support\n\n           back       1.00      0.94      0.97       359\nbuffer_overflow       0.14      0.10      0.12        20\n      ftp_write       0.00      0.00      0.00         3\n   guess_passwd       0.97      0.09      0.17      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.98      0.99      0.98       141\n           land       0.00      0.00      0.00         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      1.00      1.00      4657\n           nmap       0.96      1.00      0.98        73\n         normal       0.90      0.96      0.93      9711\n           perl       0.00      0.00      0.00         2\n            phf       0.00      0.00      0.00         2\n            pod       0.71      0.95      0.81        41\n      portsweep       0.79      0.94      0.86       157\n        rootkit       0.01      0.38      0.02        13\n          satan       0.82      1.00      0.90       735\n          smurf       0.98      1.00      0.99       665\n       teardrop       0.24      1.00      0.39        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       0.94      0.68      0.79       944\n\n       accuracy                           0.90     18794\n      macro avg       0.47      0.50      0.45     18794\n   weighted avg       0.93      0.90      0.89     18794\n\n","output_type":"stream"}],"execution_count":38}]}