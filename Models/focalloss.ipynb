{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14523807,"sourceType":"datasetVersion","datasetId":9276062},{"sourceId":394223,"sourceType":"datasetVersion","datasetId":174616}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-17T06:04:16.149987Z","iopub.execute_input":"2026-01-17T06:04:16.150251Z","iopub.status.idle":"2026-01-17T06:04:17.557055Z","shell.execute_reply.started":"2026-01-17T06:04:16.150226Z","shell.execute_reply":"2026-01-17T06:04:17.556132Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nslkdd/KDDTest+.arff\n/kaggle/input/nslkdd/KDDTest-21.arff\n/kaggle/input/nslkdd/KDDTest1.jpg\n/kaggle/input/nslkdd/KDDTrain+.txt\n/kaggle/input/nslkdd/KDDTrain+_20Percent.txt\n/kaggle/input/nslkdd/KDDTest-21.txt\n/kaggle/input/nslkdd/KDDTest+.txt\n/kaggle/input/nslkdd/KDDTrain+.arff\n/kaggle/input/nslkdd/index.html\n/kaggle/input/nslkdd/KDDTrain+_20Percent.arff\n/kaggle/input/nslkdd/KDDTrain1.jpg\n/kaggle/input/nslkdd/nsl-kdd/KDDTest+.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTest-21.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTest1.jpg\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+_20Percent.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTest-21.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTest+.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+.arff\n/kaggle/input/nslkdd/nsl-kdd/index.html\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+_20Percent.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain1.jpg\n/kaggle/input/nsl-kdd-augmented/smote_augmented.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install torch torchvision scikit-learn pandas numpy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T06:04:17.558597Z","iopub.execute_input":"2026-01-17T06:04:17.558987Z","iopub.status.idle":"2026-01-17T06:04:22.446046Z","shell.execute_reply.started":"2026-01-17T06:04:17.558960Z","shell.execute_reply":"2026-01-17T06:04:22.444958Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T06:04:22.447464Z","iopub.execute_input":"2026-01-17T06:04:22.447776Z","iopub.status.idle":"2026-01-17T06:04:27.384879Z","shell.execute_reply.started":"2026-01-17T06:04:22.447745Z","shell.execute_reply":"2026-01-17T06:04:27.384265Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"BATCH_SIZE = 512\nEPOCHS = 30\nLR = 1e-3\nLAMBDA = 0.5\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T06:04:27.385749Z","iopub.execute_input":"2026-01-17T06:04:27.386122Z","iopub.status.idle":"2026-01-17T06:04:27.479161Z","shell.execute_reply.started":"2026-01-17T06:04:27.386086Z","shell.execute_reply":"2026-01-17T06:04:27.478271Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\n\n# ---------------------------\n# Paths\n# ---------------------------\nlocal_path = \"/kaggle/input/nslkdd/\"\ntrain_path = \"/kaggle/input/nsl-kdd-augmented/\"\n\n# ---------------------------\n# Load datasets\n# ---------------------------\n# Note: Use header=0 if your augmented CSV has headers, otherwise None\ndf_train = pd.read_csv(train_path + \"smote_augmented.csv\") \ndf_test = pd.read_csv(local_path + \"KDDTest+.txt\", header=None)\n\n# ---------------------------\n# Assign columns to test data\n# ---------------------------\n# Note: The 'outcome' is typically index 41 and 'level' is index 42 in the raw txt\ncolumns = [\n    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land',\n    'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n    'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n    'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count',\n    'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n    'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n    'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n    'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate',\n    'outcome', 'level'\n]\ndf_test.columns = columns\n\n# ---------------------------\n# Separate features and labels\n# ---------------------------\ntarget = \"outcome\"\ncat_cols = ['protocol_type', 'service', 'flag']\n\n# Ensure num_cols uses all available features except target and level\nX_train_aug = df_train.drop(columns=['outcome', 'level'], errors='ignore')\ny_train_aug = df_train['outcome']\n\nX_test = df_test.drop(columns=['outcome', 'level'], errors='ignore')\ny_test = df_test['outcome']\n\nnum_cols = [c for c in X_train_aug.columns if c not in cat_cols]\n\n# ---------------------------\n# Filter test labels that exist in training\n# ---------------------------\n# CRITICAL: We only keep test rows where the outcome exists in the augmented training set\n# If SMOTE reduced classes, we must filter the test set to avoid LabelEncoder errors\ntrain_labels = set(y_train_aug.unique())\nmask = y_test.isin(train_labels)\n\nif mask.sum() == 0:\n    print(\"WARNING: No matching labels between train and test! Checking for string/int mismatch...\")\n    # Standardize to string in case of type mismatch\n    y_train_aug = y_train_aug.astype(str)\n    y_test = y_test.astype(str)\n    train_labels = set(y_train_aug.unique())\n    mask = y_test.isin(train_labels)\n\nX_test = X_test[mask].reset_index(drop=True)\ny_test = y_test[mask].reset_index(drop=True)\n\nprint(f\"Test data retained: {len(X_test)} samples\")\nprint(\"Filtered test label distribution:\\n\", y_test.value_counts())\n\n# ---------------------------\n# Column Transformer: scale numeric + encode categorical\n# ---------------------------\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), num_cols),\n        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n    ]\n)\n\nX_train_processed = preprocessor.fit_transform(X_train_aug)\nX_test_processed  = preprocessor.transform(X_test)\n\n# ---------------------------\n# Encode target labels\n# ---------------------------\nlabel_enc = LabelEncoder()\ny_train_encoded = label_enc.fit_transform(y_train_aug)\ny_test_encoded  = label_enc.transform(y_test)\n\nprint(f\"Processed training shape: {X_train_processed.shape}\")\nprint(f\"Processed test shape: {X_test_processed.shape}\")\n\nclass TabDataset(Dataset):\n    def __init__(self, X_processed, y_encoded, cat_indices, num_indices):\n        # X_processed is the 122-feature matrix from the ColumnTransformer\n        self.x_cat = torch.tensor(X_processed[:, cat_indices], dtype=torch.long)\n        self.x_num = torch.tensor(X_processed[:, num_indices], dtype=torch.float32)\n        self.y = torch.tensor(y_encoded, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return self.x_cat[idx], self.x_num[idx], self.y[idx]\n\n# Identify indices for cat and num features in the 122-column processed matrix\n# Assuming 'preprocessor' is the ColumnTransformer from previous step\ncat_slice = preprocessor.output_indices_['cat']\nnum_slice = preprocessor.output_indices_['num']\n\ntrain_dataset = TabDataset(X_train_processed, y_train_encoded, cat_slice, num_slice)\ntest_dataset = TabDataset(X_test_processed, y_test_encoded, cat_slice, num_slice)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T06:22:06.751043Z","iopub.execute_input":"2026-01-17T06:22:06.751420Z","iopub.status.idle":"2026-01-17T06:22:10.184256Z","shell.execute_reply.started":"2026-01-17T06:22:06.751394Z","shell.execute_reply":"2026-01-17T06:22:10.183652Z"}},"outputs":[{"name":"stdout","text":"Test data retained: 18794 samples\nFiltered test label distribution:\n outcome\nnormal             9711\nneptune            4657\nguess_passwd       1231\nwarezmaster         944\nsatan               735\nsmurf               665\nback                359\nportsweep           157\nipsweep             141\nnmap                 73\npod                  41\nbuffer_overflow      20\nmultihop             18\nrootkit              13\nteardrop             12\nland                  7\nftp_write             3\nperl                  2\nloadmodule            2\nphf                   2\nimap                  1\nName: count, dtype: int64\nProcessed training shape: (557934, 122)\nProcessed test shape: (18794, 122)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# 1. Determine number of classes from the LabelEncoder\nnum_classes = len(label_enc.classes_)\n\n# 2. Identify column indices for cat and num features in the processed matrix\n# categorical_cols were ['protocol_type', 'service', 'flag']\ncat_cols = ['protocol_type', 'service', 'flag']\nnum_cols = [c for c in X_train_aug.columns if c not in cat_cols]\n\n# We need to know which columns in X_train_processed belong to 'num' and 'cat'\n# Based on your ColumnTransformer: num is first, then cat\nnum_feature_count = len(num_cols)\n# Categorical features are One-Hot Encoded, so we find where they start\n# X_train_processed[:, :num_feature_count] is numerical\n# X_train_processed[:, num_feature_count:] is categorical (as one-hot)\n\nclass TabDataset(Dataset):\n    def __init__(self, X, y, num_feat_count):\n        # Convert sparse to dense if necessary\n        if hasattr(X, \"toarray\"): X = X.toarray()\n        \n        # In your ColumnTransformer, numerical columns come first\n        self.x_num = torch.tensor(X[:, :num_feat_count], dtype=torch.float32)\n        \n        # For TabTransformer, categorical features usually need Label Encoding \n        # for embeddings, but since you used OneHotEncoder in the preprocessor,\n        # we will pass the numerical and encoded features as separate tensors.\n        # NOTE: To use nn.Embedding, we need the raw categorical integer codes.\n        self.x_cat = torch.tensor(X[:, num_feat_count:], dtype=torch.float32) \n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return self.x_cat[idx], self.x_num[idx], self.y[idx]\n\nBATCH_SIZE = 512\ntrain_dataset = TabDataset(X_train_processed, y_train_encoded, num_feature_count)\ntest_dataset  = TabDataset(X_test_processed, y_test_encoded, num_feature_count)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T06:25:08.719598Z","iopub.execute_input":"2026-01-17T06:25:08.720182Z","iopub.status.idle":"2026-01-17T06:25:08.883132Z","shell.execute_reply.started":"2026-01-17T06:25:08.720153Z","shell.execute_reply":"2026-01-17T06:25:08.882095Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"print(df_train.columns.tolist())\nprint(df_test.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T06:25:13.538839Z","iopub.execute_input":"2026-01-17T06:25:13.539552Z","iopub.status.idle":"2026-01-17T06:25:13.543382Z","shell.execute_reply.started":"2026-01-17T06:25:13.539508Z","shell.execute_reply":"2026-01-17T06:25:13.542717Z"}},"outputs":[{"name":"stdout","text":"['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'level', 'outcome']\n['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'outcome', 'level']\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import torch.nn as nn\n\nclass TabTransformer(nn.Module):\n    def __init__(self, cat_feature_count, num_features, num_classes, emb_dim=32, depth=4, heads=4, dropout=0.1):\n        super().__init__()\n\n        # Since your preprocessor used OneHotEncoder, cat_feature_count is the \n        # total number of binary columns created (e.g., ~84).\n        # We project these into the embedding space to work with the Transformer.\n        self.cat_projection = nn.Linear(cat_feature_count, emb_dim)\n\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(\n                d_model=emb_dim,\n                nhead=heads,\n                dim_feedforward=emb_dim * 4,\n                dropout=dropout,\n                batch_first=True\n            ),\n            num_layers=depth\n        )\n\n        self.mlp = nn.Sequential(\n            nn.Linear(emb_dim + num_features, 256),\n            nn.ReLU(),\n            nn.BatchNorm1d(256),\n            nn.Dropout(0.3),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x_cat, x_num):\n        # x_cat is [batch, cat_onehot_dim]\n        x = self.cat_projection(x_cat) # [batch, emb_dim]\n        x = x.unsqueeze(1)             # [batch, 1, emb_dim] for Transformer\n        x = self.transformer(x)\n        x = x.squeeze(1)               # [batch, emb_dim]\n        \n        x = torch.cat([x, x_num], dim=1)\n        return self.mlp(x)\n\n# Initialization\ncat_onehot_dim = X_train_processed.shape[1] - num_feature_count\nmodel = TabTransformer(cat_onehot_dim, num_feature_count, num_classes).to(DEVICE)\n\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR)\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T06:25:16.379844Z","iopub.execute_input":"2026-01-17T06:25:16.380154Z","iopub.status.idle":"2026-01-17T06:25:19.698038Z","shell.execute_reply.started":"2026-01-17T06:25:16.380128Z","shell.execute_reply":"2026-01-17T06:25:19.697485Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n# 1. Define the FocalLoss class\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=None, gamma=2):\n        super().__init__()\n        self.gamma = gamma\n        self.alpha = alpha # If provided, this should be a tensor of weights per class\n\n    def forward(self, inputs, targets):\n        # inputs: [batch_size, num_classes], targets: [batch_size]\n        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n        pt = torch.exp(-ce_loss)\n        loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n        return loss\n\n# 2. Re-initialize the model\ncat_onehot_dim = X_train_processed.shape[1] - num_feature_count\nmodel = TabTransformer(cat_onehot_dim, num_feature_count, num_classes).to(DEVICE)\n\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\n\n# 3. Setup Optimizer and Focal Loss\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n\n# OPTIONAL: Calculate alpha weights to further handle imbalance\n# This gives more weight to classes with fewer samples\nclass_counts = np.bincount(y_train_encoded)\nweights = 1.0 / (class_counts + 1e-6)\nweights = torch.tensor(weights / weights.sum() * num_classes).float().to(DEVICE)\n\n# Set criterion to FocalLoss\ncriterion = FocalLoss(alpha=weights, gamma=2)\n\n# ---------------------------\n# Training Loop remains the same\n# ---------------------------\ndef train_epoch(model, loader):\n    model.train()\n    total_loss = 0\n    for x_cat, x_num, y in loader:\n        x_cat, x_num, y = x_cat.to(DEVICE), x_num.to(DEVICE), y.to(DEVICE)\n        \n        logits = model(x_cat, x_num)\n        \n        # Consistency Regularization\n        noise = torch.randn_like(x_num) * 0.05\n        logits_aug = model(x_cat, x_num + noise)\n        \n        loss_cls = criterion(logits, y)  # Now uses Focal Loss\n        loss_cons = torch.mean((logits - logits_aug)**2)\n        loss = loss_cls + LAMBDA * loss_cons\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T06:51:41.927674Z","iopub.execute_input":"2026-01-17T06:51:41.928592Z","iopub.status.idle":"2026-01-17T06:51:41.950361Z","shell.execute_reply.started":"2026-01-17T06:51:41.928559Z","shell.execute_reply":"2026-01-17T06:51:41.949492Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report\n\ndef train_epoch(model, loader):\n    model.train()\n    total_loss = 0\n    for x_cat, x_num, y in loader:\n        x_cat, x_num, y = x_cat.to(DEVICE), x_num.to(DEVICE), y.to(DEVICE)\n        \n        logits = model(x_cat, x_num)\n        \n        # Consistency Regularization (Optional, based on your previous code)\n        noise = torch.randn_like(x_num) * 0.05\n        logits_aug = model(x_cat, x_num + noise)\n        \n        loss_cls = criterion(logits, y)\n        loss_cons = torch.mean((logits - logits_aug)**2)\n        loss = loss_cls + LAMBDA * loss_cons\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(loader)\n\ndef evaluate(model, loader):\n    model.eval()\n    all_preds, all_true = [], []\n    with torch.no_grad():\n        for x_cat, x_num, y in loader:\n            x_cat, x_num = x_cat.to(DEVICE), x_num.to(DEVICE)\n            logits = model(x_cat, x_num)\n            preds = torch.argmax(logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_true.extend(y.numpy())\n\n    print(\"Accuracy:\", accuracy_score(all_true, all_preds))\n    print(classification_report(all_true, all_preds, target_names=label_enc.classes_))\n\n# Execution Loop\nfor epoch in range(EPOCHS):\n    loss = train_epoch(model, train_loader)\n    print(f\"Epoch {epoch+1}: Loss = {loss:.4f}\")\n\nevaluate(model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T06:51:41.951964Z","iopub.execute_input":"2026-01-17T06:51:41.952377Z","iopub.status.idle":"2026-01-17T07:13:49.329615Z","shell.execute_reply.started":"2026-01-17T06:51:41.952345Z","shell.execute_reply":"2026-01-17T07:13:49.328839Z"}},"outputs":[{"name":"stdout","text":"Epoch 1: Loss = 0.2281\nEpoch 2: Loss = 0.1563\nEpoch 3: Loss = 0.1480\nEpoch 4: Loss = 0.1438\nEpoch 5: Loss = 0.1415\nEpoch 6: Loss = 0.1399\nEpoch 7: Loss = 0.1389\nEpoch 8: Loss = 0.1382\nEpoch 9: Loss = 0.1372\nEpoch 10: Loss = 0.1374\nEpoch 11: Loss = 0.1365\nEpoch 12: Loss = 0.1358\nEpoch 13: Loss = 0.1353\nEpoch 14: Loss = 0.1352\nEpoch 15: Loss = 0.1350\nEpoch 16: Loss = 0.1347\nEpoch 17: Loss = 0.1346\nEpoch 18: Loss = 0.1343\nEpoch 19: Loss = 0.1338\nEpoch 20: Loss = 0.1337\nEpoch 21: Loss = 0.1335\nEpoch 22: Loss = 0.1337\nEpoch 23: Loss = 0.1338\nEpoch 24: Loss = 0.1333\nEpoch 25: Loss = 0.1327\nEpoch 26: Loss = 0.1332\nEpoch 27: Loss = 0.1328\nEpoch 28: Loss = 0.1328\nEpoch 29: Loss = 0.1326\nEpoch 30: Loss = 0.1327\nAccuracy: 0.8183994891986804\n                 precision    recall  f1-score   support\n\n           back       0.98      1.00      0.99       359\nbuffer_overflow       0.04      0.20      0.06        20\n      ftp_write       0.03      0.33      0.06         3\n   guess_passwd       0.38      0.00      0.01      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.89      0.98      0.93       141\n           land       1.00      1.00      1.00         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.03      0.06      0.04        18\n        neptune       1.00      0.99      0.99      4657\n           nmap       0.75      1.00      0.86        73\n         normal       0.88      0.89      0.88      9711\n           perl       0.25      1.00      0.40         2\n            phf       0.04      0.50      0.08         2\n            pod       0.55      0.88      0.68        41\n      portsweep       0.78      0.93      0.85       157\n        rootkit       0.01      0.08      0.02        13\n          satan       0.74      0.96      0.83       735\n          smurf       0.97      1.00      0.98       665\n            spy       0.00      0.00      0.00         0\n       teardrop       0.03      1.00      0.06        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       0.00      0.00      0.00       944\n\n       accuracy                           0.82     18794\n      macro avg       0.41      0.56      0.42     18794\n   weighted avg       0.82      0.82      0.81     18794\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}