{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14523807,"sourceType":"datasetVersion","datasetId":9276062},{"sourceId":394223,"sourceType":"datasetVersion","datasetId":174616}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-17T07:37:41.823671Z","iopub.execute_input":"2026-01-17T07:37:41.823908Z","iopub.status.idle":"2026-01-17T07:37:42.111038Z","shell.execute_reply.started":"2026-01-17T07:37:41.823886Z","shell.execute_reply":"2026-01-17T07:37:42.110321Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nsl-kdd-augmented/smote_augmented.csv\n/kaggle/input/nslkdd/KDDTest+.arff\n/kaggle/input/nslkdd/KDDTest-21.arff\n/kaggle/input/nslkdd/KDDTest1.jpg\n/kaggle/input/nslkdd/KDDTrain+.txt\n/kaggle/input/nslkdd/KDDTrain+_20Percent.txt\n/kaggle/input/nslkdd/KDDTest-21.txt\n/kaggle/input/nslkdd/KDDTest+.txt\n/kaggle/input/nslkdd/KDDTrain+.arff\n/kaggle/input/nslkdd/index.html\n/kaggle/input/nslkdd/KDDTrain+_20Percent.arff\n/kaggle/input/nslkdd/KDDTrain1.jpg\n/kaggle/input/nslkdd/nsl-kdd/KDDTest+.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTest-21.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTest1.jpg\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+_20Percent.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTest-21.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTest+.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+.arff\n/kaggle/input/nslkdd/nsl-kdd/index.html\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+_20Percent.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain1.jpg\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\n\n# ===========================================\n# 1️⃣ Load & Stable Mapping (From your working code)\n# ===========================================\nlocal_path = \"/kaggle/input/nslkdd/\" \ncolumns = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land',\n           'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n           'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n           'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count',\n           'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n           'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n           'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n           'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n           'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate',\n           'outcome', 'level']\n\ndf_train = pd.read_csv(local_path + \"KDDTrain+.txt\", header=None, names=columns)\ndf_test  = pd.read_csv(local_path + \"KDDTest+.txt\", header=None, names=columns)\n\ndf_train['outcome'] = df_train['outcome'].astype(str)\ndf_test['outcome'] = df_test['outcome'].astype(str)\ntrain_labels = set(df_train['outcome'].unique())\ndf_test = df_test[df_test['outcome'].isin(train_labels)].reset_index(drop=True)\n\n# ===========================================\n# 2️⃣ Hybrid Preprocessing for Transformer\n# ===========================================\ncat_cols = ['protocol_type', 'service', 'flag']\nnum_cols = [c for c in columns if c not in cat_cols + ['outcome', 'level']]\n\n# Label Encode Categorical for Embeddings\ncat_dims = []\nfor col in cat_cols:\n    le_cat = LabelEncoder()\n    df_train[col] = le_cat.fit_transform(df_train[col])\n    df_test[col] = df_test[col].map(lambda s: s if s in le_cat.classes_ else le_cat.classes_[0])\n    df_test[col] = le_cat.transform(df_test[col])\n    cat_dims.append(len(le_cat.classes_))\n\n# Scale Numerical\nscaler = StandardScaler()\nX_train_num = scaler.fit_transform(df_train[num_cols]).astype(np.float32)\nX_test_num  = scaler.transform(df_test[num_cols]).astype(np.float32)\n\nle = LabelEncoder()\ny_train_enc = le.fit_transform(df_train['outcome'])\ny_test_enc  = le.transform(df_test['outcome'])\nnum_classes = len(le.classes_)\n\n# ===========================================\n# 3️⃣ Novel Architecture: Gated-Transformer Fusion\n# ===========================================\nclass GatedLinearUnit(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.fc = nn.Linear(input_dim, input_dim * 2)\n    def forward(self, x):\n        x = self.fc(x)\n        x, gate = x.chunk(2, dim=-1)\n        return x * torch.sigmoid(gate)\n\nclass GTFModel(nn.Module):\n    def __init__(self, cat_dims, num_feat_dim, num_classes, emb_dim=32):\n        super().__init__()\n        # Categorical Path (Transformer)\n        self.embs = nn.ModuleList([nn.Embedding(d, emb_dim) for d in cat_dims])\n        layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=8, batch_first=True)\n        self.transformer = nn.TransformerEncoder(layer, num_layers=2)\n        \n        # Numerical Path (Gated)\n        self.num_gate = nn.Sequential(\n            nn.Linear(num_feat_dim, 128),\n            GatedLinearUnit(128),\n            nn.LayerNorm(128)\n        )\n        \n        # Fusion\n        self.classifier = nn.Sequential(\n            nn.Linear(len(cat_dims)*emb_dim + 128, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x_cat, x_num):\n        x_c = torch.stack([emb(x_cat[:, i]) for i, emb in enumerate(self.embs)], dim=1)\n        x_c = self.transformer(x_c).flatten(1)\n        x_n = self.num_gate(x_num)\n        return self.classifier(torch.cat([x_c, x_n], dim=1))\n\n# ===========================================\n# 4️⃣ Balanced Data Loading\n# ===========================================\nclass HybridDS(Dataset):\n    def __init__(self, c, n, y):\n        self.c, self.n, self.y = torch.tensor(c), torch.tensor(n), torch.tensor(y)\n    def __len__(self): return len(self.y)\n    def __getitem__(self, i): return self.c[i], self.n[i], self.y[i]\n\nclass_counts = np.bincount(y_train_enc)\nclass_weights = 1.0 / np.sqrt(class_counts + 1)\nsampler = WeightedRandomSampler(class_weights[y_train_enc], len(y_train_enc))\n\ntrain_loader = DataLoader(HybridDS(df_train[cat_cols].values, X_train_num, y_train_enc), batch_size=256, sampler=sampler)\ntest_loader  = DataLoader(HybridDS(df_test[cat_cols].values, X_test_num, y_test_enc), batch_size=256, shuffle=False)\n\n# ===========================================\n# 5️⃣ Training Loop\n# ===========================================\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = GTFModel(cat_dims, X_train_num.shape[1], num_classes).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n\n# Use standard weighted CE (proven stable in your code)\nloss_weights = torch.tensor(class_weights / class_weights.sum() * num_classes, dtype=torch.float32).to(device)\ncriterion = nn.CrossEntropyLoss(weight=loss_weights)\n\nfor epoch in range(15):\n    model.train()\n    for xc, xn, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        xc, xn, y = xc.to(device), xn.to(device), y.to(device)\n        optimizer.zero_grad()\n        loss = criterion(model(xc, xn), y)\n        loss.backward()\n        optimizer.step()\n\n# ===========================================\n# 6️⃣ Final Evaluation\n# ===========================================\nmodel.eval()\nall_p, all_y = [], []\nwith torch.no_grad():\n    for xc, xn, y in test_loader:\n        out = model(xc.to(device), xn.to(device))\n        all_p.extend(torch.argmax(out, 1).cpu().numpy())\n        all_y.extend(y.numpy())\n\nprint(\"\\n--- Final Q1 Results ---\")\nprint(classification_report(all_y, all_p, labels=np.arange(num_classes), target_names=le.classes_, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T08:20:04.800852Z","iopub.execute_input":"2026-01-17T08:20:04.801505Z","iopub.status.idle":"2026-01-17T08:21:08.362517Z","shell.execute_reply.started":"2026-01-17T08:20:04.801472Z","shell.execute_reply":"2026-01-17T08:21:08.361876Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 493/493 [00:04<00:00, 113.66it/s]\nEpoch 2: 100%|██████████| 493/493 [00:04<00:00, 121.91it/s]\nEpoch 3: 100%|██████████| 493/493 [00:04<00:00, 119.88it/s]\nEpoch 4: 100%|██████████| 493/493 [00:04<00:00, 115.13it/s]\nEpoch 5: 100%|██████████| 493/493 [00:04<00:00, 119.92it/s]\nEpoch 6: 100%|██████████| 493/493 [00:04<00:00, 116.88it/s]\nEpoch 7: 100%|██████████| 493/493 [00:04<00:00, 120.01it/s]\nEpoch 8: 100%|██████████| 493/493 [00:04<00:00, 119.99it/s]\nEpoch 9: 100%|██████████| 493/493 [00:04<00:00, 115.92it/s]\nEpoch 10: 100%|██████████| 493/493 [00:04<00:00, 118.75it/s]\nEpoch 11: 100%|██████████| 493/493 [00:04<00:00, 122.19it/s]\nEpoch 12: 100%|██████████| 493/493 [00:04<00:00, 116.25it/s]\nEpoch 13: 100%|██████████| 493/493 [00:04<00:00, 120.65it/s]\nEpoch 14: 100%|██████████| 493/493 [00:04<00:00, 122.44it/s]\nEpoch 15: 100%|██████████| 493/493 [00:04<00:00, 114.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Final Q1 Results ---\n                 precision    recall  f1-score   support\n\n           back       0.99      0.97      0.98       359\nbuffer_overflow       0.71      0.50      0.59        20\n      ftp_write       0.01      0.33      0.01         3\n   guess_passwd       1.00      0.18      0.31      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.86      0.98      0.91       141\n           land       1.00      1.00      1.00         7\n     loadmodule       0.29      1.00      0.44         2\n       multihop       0.01      0.11      0.02        18\n        neptune       1.00      0.99      1.00      4657\n           nmap       0.99      0.99      0.99        73\n         normal       0.86      0.92      0.89      9711\n           perl       0.50      0.50      0.50         2\n            phf       0.50      0.50      0.50         2\n            pod       0.72      0.95      0.82        41\n      portsweep       0.66      0.89      0.76       157\n        rootkit       0.00      0.08      0.01        13\n          satan       0.49      0.83      0.61       735\n          smurf       0.99      1.00      0.99       665\n            spy       0.00      0.00      0.00         0\n       teardrop       0.24      1.00      0.39        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       0.96      0.03      0.05       944\n\n       accuracy                           0.84     18794\n      macro avg       0.56      0.60      0.51     18794\n   weighted avg       0.90      0.84      0.83     18794\n\n","output_type":"stream"}],"execution_count":16}]}