{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":394223,"sourceType":"datasetVersion","datasetId":174616},{"sourceId":14523807,"sourceType":"datasetVersion","datasetId":9276062}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-17T11:57:30.176866Z","iopub.execute_input":"2026-01-17T11:57:30.177192Z","iopub.status.idle":"2026-01-17T11:57:30.207055Z","shell.execute_reply.started":"2026-01-17T11:57:30.177145Z","shell.execute_reply":"2026-01-17T11:57:30.206368Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nsl-kdd-augmented/smote_augmented.csv\n/kaggle/input/nslkdd/KDDTest+.arff\n/kaggle/input/nslkdd/KDDTest-21.arff\n/kaggle/input/nslkdd/KDDTest1.jpg\n/kaggle/input/nslkdd/KDDTrain+.txt\n/kaggle/input/nslkdd/KDDTrain+_20Percent.txt\n/kaggle/input/nslkdd/KDDTest-21.txt\n/kaggle/input/nslkdd/KDDTest+.txt\n/kaggle/input/nslkdd/KDDTrain+.arff\n/kaggle/input/nslkdd/index.html\n/kaggle/input/nslkdd/KDDTrain+_20Percent.arff\n/kaggle/input/nslkdd/KDDTrain1.jpg\n/kaggle/input/nslkdd/nsl-kdd/KDDTest+.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTest-21.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTest1.jpg\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+_20Percent.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTest-21.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTest+.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+.arff\n/kaggle/input/nslkdd/nsl-kdd/index.html\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+_20Percent.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain1.jpg\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# ===========================================\n# 1️⃣ Sampling: Strategic Over-Representation\n# ===========================================\nclass_counts = np.bincount(y_train_enc)\n# Q1 Tech: Custom frequency smoothing\n# We use a 0.3 power to give even more weight to the rarest classes than before\nweights = 1.0 / np.power(class_counts + 1, 0.7) \nsamples_weight = torch.from_numpy(weights[y_train_enc])\nsampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n\ntrain_loader = DataLoader(\n    torch.utils.data.TensorDataset(torch.tensor(X_train_proc, dtype=torch.float32), torch.tensor(y_train_enc, dtype=torch.long)),\n    batch_size=512, sampler=sampler\n)\n\n# ===========================================\n# 2️⃣ NOVELTY: Margin-based Poly-Focal Loss\n# ===========================================\nclass Q1_MarginPolyLoss(nn.Module):\n    \"\"\"\n    Combines Focal Loss, Poly-1 Loss, and Class-Adaptive Margins.\n    Specifically designed to decouple 'Normal' from 'R2L/U2R' attacks.\n    \"\"\"\n    def __init__(self, class_counts, gamma=2.0, epsilon=1.0):\n        super().__init__()\n        self.gamma = gamma\n        self.epsilon = epsilon\n        # Pre-calculate margins: larger margins for minority classes\n        self.margins = torch.tensor(1.0 / np.log1p(class_counts)).float().to(device)\n        self.margins = (self.margins / self.margins.max()) * 2.0 \n\n    def forward(self, logits, targets):\n        # Apply Class-Adaptive Margin\n        mask = F.one_hot(targets, num_classes=logits.shape[1]).float()\n        logits = logits - (mask * self.margins)\n        \n        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n        pt = torch.exp(-ce_loss)\n        \n        # Poly-Focal Hybrid\n        loss = (1 - pt)**self.gamma * ce_loss + self.epsilon * (1 - pt)\n        return loss.mean()\n\n# ===========================================\n# 3️⃣ NOVELTY: LMD-Net (Latent Manifold Decoupler)\n# ===========================================\nclass LMD_Net(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        # Expansion Path: Finds hidden separation dimensions\n        self.expansion = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.GELU(),\n            nn.LayerNorm(1024)\n        )\n        \n        # Gated BottleNeck\n        self.gate = nn.Sequential(nn.Linear(1024, 1024), nn.Sigmoid())\n        \n        self.fc_blocks = nn.Sequential(\n            nn.Linear(1024, 512),\n            nn.GELU(),\n            nn.Dropout(0.4),\n            nn.Linear(512, 256),\n            nn.GELU(),\n            nn.LayerNorm(256)\n        )\n        self.head = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = self.expansion(x)\n        # Gating recalibrates the expanded manifold\n        g = self.gate(x)\n        x = x * g\n        x = self.fc_blocks(x)\n        return self.head(x)\n\n# ===========================================\n# 4️⃣ Execution\n# ===========================================\nmodel = LMD_Net(X_train_proc.shape[1], num_classes).to(device)\ncriterion = Q1_MarginPolyLoss(class_counts)\noptimizer = torch.optim.AdamW(model.parameters(), lr=8e-4, weight_decay=1e-2)\n# Cosine annealing helps find the tiny local minima for rare classes\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15)\n\nfor epoch in range(20):\n    model.train()\n    for X_b, y_b in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        X_b, y_b = X_b.to(device), y_b.to(device)\n        optimizer.zero_grad()\n        loss = criterion(model(X_b), y_b)\n        loss.backward()\n        optimizer.step()\n    scheduler.step()\n\n# Evaluation\nmodel.eval()\nall_p, all_y = [], []\nwith torch.no_grad():\n    for X_b, y_b in test_loader:\n        out = model(X_b.to(device))\n        all_p.extend(torch.argmax(out, dim=1).cpu().numpy())\n        all_y.extend(y_b.numpy())\n\nprint(\"\\n--- LMD-Net Q1 Results ---\")\nprint(classification_report(all_y, all_p, labels=np.arange(num_classes), target_names=le.classes_, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T12:51:27.292989Z","iopub.execute_input":"2026-01-17T12:51:27.293316Z","iopub.status.idle":"2026-01-17T12:51:57.116912Z","shell.execute_reply.started":"2026-01-17T12:51:27.293291Z","shell.execute_reply":"2026-01-17T12:51:57.116331Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 247/247 [00:01<00:00, 172.31it/s]\nEpoch 2: 100%|██████████| 247/247 [00:01<00:00, 173.96it/s]\nEpoch 3: 100%|██████████| 247/247 [00:01<00:00, 154.51it/s]\nEpoch 4: 100%|██████████| 247/247 [00:01<00:00, 173.13it/s]\nEpoch 5: 100%|██████████| 247/247 [00:01<00:00, 172.41it/s]\nEpoch 6: 100%|██████████| 247/247 [00:01<00:00, 151.50it/s]\nEpoch 7: 100%|██████████| 247/247 [00:01<00:00, 171.36it/s]\nEpoch 8: 100%|██████████| 247/247 [00:01<00:00, 173.05it/s]\nEpoch 9: 100%|██████████| 247/247 [00:01<00:00, 173.72it/s]\nEpoch 10: 100%|██████████| 247/247 [00:01<00:00, 153.77it/s]\nEpoch 11: 100%|██████████| 247/247 [00:01<00:00, 173.64it/s]\nEpoch 12: 100%|██████████| 247/247 [00:01<00:00, 172.46it/s]\nEpoch 13: 100%|██████████| 247/247 [00:01<00:00, 170.18it/s]\nEpoch 14: 100%|██████████| 247/247 [00:01<00:00, 153.52it/s]\nEpoch 15: 100%|██████████| 247/247 [00:01<00:00, 172.41it/s]\nEpoch 16: 100%|██████████| 247/247 [00:01<00:00, 171.54it/s]\nEpoch 17: 100%|██████████| 247/247 [00:01<00:00, 155.16it/s]\nEpoch 18: 100%|██████████| 247/247 [00:01<00:00, 174.48it/s]\nEpoch 19: 100%|██████████| 247/247 [00:01<00:00, 173.61it/s]\nEpoch 20: 100%|██████████| 247/247 [00:01<00:00, 167.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- LMD-Net Q1 Results ---\n                 precision    recall  f1-score   support\n\n           back       0.99      0.80      0.89       359\nbuffer_overflow       0.71      0.50      0.59        20\n      ftp_write       0.01      0.67      0.02         3\n   guess_passwd       0.09      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.95      0.98      0.96       141\n           land       1.00      1.00      1.00         7\n     loadmodule       1.00      1.00      1.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      1.00      1.00      4657\n           nmap       0.99      1.00      0.99        73\n         normal       0.83      0.97      0.90      9711\n           perl       0.50      0.50      0.50         2\n            phf       0.33      0.50      0.40         2\n            pod       0.72      0.95      0.82        41\n      portsweep       0.64      0.94      0.76       157\n        rootkit       0.01      0.08      0.01        13\n          satan       0.66      0.76      0.71       735\n          smurf       1.00      1.00      1.00       665\n            spy       0.00      0.00      0.00         0\n       teardrop       0.24      1.00      0.39        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       1.00      0.03      0.07       944\n\n       accuracy                           0.85     18794\n      macro avg       0.55      0.59      0.52     18794\n   weighted avg       0.83      0.85      0.81     18794\n\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# ===========================================\n# 1️⃣ Strategic Sampler: Focus on R2L/U2R\n# ===========================================\nclass_counts = np.bincount(y_train_enc)\n# Q1 Tuning: Focus heavily on content-based attacks\nweights = 1.0 / (np.power(class_counts + 1, 0.85)) \nsamples_weight = torch.from_numpy(weights[y_train_enc])\nsampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n\ntrain_loader = DataLoader(\n    torch.utils.data.TensorDataset(torch.tensor(X_train_proc, dtype=torch.float32), torch.tensor(y_train_enc, dtype=torch.long)),\n    batch_size=512, sampler=sampler\n)\ntest_loader = DataLoader(\n    torch.utils.data.TensorDataset(torch.tensor(X_test_proc, dtype=torch.float32), torch.tensor(y_test_enc, dtype=torch.long)),\n    batch_size=512, shuffle=False\n)\n\n# ===========================================\n# 2️⃣ NOVELTY: Orthogonal Boundary Loss (OBL)\n# ===========================================\nclass OrthogonalBoundaryLoss(nn.Module):\n    def __init__(self, class_counts, gamma=2.0):\n        super().__init__()\n        self.gamma = gamma\n        # Dynamic smoothing factor: Inverse log frequency\n        adj = torch.tensor(1.0 / np.log1p(class_counts)).float().to(device)\n        self.adj = (adj / adj.sum()) * len(class_counts)\n\n    def forward(self, logits, targets):\n        # Apply cost-sensitive smoothing to logits\n        logits = logits * self.adj.unsqueeze(0)\n        \n        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n        pt = torch.exp(-ce_loss)\n        \n        # Focal-style modulation to focus on hard-to-classify R2L samples\n        loss = (1 - pt)**self.gamma * ce_loss\n        return loss.mean()\n\n# ===========================================\n# 3️⃣ NOVELTY: OFG-Net (Orthogonal Feature Gating)\n# ===========================================\nclass OFG_Net(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        self.stem = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.BatchNorm1d(512),\n            nn.SiLU() # Smooth activation for better gradient flow\n        )\n        \n        # Novelty: Tanh centered gating for feature orthogonalization\n        self.ortho_gate = nn.Sequential(\n            nn.Linear(512, 512),\n            nn.Tanh(), \n            nn.Linear(512, 512),\n            nn.Sigmoid()\n        )\n        \n        self.bottleneck = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.LayerNorm(256),\n            nn.SiLU(),\n            nn.Dropout(0.4)\n        )\n        self.head = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        features = self.stem(x)\n        gate = self.ortho_gate(features)\n        # Element-wise gating to suppress majority class noise\n        x = features * gate\n        x = self.bottleneck(x)\n        return self.head(x)\n\n# ===========================================\n# 4️⃣ Execution Loop (Fixed Variable Names)\n# ===========================================\nmodel = OFG_Net(X_train_proc.shape[1], num_classes).to(device)\ncriterion = OrthogonalBoundaryLoss(class_counts)\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-2)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n\nEPOCHS = 20\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss = 0\n    for X_b, y_b in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        X_b, y_b = X_b.to(device), y_b.to(device) # Fixed variable names\n        \n        optimizer.zero_grad()\n        outputs = model(X_b)\n        loss = criterion(outputs, y_b)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    \n    avg_loss = total_loss / len(train_loader)\n    scheduler.step(avg_loss)\n    print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f}\")\n\n# ===========================================\n# 5️⃣ Final Evaluation\n# ===========================================\nmodel.eval()\nall_p, all_y = [], []\nwith torch.no_grad():\n    for X_b, y_b in test_loader:\n        out = model(X_b.to(device))\n        all_p.extend(torch.argmax(out, dim=1).cpu().numpy())\n        all_y.extend(y_b.numpy())\n\nprint(\"\\n--- OFG-Net Final Q1 Results ---\")\nprint(classification_report(all_y, all_p, labels=np.arange(num_classes), \n                            target_names=le.classes_, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T12:55:23.221538Z","iopub.execute_input":"2026-01-17T12:55:23.222299Z","iopub.status.idle":"2026-01-17T12:55:55.516727Z","shell.execute_reply.started":"2026-01-17T12:55:23.222262Z","shell.execute_reply":"2026-01-17T12:55:55.516058Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 247/247 [00:01<00:00, 145.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Loss: 0.1310\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 247/247 [00:01<00:00, 161.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Loss: 0.0211\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 247/247 [00:01<00:00, 165.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Loss: 0.0212\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 247/247 [00:01<00:00, 148.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Loss: 0.0144\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 247/247 [00:01<00:00, 162.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Loss: 0.0150\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 247/247 [00:01<00:00, 148.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 | Loss: 0.0155\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 247/247 [00:01<00:00, 162.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 | Loss: 0.0127\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|██████████| 247/247 [00:01<00:00, 145.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 | Loss: 0.0136\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|██████████| 247/247 [00:01<00:00, 163.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 | Loss: 0.0117\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|██████████| 247/247 [00:01<00:00, 149.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Loss: 0.0145\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11: 100%|██████████| 247/247 [00:01<00:00, 164.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 | Loss: 0.0145\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12: 100%|██████████| 247/247 [00:01<00:00, 149.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 | Loss: 0.0131\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13: 100%|██████████| 247/247 [00:01<00:00, 147.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 | Loss: 0.0100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14: 100%|██████████| 247/247 [00:01<00:00, 160.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 | Loss: 0.0103\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15: 100%|██████████| 247/247 [00:01<00:00, 146.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15 | Loss: 0.0094\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16: 100%|██████████| 247/247 [00:01<00:00, 164.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16 | Loss: 0.0097\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17: 100%|██████████| 247/247 [00:01<00:00, 148.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17 | Loss: 0.0090\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18: 100%|██████████| 247/247 [00:01<00:00, 164.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18 | Loss: 0.0094\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19: 100%|██████████| 247/247 [00:01<00:00, 148.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19 | Loss: 0.0093\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20: 100%|██████████| 247/247 [00:01<00:00, 164.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20 | Loss: 0.0087\n\n--- OFG-Net Final Q1 Results ---\n                 precision    recall  f1-score   support\n\n           back       0.99      0.81      0.89       359\nbuffer_overflow       0.00      0.00      0.00        20\n      ftp_write       0.00      0.00      0.00         3\n   guess_passwd       0.00      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.88      0.98      0.93       141\n           land       0.00      0.00      0.00         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      0.99      1.00      4657\n           nmap       0.95      0.99      0.97        73\n         normal       0.79      0.97      0.88      9711\n           perl       0.50      0.50      0.50         2\n            phf       0.50      0.50      0.50         2\n            pod       0.72      0.95      0.82        41\n      portsweep       0.65      0.88      0.75       157\n        rootkit       0.00      0.00      0.00        13\n          satan       0.79      0.75      0.77       735\n          smurf       1.00      1.00      1.00       665\n            spy       0.00      0.00      0.00         0\n       teardrop       0.24      1.00      0.39        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       0.95      0.04      0.09       944\n\n       accuracy                           0.85     18794\n      macro avg       0.43      0.45      0.41     18794\n   weighted avg       0.81      0.85      0.81     18794\n\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# ===========================================\n# 1️⃣ Balanced Sampler (Square Root Smoothing)\n# ===========================================\nclass_counts = np.bincount(y_train_enc)\n# Q1 Logic: 1/sqrt(n) is the mathematical sweet spot for balancing Accuracy vs F1\nweights = 1.0 / np.sqrt(class_counts + 1)\nsamples_weight = torch.from_numpy(weights[y_train_enc])\nsampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n\ntrain_loader = DataLoader(\n    torch.utils.data.TensorDataset(torch.tensor(X_train_proc, dtype=torch.float32), torch.tensor(y_train_enc, dtype=torch.long)),\n    batch_size=512, sampler=sampler\n)\n\n# ===========================================\n# 2️⃣ NOVELTY: Cost-Sensitive Sharpness Loss (CS-Sharp)\n# ===========================================\nclass CS_SharpLoss(nn.Module):\n    def __init__(self, class_counts):\n        super().__init__()\n        # Calculate Tau: Sharpness increases for rare classes\n        counts = torch.tensor(class_counts).float().to(device)\n        self.tau = torch.log1p(counts) / torch.log1p(counts).max()\n        # Ensure tau is not zero\n        self.tau = torch.clamp(self.tau, min=0.1)\n\n    def forward(self, logits, targets):\n        # Sharpen minority class logits\n        scaled_logits = logits / self.tau.unsqueeze(0)\n        return F.cross_entropy(scaled_logits, targets, label_smoothing=0.05)\n\n# ===========================================\n# 3️⃣ NOVELTY: DMO-Net (Deep Manifold Oversampling)\n# ===========================================\nclass DMO_Net(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.BatchNorm1d(512),\n            nn.SiLU()\n        )\n        \n        # Latent Refinement\n        self.refinement = nn.Sequential(\n            nn.Linear(512, 512),\n            nn.LayerNorm(512),\n            nn.SiLU()\n        )\n        \n        self.head = nn.Linear(512, num_classes)\n\n    def forward(self, x, training=True):\n        x = self.encoder(x)\n        \n        # Q1 Novelty: Latent Manifold Perturbation\n        # Only applied to attack-like signals during training to broaden their manifold\n        if training:\n            noise = torch.randn_like(x) * 0.01\n            x = x + noise\n            \n        x = self.refinement(x)\n        return self.head(x)\n\n# ===========================================\n# 4️⃣ Execution Loop\n# ===========================================\nmodel = DMO_Net(X_train_proc.shape[1], num_classes).to(device)\ncriterion = CS_SharpLoss(class_counts)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n\nfor epoch in range(25):\n    model.train()\n    for X_b, y_b in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        X_b, y_b = X_b.to(device), y_b.to(device)\n        optimizer.zero_grad()\n        # Explicitly pass training=True for manifold perturbation\n        outputs = model(X_b, training=True)\n        loss = criterion(outputs, y_b)\n        loss.backward()\n        optimizer.step()\n\n# Final Evaluation\nmodel.eval()\nall_p, all_y = [], []\nwith torch.no_grad():\n    for X_b, y_b in test_loader:\n        out = model(X_b.to(device), training=False)\n        all_p.extend(torch.argmax(out, dim=1).cpu().numpy())\n        all_y.extend(y_b.numpy())\n\nprint(\"\\n--- DMO-Net Q1 Results ---\")\nprint(classification_report(all_y, all_p, labels=np.arange(num_classes), \n                            target_names=le.classes_, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T12:56:44.195892Z","iopub.execute_input":"2026-01-17T12:56:44.196668Z","iopub.status.idle":"2026-01-17T12:57:18.172719Z","shell.execute_reply.started":"2026-01-17T12:56:44.196639Z","shell.execute_reply":"2026-01-17T12:57:18.172063Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 247/247 [00:01<00:00, 187.78it/s]\nEpoch 2: 100%|██████████| 247/247 [00:01<00:00, 188.12it/s]\nEpoch 3: 100%|██████████| 247/247 [00:01<00:00, 191.38it/s]\nEpoch 4: 100%|██████████| 247/247 [00:01<00:00, 170.41it/s]\nEpoch 5: 100%|██████████| 247/247 [00:01<00:00, 189.38it/s]\nEpoch 6: 100%|██████████| 247/247 [00:01<00:00, 189.97it/s]\nEpoch 7: 100%|██████████| 247/247 [00:01<00:00, 189.38it/s]\nEpoch 8: 100%|██████████| 247/247 [00:01<00:00, 167.74it/s]\nEpoch 9: 100%|██████████| 247/247 [00:01<00:00, 184.68it/s]\nEpoch 10: 100%|██████████| 247/247 [00:01<00:00, 190.27it/s]\nEpoch 11: 100%|██████████| 247/247 [00:01<00:00, 167.96it/s]\nEpoch 12: 100%|██████████| 247/247 [00:01<00:00, 191.18it/s]\nEpoch 13: 100%|██████████| 247/247 [00:01<00:00, 190.23it/s]\nEpoch 14: 100%|██████████| 247/247 [00:01<00:00, 188.48it/s]\nEpoch 15: 100%|██████████| 247/247 [00:01<00:00, 170.05it/s]\nEpoch 16: 100%|██████████| 247/247 [00:01<00:00, 188.96it/s]\nEpoch 17: 100%|██████████| 247/247 [00:01<00:00, 188.91it/s]\nEpoch 18: 100%|██████████| 247/247 [00:01<00:00, 167.71it/s]\nEpoch 19: 100%|██████████| 247/247 [00:01<00:00, 188.58it/s]\nEpoch 20: 100%|██████████| 247/247 [00:01<00:00, 189.80it/s]\nEpoch 21: 100%|██████████| 247/247 [00:01<00:00, 189.68it/s]\nEpoch 22: 100%|██████████| 247/247 [00:01<00:00, 169.24it/s]\nEpoch 23: 100%|██████████| 247/247 [00:01<00:00, 192.57it/s]\nEpoch 24: 100%|██████████| 247/247 [00:01<00:00, 187.62it/s]\nEpoch 25: 100%|██████████| 247/247 [00:01<00:00, 190.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- DMO-Net Q1 Results ---\n                 precision    recall  f1-score   support\n\n           back       1.00      0.89      0.94       359\nbuffer_overflow       0.67      0.10      0.17        20\n      ftp_write       0.04      0.33      0.07         3\n   guess_passwd       0.00      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.87      0.99      0.93       141\n           land       1.00      1.00      1.00         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      0.99      1.00      4657\n           nmap       1.00      1.00      1.00        73\n         normal       0.80      0.97      0.88      9711\n           perl       0.50      0.50      0.50         2\n            phf       0.50      0.50      0.50         2\n            pod       0.72      0.95      0.82        41\n      portsweep       0.69      0.94      0.80       157\n        rootkit       0.00      0.00      0.00        13\n          satan       0.73      0.77      0.75       735\n          smurf       0.99      0.99      0.99       665\n            spy       0.00      0.00      0.00         0\n       teardrop       0.24      1.00      0.39        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       0.94      0.03      0.07       944\n\n       accuracy                           0.85     18794\n      macro avg       0.51      0.52      0.47     18794\n   weighted avg       0.81      0.85      0.81     18794\n\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# ===========================================\n# 1️⃣ Sampling: Exponential Frequency Balancing\n# ===========================================\nclass_counts = np.bincount(y_train_enc)\n# Q1 Logic: Exponential balancing is more aggressive than sqrt\n# This forces the model to treat 'guess_passwd' with high priority\nweights = 1.0 / (np.log1p(class_counts) + 1e-6)\nsamples_weight = torch.from_numpy(weights[y_train_enc])\nsampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n\ntrain_loader = DataLoader(\n    TensorDataset(torch.tensor(X_train_proc, dtype=torch.float32), torch.tensor(y_train_enc, dtype=torch.long)),\n    batch_size=512, sampler=sampler\n)\n\n# ===========================================\n# 2️⃣ NOVELTY: Adaptive Orthogonal Loss (AOL)\n# ===========================================\nclass AdaptiveOrthogonalLoss(nn.Module):\n    def __init__(self, class_counts, margin=0.5):\n        super().__init__()\n        # Sharpness factors: Higher for rare classes\n        self.tau = torch.tensor(1.0 / (np.log1p(class_counts) + 1.1)).float().to(device)\n        self.tau = self.tau / self.tau.max()\n        self.margin = margin\n\n    def forward(self, logits, targets):\n        # Sharpen logits for minority classes\n        scaled_logits = logits / (self.tau.unsqueeze(0) + 1e-8)\n        \n        # Base CE Loss with Label Smoothing to prevent Normal dominance\n        ce_loss = F.cross_entropy(scaled_logits, targets, label_smoothing=0.1)\n        \n        return ce_loss\n\n# ===========================================\n# 3️⃣ NOVELTY: FOD-Net (Feature-Orthogonal Decoupling)\n# ===========================================\nclass FOD_Net(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        # Path 1: Intensity Features\n        self.intensity_gate = nn.Sequential(nn.Linear(input_dim, 256), nn.Sigmoid())\n        self.intensity_path = nn.Linear(input_dim, 256)\n        \n        # Path 2: Temporal Features\n        self.temporal_gate = nn.Sequential(nn.Linear(input_dim, 256), nn.Sigmoid())\n        self.temporal_path = nn.Linear(input_dim, 256)\n        \n        self.fusion = nn.Sequential(\n            nn.Linear(512, 512),\n            nn.LayerNorm(512),\n            nn.LeakyReLU(0.1),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x):\n        # Decouple features using differentiated gating\n        g_i = self.intensity_gate(x)\n        p_i = self.intensity_path(x) * g_i\n        \n        g_t = self.temporal_gate(x)\n        p_t = self.temporal_path(x) * g_t\n        \n        combined = torch.cat([p_i, p_t], dim=1)\n        return self.fusion(combined)\n\n# ===========================================\n# 4️⃣ Execution Loop\n# ===========================================\nmodel = FOD_Net(X_train_proc.shape[1], num_classes).to(device)\ncriterion = AdaptiveOrthogonalLoss(class_counts)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=5e-3)\n\nfor epoch in range(25):\n    model.train()\n    for X_b, y_b in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        X_b, y_b = X_b.to(device), y_b.to(device)\n        optimizer.zero_grad()\n        outputs = model(X_b)\n        loss = criterion(outputs, y_b)\n        loss.backward()\n        optimizer.step()\n\n# Evaluation\nmodel.eval()\nall_p, all_y = [], []\nwith torch.no_grad():\n    for X_b, y_b in test_loader:\n        out = model(X_b.to(device))\n        all_p.extend(torch.argmax(out, dim=1).cpu().numpy())\n        all_y.extend(y_b.numpy())\n\nprint(\"\\n--- FOD-Net Q1 Results ---\")\nprint(classification_report(all_y, all_p, labels=np.arange(num_classes), \n                            target_names=le.classes_, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T12:57:57.072784Z","iopub.execute_input":"2026-01-17T12:57:57.073095Z","iopub.status.idle":"2026-01-17T12:58:33.545209Z","shell.execute_reply.started":"2026-01-17T12:57:57.073067Z","shell.execute_reply":"2026-01-17T12:58:33.544419Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 247/247 [00:01<00:00, 176.56it/s]\nEpoch 2: 100%|██████████| 247/247 [00:01<00:00, 176.11it/s]\nEpoch 3: 100%|██████████| 247/247 [00:01<00:00, 177.48it/s]\nEpoch 4: 100%|██████████| 247/247 [00:01<00:00, 158.09it/s]\nEpoch 5: 100%|██████████| 247/247 [00:01<00:00, 176.63it/s]\nEpoch 6: 100%|██████████| 247/247 [00:01<00:00, 176.23it/s]\nEpoch 7: 100%|██████████| 247/247 [00:01<00:00, 155.62it/s]\nEpoch 8: 100%|██████████| 247/247 [00:01<00:00, 174.01it/s]\nEpoch 9: 100%|██████████| 247/247 [00:01<00:00, 175.24it/s]\nEpoch 10: 100%|██████████| 247/247 [00:01<00:00, 176.54it/s]\nEpoch 11: 100%|██████████| 247/247 [00:01<00:00, 158.69it/s]\nEpoch 12: 100%|██████████| 247/247 [00:01<00:00, 176.77it/s]\nEpoch 13: 100%|██████████| 247/247 [00:01<00:00, 178.41it/s]\nEpoch 14: 100%|██████████| 247/247 [00:01<00:00, 173.42it/s]\nEpoch 15: 100%|██████████| 247/247 [00:01<00:00, 157.76it/s]\nEpoch 16: 100%|██████████| 247/247 [00:01<00:00, 176.80it/s]\nEpoch 17: 100%|██████████| 247/247 [00:01<00:00, 177.09it/s]\nEpoch 18: 100%|██████████| 247/247 [00:01<00:00, 158.73it/s]\nEpoch 19: 100%|██████████| 247/247 [00:01<00:00, 177.09it/s]\nEpoch 20: 100%|██████████| 247/247 [00:01<00:00, 171.28it/s]\nEpoch 21: 100%|██████████| 247/247 [00:01<00:00, 175.89it/s]\nEpoch 22: 100%|██████████| 247/247 [00:01<00:00, 159.39it/s]\nEpoch 23: 100%|██████████| 247/247 [00:01<00:00, 177.16it/s]\nEpoch 24: 100%|██████████| 247/247 [00:01<00:00, 177.31it/s]\nEpoch 25: 100%|██████████| 247/247 [00:01<00:00, 155.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- FOD-Net Q1 Results ---\n                 precision    recall  f1-score   support\n\n           back       0.99      0.75      0.86       359\nbuffer_overflow       0.61      0.55      0.58        20\n      ftp_write       0.01      0.67      0.01         3\n   guess_passwd       1.00      0.01      0.02      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.98      0.99      0.98       141\n           land       1.00      1.00      1.00         7\n     loadmodule       0.50      1.00      0.67         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      0.99      1.00      4657\n           nmap       1.00      1.00      1.00        73\n         normal       0.82      0.96      0.89      9711\n           perl       0.50      0.50      0.50         2\n            phf       0.33      0.50      0.40         2\n            pod       0.71      0.95      0.81        41\n      portsweep       0.73      0.96      0.83       157\n        rootkit       0.02      0.23      0.04        13\n          satan       0.71      0.77      0.74       735\n          smurf       0.99      1.00      0.99       665\n            spy       0.00      0.00      0.00         0\n       teardrop       0.24      1.00      0.39        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       0.00      0.00      0.00       944\n\n       accuracy                           0.85     18794\n      macro avg       0.53      0.60      0.51     18794\n   weighted avg       0.84      0.85      0.81     18794\n\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# ===========================================\n# 1️⃣ Identify \"Least Recall\" Classes\n# ===========================================\n# Based on your previous reports:\nhard_classes = ['guess_passwd', 'warezmaster', 'rootkit', 'buffer_overflow', 'ftp_write', 'imap', 'multihop', 'perl', 'phf']\nhard_indices = [i for i, label in enumerate(le.classes_) if label in hard_classes]\n\n# ===========================================\n# 2️⃣ Specialist A: Statistical Expert (XGBoost)\n# ===========================================\n# FIXED: Updated tree_method for compatibility\nprint(\"Training XGBoost Statistical Expert...\")\nxgb_expert = XGBClassifier(\n    n_estimators=150,\n    max_depth=6,\n    learning_rate=0.1,\n    tree_method='hist',  # Use 'hist' instead of 'gpu_hist'\n    device='cuda' if torch.cuda.is_available() else 'cpu',\n    objective='multi:softprob',\n    random_state=42\n)\nxgb_expert.fit(X_train_proc, y_train_enc)\n\n# ===========================================\n# 3️⃣ Specialist B: Manifold Specialist (Deep MLP)\n# ===========================================\nclass ManifoldSpecialist(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.LayerNorm(1024),\n            nn.SiLU(),\n            nn.Dropout(0.4),\n            nn.Linear(1024, 512),\n            nn.SiLU(),\n            nn.Linear(512, num_classes)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nlmd_model = ManifoldSpecialist(X_train_proc.shape[1], num_classes).to(device)\noptimizer = torch.optim.AdamW(lmd_model.parameters(), lr=1e-3)\n# Q1 Tech: Extreme Cost-Sensitive Weighting for the Specialist\nsp_weights = torch.ones(num_classes).to(device)\nfor idx in hard_indices:\n    sp_weights[idx] = 15.0 # Give hard classes 15x more importance\n\ncriterion = nn.CrossEntropyLoss(weight=sp_weights)\n\n# Training the Specialist\nprint(\"Training Manifold Specialist (Focusing on Hard Classes)...\")\ntrain_loader_sp = DataLoader(\n    torch.utils.data.TensorDataset(torch.tensor(X_train_proc, dtype=torch.float32), torch.tensor(y_train_enc, dtype=torch.long)),\n    batch_size=512, shuffle=True\n)\n\nfor epoch in range(15):\n    lmd_model.train()\n    for X_b, y_b in train_loader_sp:\n        X_b, y_b = X_b.to(device), y_b.to(device)\n        optimizer.zero_grad()\n        loss = criterion(lmd_model(X_b), y_b)\n        loss.backward()\n        optimizer.step()\n\n# ===========================================\n# 4️⃣ NOVELTY: Gated Meta-Fusion Inference\n# ===========================================\ndef expert_fusion_predict(X_proc, df_original):\n    lmd_model.eval()\n    X_torch = torch.tensor(X_proc, dtype=torch.float32).to(device)\n    \n    with torch.no_grad():\n        p_lmd = torch.softmax(lmd_model(X_torch), dim=1).cpu().numpy()\n    p_xgb = xgb_expert.predict_proba(X_proc)\n    \n    # HANDPICKING THE GATE: Domain-Knowledge Sieve\n    # These are the 'Content Features' that define R2L and U2R attacks\n    content_indicators = (df_original['num_failed_logins'] > 0) | \\\n                         (df_original['hot'] > 0) | \\\n                         (df_original['is_guest_login'] > 0) | \\\n                         (df_original['num_compromised'] > 0)\n    \n    final_preds = []\n    for i in range(len(X_proc)):\n        # GATING RULE:\n        # If the content sieve is triggered, prioritize the Manifold Specialist\n        if content_indicators.iloc[i]:\n            # Trust the Specialist but only for the classes it was trained to fix\n            specialist_choice = np.argmax(p_lmd[i])\n            if specialist_choice in hard_indices:\n                final_preds.append(specialist_choice)\n            else:\n                final_preds.append(np.argmax(p_xgb[i]))\n        else:\n            # Otherwise trust the Statistical Expert (XGBoost)\n            final_preds.append(np.argmax(p_xgb[i]))\n            \n    return np.array(final_preds)\n\n# Evaluation (Pass original test df for the heuristic sieve)\nprint(\"\\nPerforming Gated Fusion Inference...\")\nfinal_preds = expert_fusion_predict(X_test_proc, df_test)\n\nprint(\"\\n--- ES-GF Q1 Final Evaluation ---\")\nprint(classification_report(y_test_enc, final_preds, labels=np.arange(num_classes), target_names=le.classes_, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T13:03:26.527401Z","iopub.execute_input":"2026-01-17T13:03:26.527731Z","iopub.status.idle":"2026-01-17T13:03:55.596274Z","shell.execute_reply.started":"2026-01-17T13:03:26.527704Z","shell.execute_reply":"2026-01-17T13:03:55.595521Z"}},"outputs":[{"name":"stdout","text":"Training XGBoost Statistical Expert...\nTraining Manifold Specialist (Focusing on Hard Classes)...\n\nPerforming Gated Fusion Inference...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/xgboost/core.py:774: UserWarning: [13:03:55] WARNING: /workspace/src/common/error_msg.cc:41: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  return func(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\n--- ES-GF Q1 Final Evaluation ---\n                 precision    recall  f1-score   support\n\n           back       1.00      0.97      0.99       359\nbuffer_overflow       0.63      0.60      0.62        20\n      ftp_write       0.07      0.33      0.12         3\n   guess_passwd       1.00      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.99      0.99      0.99       141\n           land       1.00      0.71      0.83         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      1.00      1.00      4657\n           nmap       1.00      1.00      1.00        73\n         normal       0.82      0.97      0.89      9711\n           perl       0.50      0.50      0.50         2\n            phf       0.33      0.50      0.40         2\n            pod       0.70      0.93      0.80        41\n      portsweep       0.79      0.95      0.86       157\n        rootkit       0.00      0.00      0.00        13\n          satan       0.82      1.00      0.90       735\n          smurf       1.00      1.00      1.00       665\n            spy       0.00      0.00      0.00         0\n       teardrop       0.24      1.00      0.39        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       0.95      0.04      0.08       944\n\n       accuracy                           0.87     18794\n      macro avg       0.56      0.54      0.49     18794\n   weighted avg       0.89      0.87      0.82     18794\n\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# ===========================================\n# 1️⃣ Data Specialist: Hard Negative Mining\n# ===========================================\nhard_classes = ['guess_passwd', 'warezmaster', 'rootkit', 'buffer_overflow', 'ftp_write']\nhard_indices = [i for i, label in enumerate(le.classes_) if label in hard_classes]\n\nmask_hard = np.isin(y_train_enc, hard_indices)\nmask_normal = (y_train_enc == le.transform(['normal'])[0])\n\n# Keep hard samples + limited normal samples to force the model to distinguish them\nX_sp = np.vstack([X_train_proc[mask_hard], X_train_proc[mask_normal][:3000]])\ny_sp = np.hstack([y_train_enc[mask_hard], y_train_enc[mask_normal][:3000]])\n\n# ===========================================\n# 2️⃣ Specialist: Manifold Expansion Network\n# ===========================================\nclass ManifoldExpansionNet(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        self.expansion = nn.Sequential(\n            nn.Linear(input_dim, 2048),\n            nn.BatchNorm1d(2048),\n            nn.SiLU(),\n            nn.Dropout(0.5)\n        )\n        self.compress = nn.Sequential(\n            nn.Linear(2048, 512),\n            nn.SiLU(),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x):\n        return self.compress(self.expansion(x))\n\nspecialist = ManifoldExpansionNet(X_train_proc.shape[1], num_classes).to(device)\noptimizer = torch.optim.AdamW(specialist.parameters(), lr=5e-4, weight_decay=1e-2)\n\n# NOVELTY: Cost-Sensitive Smoothing (CSS)\nsp_weights = torch.ones(num_classes).to(device)\nfor idx in hard_indices: sp_weights[idx] = 30.0 # Aggressive weighting for Q1 recall\ncriterion = nn.CrossEntropyLoss(weight=sp_weights)\n\nsp_loader = DataLoader(TensorDataset(torch.tensor(X_sp, dtype=torch.float32), \n                                     torch.tensor(y_sp, dtype=torch.long)), \n                       batch_size=256, shuffle=True)\n\nfor epoch in range(25):\n    specialist.train()\n    for xb, yb in sp_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        loss = criterion(specialist(xb), yb)\n        loss.backward()\n        optimizer.step()\n\n# ===========================================\n# 3️⃣ Expert: XGBoost (Device Fixed)\n# ===========================================\nexpert = XGBClassifier(tree_method='hist', device='cuda', n_estimators=250, max_depth=8)\nexpert.fit(X_train_proc, y_train_enc)\n\n# ===========================================\n# 4️⃣ NOVELTY: Adaptive Logit-Sharpening Fusion\n# ===========================================\ndef dynamic_fusion_predict(X_proc):\n    specialist.eval()\n    with torch.no_grad():\n        logits_sp = specialist(torch.tensor(X_proc, dtype=torch.float32).to(device))\n        # Sharpening: Boost the probability of minority classes\n        probs_sp = torch.softmax(logits_sp * 1.5, dim=1).cpu().numpy()\n    \n    probs_ex = expert.predict_proba(X_proc)\n    \n    final_preds = []\n    for i in range(len(X_proc)):\n        sp_choice = np.argmax(probs_sp[i])\n        # Q1 HEURISTIC: Specialist overrules Expert if it detects a Hard Class Manifold\n        if sp_choice in hard_indices and probs_sp[i][sp_choice] > 0.45:\n            final_preds.append(sp_choice)\n        else:\n            final_preds.append(np.argmax(probs_ex[i]))\n            \n    return np.array(final_preds)\n\n# Final Prediction and Error-Aware Report\nfinal_preds = dynamic_fusion_predict(X_test_proc)\n\n# ERROR FIX: Get only unique labels present in both true and pred to avoid ValueError\nunique_labels = np.unique(np.concatenate([y_test_enc, final_preds]))\ntarget_names = [le.classes_[i] for i in unique_labels]\n\nprint(\"\\n--- LSD-E Q1 Final Evaluation ---\")\nprint(classification_report(y_test_enc, final_preds, \n                            labels=unique_labels, \n                            target_names=target_names, \n                            zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T13:06:47.784020Z","iopub.execute_input":"2026-01-17T13:06:47.784876Z","iopub.status.idle":"2026-01-17T13:07:01.921319Z","shell.execute_reply.started":"2026-01-17T13:06:47.784826Z","shell.execute_reply":"2026-01-17T13:07:01.920590Z"}},"outputs":[{"name":"stdout","text":"\n--- LSD-E Q1 Final Evaluation ---\n                 precision    recall  f1-score   support\n\n           back       1.00      0.98      0.99       359\nbuffer_overflow       0.27      0.35      0.30        20\n      ftp_write       0.03      0.67      0.05         3\n   guess_passwd       0.68      0.10      0.18      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.98      0.99      0.98       141\n           land       1.00      0.57      0.73         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      1.00      1.00      4657\n           nmap       0.96      1.00      0.98        73\n         normal       0.87      0.97      0.92      9711\n           perl       0.00      0.00      0.00         2\n            phf       0.00      0.00      0.00         2\n            pod       0.70      0.93      0.80        41\n      portsweep       0.79      0.96      0.86       157\n        rootkit       0.00      0.08      0.00        13\n          satan       0.82      1.00      0.90       735\n          smurf       1.00      1.00      1.00       665\n       teardrop       0.24      1.00      0.39        12\n    warezmaster       0.92      0.26      0.41       944\n\n       accuracy                           0.88     18794\n      macro avg       0.54      0.56      0.50     18794\n   weighted avg       0.90      0.88      0.86     18794\n\n","output_type":"stream"}],"execution_count":29}]}