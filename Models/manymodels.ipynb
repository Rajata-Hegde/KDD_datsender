{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14523807,"sourceType":"datasetVersion","datasetId":9276062},{"sourceId":394223,"sourceType":"datasetVersion","datasetId":174616}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-17T07:37:41.823671Z","iopub.execute_input":"2026-01-17T07:37:41.823908Z","iopub.status.idle":"2026-01-17T07:37:42.111038Z","shell.execute_reply.started":"2026-01-17T07:37:41.823886Z","shell.execute_reply":"2026-01-17T07:37:42.110321Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nsl-kdd-augmented/smote_augmented.csv\n/kaggle/input/nslkdd/KDDTest+.arff\n/kaggle/input/nslkdd/KDDTest-21.arff\n/kaggle/input/nslkdd/KDDTest1.jpg\n/kaggle/input/nslkdd/KDDTrain+.txt\n/kaggle/input/nslkdd/KDDTrain+_20Percent.txt\n/kaggle/input/nslkdd/KDDTest-21.txt\n/kaggle/input/nslkdd/KDDTest+.txt\n/kaggle/input/nslkdd/KDDTrain+.arff\n/kaggle/input/nslkdd/index.html\n/kaggle/input/nslkdd/KDDTrain+_20Percent.arff\n/kaggle/input/nslkdd/KDDTrain1.jpg\n/kaggle/input/nslkdd/nsl-kdd/KDDTest+.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTest-21.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTest1.jpg\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+_20Percent.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTest-21.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTest+.txt\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+.arff\n/kaggle/input/nslkdd/nsl-kdd/index.html\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain+_20Percent.arff\n/kaggle/input/nslkdd/nsl-kdd/KDDTrain1.jpg\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\n\n# ===========================================\n# 1️⃣ Load & Stable Mapping (From your working code)\n# ===========================================\nlocal_path = \"/kaggle/input/nslkdd/\" \ncolumns = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land',\n           'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n           'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n           'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count',\n           'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n           'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n           'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n           'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n           'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate',\n           'outcome', 'level']\n\ndf_train = pd.read_csv(local_path + \"KDDTrain+.txt\", header=None, names=columns)\ndf_test  = pd.read_csv(local_path + \"KDDTest+.txt\", header=None, names=columns)\n\ndf_train['outcome'] = df_train['outcome'].astype(str)\ndf_test['outcome'] = df_test['outcome'].astype(str)\ntrain_labels = set(df_train['outcome'].unique())\ndf_test = df_test[df_test['outcome'].isin(train_labels)].reset_index(drop=True)\n\n# ===========================================\n# 2️⃣ Hybrid Preprocessing for Transformer\n# ===========================================\ncat_cols = ['protocol_type', 'service', 'flag']\nnum_cols = [c for c in columns if c not in cat_cols + ['outcome', 'level']]\n\n# Label Encode Categorical for Embeddings\ncat_dims = []\nfor col in cat_cols:\n    le_cat = LabelEncoder()\n    df_train[col] = le_cat.fit_transform(df_train[col])\n    df_test[col] = df_test[col].map(lambda s: s if s in le_cat.classes_ else le_cat.classes_[0])\n    df_test[col] = le_cat.transform(df_test[col])\n    cat_dims.append(len(le_cat.classes_))\n\n# Scale Numerical\nscaler = StandardScaler()\nX_train_num = scaler.fit_transform(df_train[num_cols]).astype(np.float32)\nX_test_num  = scaler.transform(df_test[num_cols]).astype(np.float32)\n\nle = LabelEncoder()\ny_train_enc = le.fit_transform(df_train['outcome'])\ny_test_enc  = le.transform(df_test['outcome'])\nnum_classes = len(le.classes_)\n\n# ===========================================\n# 3️⃣ Novel Architecture: Gated-Transformer Fusion\n# ===========================================\nclass GatedLinearUnit(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.fc = nn.Linear(input_dim, input_dim * 2)\n    def forward(self, x):\n        x = self.fc(x)\n        x, gate = x.chunk(2, dim=-1)\n        return x * torch.sigmoid(gate)\n\nclass GTFModel(nn.Module):\n    def __init__(self, cat_dims, num_feat_dim, num_classes, emb_dim=32):\n        super().__init__()\n        # Categorical Path (Transformer)\n        self.embs = nn.ModuleList([nn.Embedding(d, emb_dim) for d in cat_dims])\n        layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=8, batch_first=True)\n        self.transformer = nn.TransformerEncoder(layer, num_layers=2)\n        \n        # Numerical Path (Gated)\n        self.num_gate = nn.Sequential(\n            nn.Linear(num_feat_dim, 128),\n            GatedLinearUnit(128),\n            nn.LayerNorm(128)\n        )\n        \n        # Fusion\n        self.classifier = nn.Sequential(\n            nn.Linear(len(cat_dims)*emb_dim + 128, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x_cat, x_num):\n        x_c = torch.stack([emb(x_cat[:, i]) for i, emb in enumerate(self.embs)], dim=1)\n        x_c = self.transformer(x_c).flatten(1)\n        x_n = self.num_gate(x_num)\n        return self.classifier(torch.cat([x_c, x_n], dim=1))\n\n# ===========================================\n# 4️⃣ Balanced Data Loading\n# ===========================================\nclass HybridDS(Dataset):\n    def __init__(self, c, n, y):\n        self.c, self.n, self.y = torch.tensor(c), torch.tensor(n), torch.tensor(y)\n    def __len__(self): return len(self.y)\n    def __getitem__(self, i): return self.c[i], self.n[i], self.y[i]\n\nclass_counts = np.bincount(y_train_enc)\nclass_weights = 1.0 / np.sqrt(class_counts + 1)\nsampler = WeightedRandomSampler(class_weights[y_train_enc], len(y_train_enc))\n\ntrain_loader = DataLoader(HybridDS(df_train[cat_cols].values, X_train_num, y_train_enc), batch_size=256, sampler=sampler)\ntest_loader  = DataLoader(HybridDS(df_test[cat_cols].values, X_test_num, y_test_enc), batch_size=256, shuffle=False)\n\n# ===========================================\n# 5️⃣ Training Loop\n# ===========================================\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = GTFModel(cat_dims, X_train_num.shape[1], num_classes).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n\n# Use standard weighted CE (proven stable in your code)\nloss_weights = torch.tensor(class_weights / class_weights.sum() * num_classes, dtype=torch.float32).to(device)\ncriterion = nn.CrossEntropyLoss(weight=loss_weights)\n\nfor epoch in range(15):\n    model.train()\n    for xc, xn, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        xc, xn, y = xc.to(device), xn.to(device), y.to(device)\n        optimizer.zero_grad()\n        loss = criterion(model(xc, xn), y)\n        loss.backward()\n        optimizer.step()\n\n# ===========================================\n# 6️⃣ Final Evaluation\n# ===========================================\nmodel.eval()\nall_p, all_y = [], []\nwith torch.no_grad():\n    for xc, xn, y in test_loader:\n        out = model(xc.to(device), xn.to(device))\n        all_p.extend(torch.argmax(out, 1).cpu().numpy())\n        all_y.extend(y.numpy())\n\nprint(\"\\n--- Final Q1 Results ---\")\nprint(classification_report(all_y, all_p, labels=np.arange(num_classes), target_names=le.classes_, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T08:20:04.800852Z","iopub.execute_input":"2026-01-17T08:20:04.801505Z","iopub.status.idle":"2026-01-17T08:21:08.362517Z","shell.execute_reply.started":"2026-01-17T08:20:04.801472Z","shell.execute_reply":"2026-01-17T08:21:08.361876Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 493/493 [00:04<00:00, 113.66it/s]\nEpoch 2: 100%|██████████| 493/493 [00:04<00:00, 121.91it/s]\nEpoch 3: 100%|██████████| 493/493 [00:04<00:00, 119.88it/s]\nEpoch 4: 100%|██████████| 493/493 [00:04<00:00, 115.13it/s]\nEpoch 5: 100%|██████████| 493/493 [00:04<00:00, 119.92it/s]\nEpoch 6: 100%|██████████| 493/493 [00:04<00:00, 116.88it/s]\nEpoch 7: 100%|██████████| 493/493 [00:04<00:00, 120.01it/s]\nEpoch 8: 100%|██████████| 493/493 [00:04<00:00, 119.99it/s]\nEpoch 9: 100%|██████████| 493/493 [00:04<00:00, 115.92it/s]\nEpoch 10: 100%|██████████| 493/493 [00:04<00:00, 118.75it/s]\nEpoch 11: 100%|██████████| 493/493 [00:04<00:00, 122.19it/s]\nEpoch 12: 100%|██████████| 493/493 [00:04<00:00, 116.25it/s]\nEpoch 13: 100%|██████████| 493/493 [00:04<00:00, 120.65it/s]\nEpoch 14: 100%|██████████| 493/493 [00:04<00:00, 122.44it/s]\nEpoch 15: 100%|██████████| 493/493 [00:04<00:00, 114.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Final Q1 Results ---\n                 precision    recall  f1-score   support\n\n           back       0.99      0.97      0.98       359\nbuffer_overflow       0.71      0.50      0.59        20\n      ftp_write       0.01      0.33      0.01         3\n   guess_passwd       1.00      0.18      0.31      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.86      0.98      0.91       141\n           land       1.00      1.00      1.00         7\n     loadmodule       0.29      1.00      0.44         2\n       multihop       0.01      0.11      0.02        18\n        neptune       1.00      0.99      1.00      4657\n           nmap       0.99      0.99      0.99        73\n         normal       0.86      0.92      0.89      9711\n           perl       0.50      0.50      0.50         2\n            phf       0.50      0.50      0.50         2\n            pod       0.72      0.95      0.82        41\n      portsweep       0.66      0.89      0.76       157\n        rootkit       0.00      0.08      0.01        13\n          satan       0.49      0.83      0.61       735\n          smurf       0.99      1.00      0.99       665\n            spy       0.00      0.00      0.00         0\n       teardrop       0.24      1.00      0.39        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       0.96      0.03      0.05       944\n\n       accuracy                           0.84     18794\n      macro avg       0.56      0.60      0.51     18794\n   weighted avg       0.90      0.84      0.83     18794\n\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\n# ===========================================\n# 1️⃣ Load & Stable Mapping (Retained from your working code)\n# ===========================================\nlocal_path = \"/kaggle/input/nslkdd/\" \n\ncolumns = [\n    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land',\n    'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n    'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n    'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count',\n    'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n    'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n    'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n    'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate',\n    'outcome', 'level'\n]\n\ndf_train = pd.read_csv(local_path + \"KDDTrain+.txt\", header=None, names=columns)\ndf_test  = pd.read_csv(local_path + \"KDDTest+.txt\", header=None, names=columns)\n\ndf_train['outcome'] = df_train['outcome'].astype(str)\ndf_test['outcome'] = df_test['outcome'].astype(str)\n\ntrain_labels = set(df_train['outcome'].unique())\ndf_test = df_test[df_test['outcome'].isin(train_labels)].reset_index(drop=True)\n\n# ===========================================\n# 2️⃣ Preprocessing (Stable Mapping)\n# ===========================================\ntarget = 'outcome'\ncat_cols = ['protocol_type', 'service', 'flag']\nnum_cols = [c for c in columns if c not in cat_cols + [target, 'level']]\n\npreprocessor = ColumnTransformer([\n    ('num', StandardScaler(), num_cols),\n    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n])\n\nX_train_proc = preprocessor.fit_transform(df_train.drop(columns=[target, 'level']))\nX_test_proc  = preprocessor.transform(df_test.drop(columns=[target, 'level']))\n\nle = LabelEncoder()\ny_train_enc = le.fit_transform(df_train[target])\ny_test_enc  = le.transform(df_test[target])\nnum_classes = len(le.classes_)\n\n# ===========================================\n# 3️⃣ Dataset & Weighted Sampling\n# ===========================================\nclass_counts = np.bincount(y_train_enc)\nclass_weights = 1.0 / np.sqrt(class_counts + 1)\nsample_weights = class_weights[y_train_enc]\n\nsampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(y_train_enc), replacement=True)\n\ntrain_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train_proc, dtype=torch.float32), \n                                               torch.tensor(y_train_enc, dtype=torch.long))\ntest_dataset  = torch.utils.data.TensorDataset(torch.tensor(X_test_proc, dtype=torch.float32), \n                                               torch.tensor(y_test_enc, dtype=torch.long))\n\ntrain_loader = DataLoader(train_dataset, batch_size=256, sampler=sampler)\ntest_loader  = DataLoader(test_dataset, batch_size=256, shuffle=False)\n\n# ===========================================\n# 4️⃣ NOVEL ARCHITECTURE: Gated Residual MLP\n# ===========================================\nclass GatedBlock(nn.Module):\n    def __init__(self, input_dim, output_dim, dropout=0.2):\n        super().__init__()\n        self.fc = nn.Linear(input_dim, output_dim * 2) # double for gating\n        self.ln = nn.LayerNorm(output_dim)\n        self.drop = nn.Dropout(dropout)\n\n    def forward(self, x):\n        gate_input = self.fc(x)\n        x, gate = gate_input.chunk(2, dim=-1)\n        x = x * torch.sigmoid(gate) # Gating mechanism\n        return self.drop(self.ln(x))\n\nclass Q1GatedModel(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        self.stem = nn.Linear(input_dim, 512)\n        \n        # Gated layers provide better feature refinement than simple ReLU\n        self.block1 = GatedBlock(512, 512)\n        self.block2 = GatedBlock(512, 256)\n        \n        self.head = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.stem(x))\n        x = self.block1(x)\n        x = self.block2(x)\n        return self.head(x)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = Q1GatedModel(X_train_proc.shape[1], num_classes).to(device)\n\n# Normalized weights for stability\nloss_weights = torch.tensor(class_weights / class_weights.sum() * num_classes, dtype=torch.float32).to(device)\ncriterion = nn.CrossEntropyLoss(weight=loss_weights)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n\n# ===========================================\n# 5️⃣ Training Loop\n# ===========================================\nEPOCHS = 20\nfor epoch in range(EPOCHS):\n    model.train()\n    running_loss = 0.0\n    for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(X_batch)\n        loss = criterion(outputs, y_batch)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    \n    print(f\"Epoch {epoch+1} Loss: {running_loss/len(train_loader):.4f}\")\n\n# ===========================================\n# 6️⃣ Evaluation\n# ===========================================\nmodel.eval()\nall_preds, all_labels = [], []\n\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        outputs = model(X_batch.to(device))\n        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n        all_preds.extend(preds)\n        all_labels.extend(y_batch.numpy())\n\nprint(\"\\n--- Evaluation Report (All Classes Retained) ---\")\nprint(classification_report(all_labels, all_preds, \n                            labels=np.arange(len(le.classes_)), \n                            target_names=le.classes_, \n                            zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T08:43:54.592928Z","iopub.execute_input":"2026-01-17T08:43:54.593421Z","iopub.status.idle":"2026-01-17T08:44:37.171750Z","shell.execute_reply.started":"2026-01-17T08:43:54.593348Z","shell.execute_reply":"2026-01-17T08:44:37.171095Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 493/493 [00:02<00:00, 222.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Loss: 0.2010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 493/493 [00:01<00:00, 247.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Loss: 0.0778\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 493/493 [00:01<00:00, 250.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Loss: 0.0591\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 493/493 [00:01<00:00, 247.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Loss: 0.0489\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 493/493 [00:02<00:00, 223.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Loss: 0.0422\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 493/493 [00:02<00:00, 240.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 Loss: 0.0365\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 493/493 [00:02<00:00, 241.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 Loss: 0.0366\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|██████████| 493/493 [00:02<00:00, 225.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 Loss: 0.0720\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|██████████| 493/493 [00:02<00:00, 237.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 Loss: 0.0352\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|██████████| 493/493 [00:02<00:00, 241.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 Loss: 0.0326\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11: 100%|██████████| 493/493 [00:02<00:00, 246.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 Loss: 0.0367\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12: 100%|██████████| 493/493 [00:02<00:00, 227.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 Loss: 0.0375\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13: 100%|██████████| 493/493 [00:02<00:00, 245.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 Loss: 0.0357\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14: 100%|██████████| 493/493 [00:02<00:00, 245.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 Loss: 0.0363\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15: 100%|██████████| 493/493 [00:02<00:00, 238.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15 Loss: 0.0319\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16: 100%|██████████| 493/493 [00:02<00:00, 228.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16 Loss: 0.0337\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17: 100%|██████████| 493/493 [00:02<00:00, 245.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17 Loss: 0.0537\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18: 100%|██████████| 493/493 [00:02<00:00, 239.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18 Loss: 0.0279\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19: 100%|██████████| 493/493 [00:02<00:00, 223.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19 Loss: 0.0358\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20: 100%|██████████| 493/493 [00:02<00:00, 240.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20 Loss: 0.0309\n\n--- Evaluation Report (All Classes Retained) ---\n                 precision    recall  f1-score   support\n\n           back       0.99      0.87      0.93       359\nbuffer_overflow       0.54      0.35      0.42        20\n      ftp_write       0.02      0.67      0.04         3\n   guess_passwd       1.00      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.72      0.97      0.83       141\n           land       1.00      1.00      1.00         7\n     loadmodule       0.00      0.00      0.00         2\n       multihop       0.01      0.11      0.02        18\n        neptune       1.00      1.00      1.00      4657\n           nmap       0.96      0.99      0.97        73\n         normal       0.83      0.96      0.89      9711\n           perl       1.00      0.50      0.67         2\n            phf       0.50      0.50      0.50         2\n            pod       0.71      0.95      0.81        41\n      portsweep       0.66      0.92      0.77       157\n        rootkit       0.01      0.15      0.02        13\n          satan       0.73      0.76      0.75       735\n          smurf       1.00      1.00      1.00       665\n            spy       0.00      0.00      0.00         0\n       teardrop       0.24      1.00      0.39        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       0.00      0.00      0.00       944\n\n       accuracy                           0.85     18794\n      macro avg       0.52      0.55      0.48     18794\n   weighted avg       0.84      0.85      0.81     18794\n\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\n\n# ===========================================\n# 1️⃣ Stable Pipeline: Data & Mapping\n# ===========================================\nlocal_path = \"/kaggle/input/nslkdd/\" \ncolumns = [\n    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land',\n    'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n    'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n    'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count',\n    'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n    'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n    'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n    'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate',\n    'outcome', 'level'\n]\n\ndf_train = pd.read_csv(local_path + \"KDDTrain+.txt\", header=None, names=columns)\ndf_test  = pd.read_csv(local_path + \"KDDTest+.txt\", header=None, names=columns)\n\ndf_train['outcome'] = df_train['outcome'].astype(str)\ndf_test['outcome'] = df_test['outcome'].astype(str)\ntrain_labels = set(df_train['outcome'].unique())\ndf_test = df_test[df_test['outcome'].isin(train_labels)].reset_index(drop=True)\n\n# Preprocessing\ntarget = 'outcome'\ncat_cols = ['protocol_type', 'service', 'flag']\nnum_cols = [c for c in columns if c not in cat_cols + [target, 'level']]\n\npreprocessor = ColumnTransformer([\n    ('num', StandardScaler(), num_cols),\n    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n])\n\nX_train_proc = preprocessor.fit_transform(df_train.drop(columns=[target, 'level']))\nX_test_proc  = preprocessor.transform(df_test.drop(columns=[target, 'level']))\n\nle = LabelEncoder()\ny_train_enc = le.fit_transform(df_train[target])\ny_test_enc  = le.transform(df_test[target])\nnum_classes = len(le.classes_)\n\n# ===========================================\n# 2️⃣ Sampler & Global Loss Adjustment\n# ===========================================\nclass_counts = np.bincount(y_train_enc)\n# Square root smoothing for stability\nclass_weights = 1.0 / np.sqrt(class_counts + 1)\nsample_weights = class_weights[y_train_enc]\n\n# NOVELTY: Cost-Sensitive adjustment term for Logits\n# This helps the model \"respect\" rare classes without mapping errors\nlogit_adj = torch.tensor(class_counts + 1).float().log() * 0.5 \n\nsampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(y_train_enc), replacement=True)\ntrain_loader = DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_train_proc, dtype=torch.float32), \n                                                        torch.tensor(y_train_enc, dtype=torch.long)), \n                          batch_size=512, sampler=sampler)\ntest_loader = DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test_proc, dtype=torch.float32), \n                                                       torch.tensor(y_test_enc, dtype=torch.long)), \n                         batch_size=512, shuffle=False)\n\n# ===========================================\n# 3️⃣ Architecture: GMH-ResNet\n# ===========================================\nclass GLULayer(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc = nn.Linear(dim, dim * 2)\n    def forward(self, x):\n        x = self.fc(x)\n        content, gate = x.chunk(2, dim=-1)\n        return content * torch.sigmoid(gate)\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.glu = GLULayer(dim)\n        self.ln = nn.LayerNorm(dim)\n    def forward(self, x):\n        return x + self.glu(self.ln(x))\n\nclass GMHResNet(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        self.stem = nn.Linear(input_dim, 512)\n        self.blocks = nn.Sequential(\n            ResidualBlock(512),\n            nn.Linear(512, 256),\n            ResidualBlock(256),\n            nn.Dropout(0.3)\n        )\n        self.head = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.stem(x))\n        x = self.blocks(x)\n        return self.head(x)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = GMHResNet(X_train_proc.shape[1], num_classes).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\nlogit_adj = logit_adj.to(device)\n\n# ===========================================\n# 4️⃣ Training & Evaluation\n# ===========================================\nfor epoch in range(20):\n    model.train()\n    for X_b, y_b in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        X_b, y_b = X_b.to(device), y_b.to(device)\n        optimizer.zero_grad()\n        logits = model(X_b)\n        # Apply logit adjustment for imbalanced detection\n        loss = nn.functional.cross_entropy(logits - logit_adj, y_b)\n        loss.backward()\n        optimizer.step()\n\nmodel.eval()\nall_p, all_y = [], []\nwith torch.no_grad():\n    for X_b, y_b in test_loader:\n        out = model(X_b.to(device))\n        all_p.extend(torch.argmax(out, dim=1).cpu().numpy())\n        all_y.extend(y_b.numpy())\n\nprint(classification_report(all_y, all_p, labels=np.arange(num_classes), target_names=le.classes_, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T08:45:55.955823Z","iopub.execute_input":"2026-01-17T08:45:55.956603Z","iopub.status.idle":"2026-01-17T08:46:25.808628Z","shell.execute_reply.started":"2026-01-17T08:45:55.956571Z","shell.execute_reply":"2026-01-17T08:46:25.807826Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 247/247 [00:01<00:00, 177.82it/s]\nEpoch 2: 100%|██████████| 247/247 [00:01<00:00, 177.22it/s]\nEpoch 3: 100%|██████████| 247/247 [00:01<00:00, 158.35it/s]\nEpoch 4: 100%|██████████| 247/247 [00:01<00:00, 177.70it/s]\nEpoch 5: 100%|██████████| 247/247 [00:01<00:00, 175.02it/s]\nEpoch 6: 100%|██████████| 247/247 [00:01<00:00, 159.69it/s]\nEpoch 7: 100%|██████████| 247/247 [00:01<00:00, 174.24it/s]\nEpoch 8: 100%|██████████| 247/247 [00:01<00:00, 177.01it/s]\nEpoch 9: 100%|██████████| 247/247 [00:01<00:00, 175.82it/s]\nEpoch 10: 100%|██████████| 247/247 [00:01<00:00, 156.25it/s]\nEpoch 11: 100%|██████████| 247/247 [00:01<00:00, 176.74it/s]\nEpoch 12: 100%|██████████| 247/247 [00:01<00:00, 175.85it/s]\nEpoch 13: 100%|██████████| 247/247 [00:01<00:00, 176.79it/s]\nEpoch 14: 100%|██████████| 247/247 [00:01<00:00, 153.40it/s]\nEpoch 15: 100%|██████████| 247/247 [00:01<00:00, 178.41it/s]\nEpoch 16: 100%|██████████| 247/247 [00:01<00:00, 178.28it/s]\nEpoch 17: 100%|██████████| 247/247 [00:01<00:00, 178.01it/s]\nEpoch 18: 100%|██████████| 247/247 [00:01<00:00, 158.61it/s]\nEpoch 19: 100%|██████████| 247/247 [00:01<00:00, 177.95it/s]\nEpoch 20: 100%|██████████| 247/247 [00:01<00:00, 175.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"                 precision    recall  f1-score   support\n\n           back       1.00      0.91      0.95       359\nbuffer_overflow       0.50      0.05      0.09        20\n      ftp_write       0.00      0.00      0.00         3\n   guess_passwd       0.00      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.93      0.99      0.96       141\n           land       1.00      1.00      1.00         7\n     loadmodule       1.00      1.00      1.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       0.99      1.00      1.00      4657\n           nmap       1.00      0.99      0.99        73\n         normal       0.83      0.98      0.90      9711\n           perl       0.50      0.50      0.50         2\n            phf       0.50      0.50      0.50         2\n            pod       0.75      0.93      0.83        41\n      portsweep       0.48      0.88      0.62       157\n        rootkit       0.00      0.00      0.00        13\n          satan       0.62      0.66      0.64       735\n          smurf       0.99      1.00      0.99       665\n            spy       0.00      0.00      0.00         0\n       teardrop       0.24      1.00      0.39        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       0.96      0.05      0.10       944\n\n       accuracy                           0.85     18794\n      macro avg       0.53      0.54      0.50     18794\n   weighted avg       0.82      0.85      0.81     18794\n\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\n\n# ===========================================\n# 1️⃣ Stable Pipeline: Data & Mapping\n# ===========================================\nlocal_path = \"/kaggle/input/nslkdd/\" \ncolumns = [\n    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land',\n    'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n    'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n    'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count',\n    'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n    'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n    'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n    'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate',\n    'outcome', 'level'\n]\n\ndf_train = pd.read_csv(local_path + \"KDDTrain+.txt\", header=None, names=columns)\ndf_test  = pd.read_csv(local_path + \"KDDTest+.txt\", header=None, names=columns)\n\ndf_train['outcome'] = df_train['outcome'].astype(str)\ndf_test['outcome'] = df_test['outcome'].astype(str)\ntrain_labels = set(df_train['outcome'].unique())\ndf_test = df_test[df_test['outcome'].isin(train_labels)].reset_index(drop=True)\n\n# Preprocessing\ntarget = 'outcome'\ncat_cols = ['protocol_type', 'service', 'flag']\nnum_cols = [c for c in columns if c not in cat_cols + [target, 'level']]\n\npreprocessor = ColumnTransformer([\n    ('num', StandardScaler(), num_cols),\n    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n])\n\nX_train_proc = preprocessor.fit_transform(df_train.drop(columns=[target, 'level']))\nX_test_proc  = preprocessor.transform(df_test.drop(columns=[target, 'level']))\n\nle = LabelEncoder()\ny_train_enc = le.fit_transform(df_train[target])\ny_test_enc  = le.transform(df_test[target])\nnum_classes = len(le.classes_)\n\n# ===========================================\n# 2️⃣ Sampler & Global Loss Adjustment\n# ===========================================\nclass_counts = np.bincount(y_train_enc)\n# Square root smoothing for stability\nclass_weights = 1.0 / np.sqrt(class_counts + 1)\nsample_weights = class_weights[y_train_enc]\n\n# NOVELTY: Cost-Sensitive adjustment term for Logits\n# logit_adj = log(prior_probability). This helps correct class imbalance at the boundary.\nlogit_adj = torch.tensor(class_counts + 1).float().log() * 0.5 \n\nsampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(y_train_enc), replacement=True)\ntrain_loader = DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_train_proc, dtype=torch.float32), \n                                                        torch.tensor(y_train_enc, dtype=torch.long)), \n                          batch_size=512, sampler=sampler)\ntest_loader = DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test_proc, dtype=torch.float32), \n                                                       torch.tensor(y_test_enc, dtype=torch.long)), \n                         batch_size=512, shuffle=False)\n\n# ===========================================\n# 3️⃣ Architecture: GMH-ResNet\n# ===========================================\nclass GLULayer(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc = nn.Linear(dim, dim * 2)\n    def forward(self, x):\n        x = self.fc(x)\n        content, gate = x.chunk(2, dim=-1)\n        return content * torch.sigmoid(gate)\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.glu = GLULayer(dim)\n        self.ln = nn.LayerNorm(dim)\n    def forward(self, x):\n        return x + self.glu(self.ln(x))\n\nclass GMHResNet(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        self.stem = nn.Linear(input_dim, 512)\n        self.blocks = nn.Sequential(\n            ResidualBlock(512),\n            nn.Linear(512, 256),\n            ResidualBlock(256),\n            nn.Dropout(0.3)\n        )\n        self.head = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.stem(x))\n        x = self.blocks(x)\n        return self.head(x)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = GMHResNet(X_train_proc.shape[1], num_classes).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\nlogit_adj = logit_adj.to(device)\n\n# ===========================================\n# 4️⃣ Training & Evaluation\n# ===========================================\nfor epoch in range(20):\n    model.train()\n    for X_b, y_b in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        X_b, y_b = X_b.to(device), y_b.to(device)\n        optimizer.zero_grad()\n        logits = model(X_b)\n        # Apply logit adjustment novelty: correct the bias toward majority classes\n        loss = nn.functional.cross_entropy(logits - logit_adj, y_b)\n        loss.backward()\n        optimizer.step()\n\nmodel.eval()\nall_p, all_y = [], []\nwith torch.no_grad():\n    for X_b, y_b in test_loader:\n        out = model(X_b.to(device))\n        all_p.extend(torch.argmax(out, dim=1).cpu().numpy())\n        all_y.extend(y_b.numpy())\n\nprint(classification_report(all_y, all_p, labels=np.arange(num_classes), target_names=le.classes_, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T08:47:20.254294Z","iopub.execute_input":"2026-01-17T08:47:20.254641Z","iopub.status.idle":"2026-01-17T08:47:51.109386Z","shell.execute_reply.started":"2026-01-17T08:47:20.254609Z","shell.execute_reply":"2026-01-17T08:47:51.108518Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 247/247 [00:01<00:00, 154.98it/s]\nEpoch 2: 100%|██████████| 247/247 [00:01<00:00, 178.74it/s]\nEpoch 3: 100%|██████████| 247/247 [00:01<00:00, 160.57it/s]\nEpoch 4: 100%|██████████| 247/247 [00:01<00:00, 174.56it/s]\nEpoch 5: 100%|██████████| 247/247 [00:01<00:00, 160.10it/s]\nEpoch 6: 100%|██████████| 247/247 [00:01<00:00, 154.98it/s]\nEpoch 7: 100%|██████████| 247/247 [00:01<00:00, 176.63it/s]\nEpoch 8: 100%|██████████| 247/247 [00:01<00:00, 161.57it/s]\nEpoch 9: 100%|██████████| 247/247 [00:01<00:00, 175.05it/s]\nEpoch 10: 100%|██████████| 247/247 [00:01<00:00, 155.31it/s]\nEpoch 11: 100%|██████████| 247/247 [00:01<00:00, 176.19it/s]\nEpoch 12: 100%|██████████| 247/247 [00:01<00:00, 156.84it/s]\nEpoch 13: 100%|██████████| 247/247 [00:01<00:00, 177.85it/s]\nEpoch 14: 100%|██████████| 247/247 [00:01<00:00, 159.92it/s]\nEpoch 15: 100%|██████████| 247/247 [00:01<00:00, 177.24it/s]\nEpoch 16: 100%|██████████| 247/247 [00:01<00:00, 156.90it/s]\nEpoch 17: 100%|██████████| 247/247 [00:01<00:00, 172.97it/s]\nEpoch 18: 100%|██████████| 247/247 [00:01<00:00, 157.71it/s]\nEpoch 19: 100%|██████████| 247/247 [00:01<00:00, 175.52it/s]\nEpoch 20: 100%|██████████| 247/247 [00:01<00:00, 157.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"                 precision    recall  f1-score   support\n\n           back       1.00      0.94      0.97       359\nbuffer_overflow       0.86      0.30      0.44        20\n      ftp_write       0.01      0.33      0.02         3\n   guess_passwd       0.00      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.89      0.98      0.93       141\n           land       1.00      1.00      1.00         7\n     loadmodule       1.00      1.00      1.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      0.99      1.00      4657\n           nmap       1.00      0.99      0.99        73\n         normal       0.81      0.97      0.88      9711\n           perl       0.50      0.50      0.50         2\n            phf       0.33      0.50      0.40         2\n            pod       0.75      0.93      0.83        41\n      portsweep       0.61      0.94      0.74       157\n        rootkit       0.00      0.00      0.00        13\n          satan       0.74      0.75      0.74       735\n          smurf       0.99      1.00      0.99       665\n            spy       0.00      0.00      0.00         0\n       teardrop       0.23      1.00      0.38        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       0.92      0.02      0.05       944\n\n       accuracy                           0.85     18794\n      macro avg       0.55      0.57      0.52     18794\n   weighted avg       0.82      0.85      0.81     18794\n\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\n\n# ===========================================\n# 1️⃣ Stable Data Pipeline\n# ===========================================\nlocal_path = \"/kaggle/input/nslkdd/\" \ncolumns = [\n    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land',\n    'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n    'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n    'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count',\n    'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n    'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n    'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n    'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate',\n    'outcome', 'level'\n]\n\ndf_train = pd.read_csv(local_path + \"KDDTrain+.txt\", header=None, names=columns)\ndf_test  = pd.read_csv(local_path + \"KDDTest+.txt\", header=None, names=columns)\n\ndf_train['outcome'] = df_train['outcome'].astype(str)\ndf_test['outcome'] = df_test['outcome'].astype(str)\ntrain_labels = set(df_train['outcome'].unique())\ndf_test = df_test[df_test['outcome'].isin(train_labels)].reset_index(drop=True)\n\ntarget = 'outcome'\ncat_cols = ['protocol_type', 'service', 'flag']\nnum_cols = [c for c in columns if c not in cat_cols + [target, 'level']]\n\npreprocessor = ColumnTransformer([\n    ('num', StandardScaler(), num_cols),\n    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n])\n\nX_train_proc = preprocessor.fit_transform(df_train.drop(columns=[target, 'level']))\nX_test_proc  = preprocessor.transform(df_test.drop(columns=[target, 'level']))\n\nle = LabelEncoder()\ny_train_enc = le.fit_transform(df_train[target])\ny_test_enc  = le.transform(df_test[target])\nnum_classes = len(le.classes_)\n\n# ===========================================\n# 2️⃣ Sampler & Logit Adjustment Setup\n# ===========================================\nclass_counts = np.bincount(y_train_enc)\n# Logit adjustment: log(class_frequency). Helps adjust the decision boundary.\nlogit_adj = torch.tensor(class_counts + 1).float().log().to('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass_weights = 1.0 / np.sqrt(class_counts + 1)\nsampler = WeightedRandomSampler(weights=class_weights[y_train_enc], num_samples=len(y_train_enc), replacement=True)\n\ntrain_loader = DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_train_proc, dtype=torch.float32), \n                                                        torch.tensor(y_train_enc, dtype=torch.long)), \n                          batch_size=512, sampler=sampler)\ntest_loader = DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test_proc, dtype=torch.float32), \n                                                       torch.tensor(y_test_enc, dtype=torch.long)), \n                         batch_size=512, shuffle=False)\n\n# ===========================================\n# 3️⃣ Architecture: Gated Residual Net\n# ===========================================\nclass GatedBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc = nn.Linear(dim, dim * 2)\n        self.ln = nn.LayerNorm(dim)\n\n    def forward(self, x):\n        res = x\n        x = self.fc(x)\n        x, gate = x.chunk(2, dim=-1)\n        x = x * torch.sigmoid(gate)\n        return self.ln(x + res)\n\nclass GatedResNet(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        self.stem = nn.Linear(input_dim, 512)\n        self.blocks = nn.Sequential(\n            GatedBlock(512),\n            nn.Linear(512, 256),\n            GatedBlock(256),\n            nn.Dropout(0.3)\n        )\n        self.head = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.stem(x))\n        x = self.blocks(x)\n        return self.head(x)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = GatedResNet(X_train_proc.shape[1], num_classes).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n\n# ===========================================\n# 4️⃣ Training with Logit Adjustment\n# ===========================================\nfor epoch in range(20):\n    model.train()\n    for X_b, y_b in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        X_b, y_b = X_b.to(device), y_b.to(device)\n        optimizer.zero_grad()\n        logits = model(X_b)\n        # Apply logit adjustment during loss to favor minority classes\n        loss = nn.functional.cross_entropy(logits - 0.5 * logit_adj, y_b)\n        loss.backward()\n        optimizer.step()\n\n# ===========================================\n# 5️⃣ Final Evaluation\n# ===========================================\nmodel.eval()\nall_p, all_y = [], []\nwith torch.no_grad():\n    for X_b, y_b in test_loader:\n        out = model(X_b.to(device))\n        all_p.extend(torch.argmax(out, dim=1).cpu().numpy())\n        all_y.extend(y_b.numpy())\n\nprint(classification_report(all_y, all_p, labels=np.arange(num_classes), target_names=le.classes_, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T08:49:47.629697Z","iopub.execute_input":"2026-01-17T08:49:47.630289Z","iopub.status.idle":"2026-01-17T08:50:18.380305Z","shell.execute_reply.started":"2026-01-17T08:49:47.630258Z","shell.execute_reply":"2026-01-17T08:50:18.379480Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 247/247 [00:01<00:00, 156.08it/s]\nEpoch 2: 100%|██████████| 247/247 [00:01<00:00, 175.84it/s]\nEpoch 3: 100%|██████████| 247/247 [00:01<00:00, 156.02it/s]\nEpoch 4: 100%|██████████| 247/247 [00:01<00:00, 174.84it/s]\nEpoch 5: 100%|██████████| 247/247 [00:01<00:00, 154.22it/s]\nEpoch 6: 100%|██████████| 247/247 [00:01<00:00, 156.84it/s]\nEpoch 7: 100%|██████████| 247/247 [00:01<00:00, 174.81it/s]\nEpoch 8: 100%|██████████| 247/247 [00:01<00:00, 158.87it/s]\nEpoch 9: 100%|██████████| 247/247 [00:01<00:00, 176.45it/s]\nEpoch 10: 100%|██████████| 247/247 [00:01<00:00, 159.97it/s]\nEpoch 11: 100%|██████████| 247/247 [00:01<00:00, 176.81it/s]\nEpoch 12: 100%|██████████| 247/247 [00:01<00:00, 154.73it/s]\nEpoch 13: 100%|██████████| 247/247 [00:01<00:00, 175.57it/s]\nEpoch 14: 100%|██████████| 247/247 [00:01<00:00, 159.22it/s]\nEpoch 15: 100%|██████████| 247/247 [00:01<00:00, 178.40it/s]\nEpoch 16: 100%|██████████| 247/247 [00:01<00:00, 158.88it/s]\nEpoch 17: 100%|██████████| 247/247 [00:01<00:00, 177.73it/s]\nEpoch 18: 100%|██████████| 247/247 [00:01<00:00, 179.54it/s]\nEpoch 19: 100%|██████████| 247/247 [00:01<00:00, 172.87it/s]\nEpoch 20: 100%|██████████| 247/247 [00:01<00:00, 157.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"                 precision    recall  f1-score   support\n\n           back       0.99      0.90      0.94       359\nbuffer_overflow       0.86      0.30      0.44        20\n      ftp_write       0.05      0.67      0.10         3\n   guess_passwd       0.00      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.78      0.99      0.87       141\n           land       0.00      0.00      0.00         7\n     loadmodule       1.00      1.00      1.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      1.00      1.00      4657\n           nmap       1.00      0.99      0.99        73\n         normal       0.82      0.97      0.89      9711\n           perl       0.50      0.50      0.50         2\n            phf       0.50      0.50      0.50         2\n            pod       0.72      0.95      0.82        41\n      portsweep       0.76      0.90      0.82       157\n        rootkit       0.00      0.00      0.00        13\n          satan       0.77      0.87      0.81       735\n          smurf       0.99      1.00      1.00       665\n            spy       0.00      0.00      0.00         0\n       teardrop       0.24      1.00      0.39        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       0.94      0.03      0.06       944\n\n       accuracy                           0.86     18794\n      macro avg       0.52      0.55      0.48     18794\n   weighted avg       0.82      0.86      0.82     18794\n\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\n\n# ===========================================\n# 1️⃣ Stable Data Pipeline (Fixed Mapping)\n# ===========================================\nlocal_path = \"/kaggle/input/nslkdd/\" \ncolumns = [\n    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land',\n    'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n    'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n    'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count',\n    'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n    'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n    'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n    'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate',\n    'outcome', 'level'\n]\n\ndf_train = pd.read_csv(local_path + \"KDDTrain+.txt\", header=None, names=columns)\ndf_test  = pd.read_csv(local_path + \"KDDTest+.txt\", header=None, names=columns)\n\ndf_train['outcome'] = df_train['outcome'].astype(str)\ndf_test['outcome'] = df_test['outcome'].astype(str)\ntrain_labels = set(df_train['outcome'].unique())\ndf_test = df_test[df_test['outcome'].isin(train_labels)].reset_index(drop=True)\n\ntarget = 'outcome'\ncat_cols = ['protocol_type', 'service', 'flag']\nnum_cols = [c for c in columns if c not in cat_cols + [target, 'level']]\n\npreprocessor = ColumnTransformer([\n    ('num', StandardScaler(), num_cols),\n    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n])\n\nX_train_proc = preprocessor.fit_transform(df_train.drop(columns=[target, 'level']))\nX_test_proc  = preprocessor.transform(df_test.drop(columns=[target, 'level']))\n\nle = LabelEncoder()\ny_train_enc = le.fit_transform(df_train[target])\ny_test_enc  = le.transform(df_test[target])\nnum_classes = len(le.classes_)\n\n# ===========================================\n# 2️⃣ Logit Adjustment & Sampler Setup\n# ===========================================\nclass_counts = np.bincount(y_train_enc)\n# Novelty: Logit Adjustment (LA) term - shifts decision boundary for minority classes\nlogit_adj = torch.tensor(class_counts + 1).float().log().to('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass_weights = 1.0 / np.sqrt(class_counts + 1)\nsampler = WeightedRandomSampler(weights=class_weights[y_train_enc], num_samples=len(y_train_enc), replacement=True)\n\ntrain_loader = DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_train_proc, dtype=torch.float32), \n                                                        torch.tensor(y_train_enc, dtype=torch.long)), \n                          batch_size=512, sampler=sampler)\ntest_loader = DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test_proc, dtype=torch.float32), \n                                                       torch.tensor(y_test_enc, dtype=torch.long)), \n                         batch_size=512, shuffle=False)\n\n# ===========================================\n# 3️⃣ Architecture: Gated Multi-Head Residual Net\n# ===========================================\nclass GatedBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc = nn.Linear(dim, dim * 2)\n        self.ln = nn.LayerNorm(dim)\n\n    def forward(self, x):\n        res = x\n        x = self.fc(x)\n        content, gate = x.chunk(2, dim=-1)\n        x = content * torch.sigmoid(gate) # GLU Gating\n        return self.ln(x + res) # Residual Connection\n\nclass GMHResNet(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        self.stem = nn.Linear(input_dim, 512)\n        self.blocks = nn.Sequential(\n            GatedBlock(512),\n            nn.Linear(512, 256),\n            GatedBlock(256),\n            nn.Dropout(0.3)\n        )\n        self.head = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.stem(x))\n        x = self.blocks(x)\n        return self.head(x)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = GMHResNet(X_train_proc.shape[1], num_classes).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n\n# ===========================================\n# 4️⃣ Training with Logit Adjustment\n# ===========================================\nfor epoch in range(20):\n    model.train()\n    for X_b, y_b in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        X_b, y_b = X_b.to(device), y_b.to(device)\n        optimizer.zero_grad()\n        logits = model(X_b)\n        \n        # Novelty: Applying logit adjustment in the loss to favor rare classes\n        loss = nn.functional.cross_entropy(logits - 0.5 * logit_adj, y_b)\n        \n        loss.backward()\n        optimizer.step()\n\n# ===========================================\n# 5️⃣ Evaluation\n# ===========================================\nmodel.eval()\nall_p, all_y = [], []\nwith torch.no_grad():\n    for X_b, y_b in test_loader:\n        out = model(X_b.to(device))\n        all_p.extend(torch.argmax(out, dim=1).cpu().numpy())\n        all_y.extend(y_b.numpy())\n\nprint(classification_report(all_y, all_p, labels=np.arange(num_classes), target_names=le.classes_, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T08:50:54.271924Z","iopub.execute_input":"2026-01-17T08:50:54.272251Z","iopub.status.idle":"2026-01-17T08:51:25.064528Z","shell.execute_reply.started":"2026-01-17T08:50:54.272222Z","shell.execute_reply":"2026-01-17T08:51:25.063888Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 247/247 [00:01<00:00, 171.57it/s]\nEpoch 2: 100%|██████████| 247/247 [00:01<00:00, 157.15it/s]\nEpoch 3: 100%|██████████| 247/247 [00:01<00:00, 178.38it/s]\nEpoch 4: 100%|██████████| 247/247 [00:01<00:00, 158.70it/s]\nEpoch 5: 100%|██████████| 247/247 [00:01<00:00, 178.12it/s]\nEpoch 6: 100%|██████████| 247/247 [00:01<00:00, 158.17it/s]\nEpoch 7: 100%|██████████| 247/247 [00:01<00:00, 159.23it/s]\nEpoch 8: 100%|██████████| 247/247 [00:01<00:00, 173.63it/s]\nEpoch 9: 100%|██████████| 247/247 [00:01<00:00, 159.86it/s]\nEpoch 10: 100%|██████████| 247/247 [00:01<00:00, 176.42it/s]\nEpoch 11: 100%|██████████| 247/247 [00:01<00:00, 159.17it/s]\nEpoch 12: 100%|██████████| 247/247 [00:01<00:00, 178.02it/s]\nEpoch 13: 100%|██████████| 247/247 [00:01<00:00, 158.61it/s]\nEpoch 14: 100%|██████████| 247/247 [00:01<00:00, 178.35it/s]\nEpoch 15: 100%|██████████| 247/247 [00:01<00:00, 155.84it/s]\nEpoch 16: 100%|██████████| 247/247 [00:01<00:00, 173.40it/s]\nEpoch 17: 100%|██████████| 247/247 [00:01<00:00, 158.82it/s]\nEpoch 18: 100%|██████████| 247/247 [00:01<00:00, 175.92it/s]\nEpoch 19: 100%|██████████| 247/247 [00:01<00:00, 158.93it/s]\nEpoch 20: 100%|██████████| 247/247 [00:01<00:00, 177.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"                 precision    recall  f1-score   support\n\n           back       1.00      0.74      0.85       359\nbuffer_overflow       0.00      0.00      0.00        20\n      ftp_write       0.04      0.33      0.07         3\n   guess_passwd       1.00      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.98      0.98      0.98       141\n           land       1.00      1.00      1.00         7\n     loadmodule       1.00      1.00      1.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       1.00      0.99      0.99      4657\n           nmap       1.00      1.00      1.00        73\n         normal       0.79      0.98      0.87      9711\n           perl       0.50      0.50      0.50         2\n            phf       0.50      0.50      0.50         2\n            pod       0.72      0.95      0.82        41\n      portsweep       0.74      0.89      0.81       157\n        rootkit       0.00      0.00      0.00        13\n          satan       0.79      0.73      0.76       735\n          smurf       0.99      1.00      0.99       665\n            spy       0.00      0.00      0.00         0\n       teardrop       0.24      1.00      0.39        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       0.50      0.00      0.00       944\n\n       accuracy                           0.85     18794\n      macro avg       0.56      0.55      0.50     18794\n   weighted avg       0.85      0.85      0.80     18794\n\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\n\n# ===========================================\n# 1️⃣ Stable Data Pipeline\n# ===========================================\nlocal_path = \"/kaggle/input/nslkdd/\" \ncolumns = [\n    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land',\n    'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n    'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n    'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count',\n    'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n    'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n    'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n    'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate',\n    'outcome', 'level'\n]\n\ndf_train = pd.read_csv(local_path + \"KDDTrain+.txt\", header=None, names=columns)\ndf_test  = pd.read_csv(local_path + \"KDDTest+.txt\", header=None, names=columns)\n\ndf_train['outcome'] = df_train['outcome'].astype(str)\ndf_test['outcome'] = df_test['outcome'].astype(str)\ntrain_labels = set(df_train['outcome'].unique())\ndf_test = df_test[df_test['outcome'].isin(train_labels)].reset_index(drop=True)\n\ntarget = 'outcome'\ncat_cols = ['protocol_type', 'service', 'flag']\nnum_cols = [c for c in columns if c not in cat_cols + [target, 'level']]\n\npreprocessor = ColumnTransformer([\n    ('num', StandardScaler(), num_cols),\n    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n])\n\nX_train_proc = preprocessor.fit_transform(df_train.drop(columns=[target, 'level']))\nX_test_proc  = preprocessor.transform(df_test.drop(columns=[target, 'level']))\n\nle = LabelEncoder()\ny_train_enc = le.fit_transform(df_train[target])\ny_test_enc  = le.transform(df_test[target])\nnum_classes = len(le.classes_)\n\n# ===========================================\n# 2️⃣ Sampler & Logit Adjustment Setup\n# ===========================================\nclass_counts = np.bincount(y_train_enc)\n# Novelty: Logit adjustment term - log(class_frequency)\nlogit_adj = torch.tensor(class_counts + 1).float().log().to('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass_weights = 1.0 / np.sqrt(class_counts + 1)\nsampler = WeightedRandomSampler(weights=class_weights[y_train_enc], num_samples=len(y_train_enc), replacement=True)\n\ntrain_loader = DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_train_proc, dtype=torch.float32), \n                                                        torch.tensor(y_train_enc, dtype=torch.long)), \n                          batch_size=512, sampler=sampler)\ntest_loader = DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test_proc, dtype=torch.float32), \n                                                       torch.tensor(y_test_enc, dtype=torch.long)), \n                         batch_size=512, shuffle=False)\n\n# ===========================================\n# 3️⃣ Architecture: Gated Multi-Head Residual Net\n# ===========================================\nclass GatedBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc = nn.Linear(dim, dim * 2)\n        self.ln = nn.LayerNorm(dim)\n\n    def forward(self, x):\n        res = x\n        x = self.fc(x)\n        content, gate = x.chunk(2, dim=-1)\n        x = content * torch.sigmoid(gate) # GLU Gating novelty\n        return self.ln(x + res) # Residual Connection novelty\n\nclass GMHResNet(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        self.stem = nn.Linear(input_dim, 512)\n        self.blocks = nn.Sequential(\n            GatedBlock(512),\n            nn.Linear(512, 256),\n            GatedBlock(256),\n            nn.Dropout(0.3)\n        )\n        self.head = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.stem(x))\n        x = self.blocks(x)\n        return self.head(x)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = GMHResNet(X_train_proc.shape[1], num_classes).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n\n# ===========================================\n# 4️⃣ Training with Logit Adjustment\n# ===========================================\nfor epoch in range(20):\n    model.train()\n    for X_b, y_b in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        X_b, y_b = X_b.to(device), y_b.to(device)\n        optimizer.zero_grad()\n        logits = model(X_b)\n        \n        # Applying Logit Adjustment in loss calculation\n        loss = nn.functional.cross_entropy(logits - 0.5 * logit_adj, y_b)\n        \n        loss.backward()\n        optimizer.step()\n\n# ===========================================\n# 5️⃣ Evaluation\n# ===========================================\nmodel.eval()\nall_p, all_y = [], []\nwith torch.no_grad():\n    for X_b, y_b in test_loader:\n        out = model(X_b.to(device))\n        all_p.extend(torch.argmax(out, dim=1).cpu().numpy())\n        all_y.extend(y_b.numpy())\n\nprint(classification_report(all_y, all_p, labels=np.arange(num_classes), target_names=le.classes_, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T08:52:33.346095Z","iopub.execute_input":"2026-01-17T08:52:33.346837Z","iopub.status.idle":"2026-01-17T08:53:04.176565Z","shell.execute_reply.started":"2026-01-17T08:52:33.346787Z","shell.execute_reply":"2026-01-17T08:53:04.175902Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 247/247 [00:01<00:00, 176.64it/s]\nEpoch 2: 100%|██████████| 247/247 [00:01<00:00, 154.03it/s]\nEpoch 3: 100%|██████████| 247/247 [00:01<00:00, 172.10it/s]\nEpoch 4: 100%|██████████| 247/247 [00:01<00:00, 155.95it/s]\nEpoch 5: 100%|██████████| 247/247 [00:01<00:00, 175.88it/s]\nEpoch 6: 100%|██████████| 247/247 [00:01<00:00, 157.34it/s]\nEpoch 7: 100%|██████████| 247/247 [00:01<00:00, 176.42it/s]\nEpoch 8: 100%|██████████| 247/247 [00:01<00:00, 155.08it/s]\nEpoch 9: 100%|██████████| 247/247 [00:01<00:00, 177.62it/s]\nEpoch 10: 100%|██████████| 247/247 [00:01<00:00, 157.68it/s]\nEpoch 11: 100%|██████████| 247/247 [00:01<00:00, 176.71it/s]\nEpoch 12: 100%|██████████| 247/247 [00:01<00:00, 156.82it/s]\nEpoch 13: 100%|██████████| 247/247 [00:01<00:00, 173.46it/s]\nEpoch 14: 100%|██████████| 247/247 [00:01<00:00, 157.72it/s]\nEpoch 15: 100%|██████████| 247/247 [00:01<00:00, 156.00it/s]\nEpoch 16: 100%|██████████| 247/247 [00:01<00:00, 176.29it/s]\nEpoch 17: 100%|██████████| 247/247 [00:01<00:00, 159.33it/s]\nEpoch 18: 100%|██████████| 247/247 [00:01<00:00, 176.29it/s]\nEpoch 19: 100%|██████████| 247/247 [00:01<00:00, 159.32it/s]\nEpoch 20: 100%|██████████| 247/247 [00:01<00:00, 176.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"                 precision    recall  f1-score   support\n\n           back       0.99      0.91      0.95       359\nbuffer_overflow       0.80      0.20      0.32        20\n      ftp_write       0.02      0.67      0.04         3\n   guess_passwd       0.00      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.95      0.98      0.97       141\n           land       1.00      1.00      1.00         7\n     loadmodule       1.00      1.00      1.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       0.98      1.00      0.99      4657\n           nmap       1.00      0.99      0.99        73\n         normal       0.81      0.97      0.88      9711\n           perl       0.50      0.50      0.50         2\n            phf       0.50      0.50      0.50         2\n            pod       0.72      0.95      0.82        41\n      portsweep       0.61      0.96      0.75       157\n        rootkit       0.00      0.00      0.00        13\n          satan       0.79      0.71      0.74       735\n          smurf       0.99      1.00      1.00       665\n            spy       0.00      0.00      0.00         0\n       teardrop       0.22      1.00      0.36        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       1.00      0.00      0.00       944\n\n       accuracy                           0.85     18794\n      macro avg       0.56      0.58      0.51     18794\n   weighted avg       0.82      0.85      0.80     18794\n\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\n\n# ===========================================\n# 1️⃣ Stable Pipeline: Data & Mapping\n# ===========================================\nlocal_path = \"/kaggle/input/nslkdd/\" \ncolumns = [\n    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land',\n    'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n    'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n    'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count',\n    'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n    'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n    'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n    'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate',\n    'outcome', 'level'\n]\n\ndf_train = pd.read_csv(local_path + \"KDDTrain+.txt\", header=None, names=columns)\ndf_test  = pd.read_csv(local_path + \"KDDTest+.txt\", header=None, names=columns)\n\ndf_train['outcome'] = df_train['outcome'].astype(str)\ndf_test['outcome'] = df_test['outcome'].astype(str)\ntrain_labels = set(df_train['outcome'].unique())\ndf_test = df_test[df_test['outcome'].isin(train_labels)].reset_index(drop=True)\n\ntarget = 'outcome'\ncat_cols = ['protocol_type', 'service', 'flag']\nnum_cols = [c for c in columns if c not in cat_cols + [target, 'level']]\n\n# Stable ColumnTransformer\npreprocessor = ColumnTransformer([\n    ('num', StandardScaler(), num_cols),\n    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n])\n\nX_train_proc = preprocessor.fit_transform(df_train.drop(columns=[target, 'level']))\nX_test_proc  = preprocessor.transform(df_test.drop(columns=[target, 'level']))\n\nle = LabelEncoder()\ny_train_enc = le.fit_transform(df_train[target])\ny_test_enc  = le.transform(df_test[target])\nnum_classes = len(le.classes_)\n\n# ===========================================\n# 2️⃣ Balanced Batch Sampling & Logit Adj\n# ===========================================\nclass_counts = np.bincount(y_train_enc)\n# Novelty: Logit Adjustment shifts boundary for R2L/U2R\nlogit_adj = torch.tensor(class_counts + 1).float().log().to('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Stronger balancing (1/n) to force minority class learning\nclass_weights = 1.0 / (class_counts + 1e-6)\nsample_weights = class_weights[y_train_enc]\n\nsampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(y_train_enc), replacement=True)\n\ntrain_loader = DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_train_proc, dtype=torch.float32), \n                                                        torch.tensor(y_train_enc, dtype=torch.long)), \n                          batch_size=512, sampler=sampler)\ntest_loader = DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test_proc, dtype=torch.float32), \n                                                       torch.tensor(y_test_enc, dtype=torch.long)), \n                         batch_size=512, shuffle=False)\n\n# ===========================================\n# 3️⃣ Architecture: Gated Residual Network\n# ===========================================\nclass GLUBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc = nn.Linear(dim, dim * 2)\n        self.ln = nn.LayerNorm(dim)\n    def forward(self, x):\n        res = x\n        x = self.fc(x)\n        content, gate = x.chunk(2, dim=-1)\n        x = content * torch.sigmoid(gate)\n        return self.ln(x + res) # Residual connection\n\nclass GDPRNet(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        self.stem = nn.Linear(input_dim, 1024)\n        self.blocks = nn.Sequential(\n            GLUBlock(1024),\n            nn.Linear(1024, 512),\n            GLUBlock(512),\n            nn.Dropout(0.4)\n        )\n        self.head = nn.Linear(512, num_classes)\n    def forward(self, x):\n        x = F.gelu(self.stem(x))\n        x = self.blocks(x)\n        return self.head(x)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = GDPRNet(X_train_proc.shape[1], num_classes).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.05)\n\n# ===========================================\n# 4️⃣ Training with Boundary Refinement\n# ===========================================\nfor epoch in range(25):\n    model.train()\n    for X_b, y_b in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        X_b, y_b = X_b.to(device), y_b.to(device)\n        optimizer.zero_grad()\n        logits = model(X_b)\n        # Novelty: Logit Adjustment in CrossEntropy\n        loss = F.cross_entropy(logits - 0.5 * logit_adj, y_b, label_smoothing=0.1)\n        loss.backward()\n        optimizer.step()\n\n# ===========================================\n# 5️⃣ Q1 Evaluation\n# ===========================================\nmodel.eval()\nall_p, all_y = [], []\nwith torch.no_grad():\n    for X_b, y_b in test_loader:\n        out = model(X_b.to(device))\n        all_p.extend(torch.argmax(out, dim=1).cpu().numpy())\n        all_y.extend(y_b.numpy())\n\nprint(classification_report(all_y, all_p, labels=np.arange(num_classes), \n                            target_names=le.classes_, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T09:03:47.306504Z","iopub.execute_input":"2026-01-17T09:03:47.306860Z","iopub.status.idle":"2026-01-17T09:04:27.278866Z","shell.execute_reply.started":"2026-01-17T09:03:47.306830Z","shell.execute_reply":"2026-01-17T09:04:27.278222Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 247/247 [00:01<00:00, 149.06it/s]\nEpoch 2: 100%|██████████| 247/247 [00:01<00:00, 171.27it/s]\nEpoch 3: 100%|██████████| 247/247 [00:01<00:00, 151.38it/s]\nEpoch 4: 100%|██████████| 247/247 [00:01<00:00, 170.72it/s]\nEpoch 5: 100%|██████████| 247/247 [00:01<00:00, 149.25it/s]\nEpoch 6: 100%|██████████| 247/247 [00:01<00:00, 169.23it/s]\nEpoch 7: 100%|██████████| 247/247 [00:01<00:00, 148.59it/s]\nEpoch 8: 100%|██████████| 247/247 [00:01<00:00, 167.85it/s]\nEpoch 9: 100%|██████████| 247/247 [00:01<00:00, 151.14it/s]\nEpoch 10: 100%|██████████| 247/247 [00:01<00:00, 168.81it/s]\nEpoch 11: 100%|██████████| 247/247 [00:01<00:00, 153.15it/s]\nEpoch 12: 100%|██████████| 247/247 [00:01<00:00, 150.18it/s]\nEpoch 13: 100%|██████████| 247/247 [00:01<00:00, 170.31it/s]\nEpoch 14: 100%|██████████| 247/247 [00:01<00:00, 153.70it/s]\nEpoch 15: 100%|██████████| 247/247 [00:01<00:00, 170.84it/s]\nEpoch 16: 100%|██████████| 247/247 [00:01<00:00, 153.91it/s]\nEpoch 17: 100%|██████████| 247/247 [00:01<00:00, 170.22it/s]\nEpoch 18: 100%|██████████| 247/247 [00:01<00:00, 151.17it/s]\nEpoch 19: 100%|██████████| 247/247 [00:01<00:00, 169.43it/s]\nEpoch 20: 100%|██████████| 247/247 [00:01<00:00, 151.72it/s]\nEpoch 21: 100%|██████████| 247/247 [00:01<00:00, 167.12it/s]\nEpoch 22: 100%|██████████| 247/247 [00:01<00:00, 152.86it/s]\nEpoch 23: 100%|██████████| 247/247 [00:01<00:00, 170.24it/s]\nEpoch 24: 100%|██████████| 247/247 [00:01<00:00, 149.14it/s]\nEpoch 25: 100%|██████████| 247/247 [00:01<00:00, 164.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"                 precision    recall  f1-score   support\n\n           back       0.47      1.00      0.64       359\nbuffer_overflow       0.86      0.30      0.44        20\n      ftp_write       0.06      0.33      0.11         3\n   guess_passwd       1.00      0.00      0.00      1231\n           imap       0.00      0.00      0.00         1\n        ipsweep       0.85      0.96      0.90       141\n           land       1.00      1.00      1.00         7\n     loadmodule       1.00      1.00      1.00         2\n       multihop       0.00      0.00      0.00        18\n        neptune       0.99      1.00      1.00      4657\n           nmap       0.95      0.99      0.97        73\n         normal       0.80      0.93      0.86      9711\n           perl       0.50      0.50      0.50         2\n            phf       0.50      0.50      0.50         2\n            pod       0.72      0.95      0.82        41\n      portsweep       0.76      0.87      0.81       157\n        rootkit       0.02      0.15      0.03        13\n          satan       0.70      0.70      0.70       735\n          smurf       0.99      1.00      0.99       665\n            spy       0.00      0.00      0.00         0\n       teardrop       0.24      1.00      0.39        12\n    warezclient       0.00      0.00      0.00         0\n    warezmaster       0.00      0.00      0.00       944\n\n       accuracy                           0.83     18794\n      macro avg       0.54      0.57      0.51     18794\n   weighted avg       0.81      0.83      0.79     18794\n\n","output_type":"stream"}],"execution_count":28}]}